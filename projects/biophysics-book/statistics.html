<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Statistics for Biology</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability, statistics, Bayes, CDF, PDF">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="This chapter reviews basic concepts of probability and statistics that are useful for understanding the later material.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/biophysics-book/statistics"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        
        <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Biophysical Chemistry Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Statistics">
                <h1>Statistics for Biology</h1>

                <h2>Theory</h2>

                <h3>Law of Large Numbers</h3>

The Law of Large Numbers states that the sample average approaches the distribution mean as n, the number of samples, gets large.

                <figure>
                <img src="law-of-large-numbers.png" alt="Law of Large Numbers" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Law of Large Numbers for Coin Flip</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>
                <h3>Central Limit Theorem</h3>

The Central Limit Theorem goes further and describes how sample averages of independent observations are distributed if you plotted a histogram of sample averages. 
Regardless of the original distribution, the sample average will be distributed as a normal distribution, centered at the true mean with variance \(\frac{\sigma^2}{n}\). 

    <figure>
                <img src="central-limit-theorem.png" alt="Central Limit Theorem" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Central Limit Theorem with Varying Sample Size and Number of Trials</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>

<h2>Data</h2>

    <h3>Ordinary Least Squares</h3>

    <h4>Introduction to OLS</h4>
    Ordinary Least Squares (OLS) regression is a fundamental statistical method used to explore the relationship between a dependent variable and one or more independent variables.
    <br><br>
    OLS regression attempts to find the line (in simple regression) or hyperplane (in multiple regression) that minimizes the sum of the squared differences (the "errors") between the observed values and the values predicted by the model.

    <h4>Single Variable</h4>

    Simple linear regression involves one independent variable and one dependent variable. The relationship is expressed as:

    $$y=\beta_0+\beta_1 x+\varepsilon$$

    where y is the dependent variable, x is the independent variable, \(\beta_0\) is the intercept, \(\beta_1\) is the slope, and epsilon is the error.

    <h4>Multiple Linear Regression</h4>
    While tougher to visualize, the model can be expanded to have multiple indpeendent variables

    $$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\dots+\beta_n x_n +\varepsilon$$

    <h4>Assumptions</h4>
    The Gauss-Markov assumptions are fundamental prerequisites that underpin the Ordinary Least Squares (OLS) regression analysis. These assumptions ensure that the OLS estimators are BLUE (Best Linear Unbiased Estimators), which means they are the most efficient linear estimators with the least variance among the class of linear estimators. 

    <h5>Linearity in Parameters</h5>
    The relationship between the dependent and independent variables is linear in the parameters. This means that the model can be written as
    $$y=\beta_0 +\sum_i \beta_i x_i + \epsilon$$

    <h5>No Multicollinearity</h5>
    In multiple regression, the independent variables cannot be written as linear combinations of each other. This ensures that the matrix of predictors has full rank, which is necessary for the matrix to be invertible.

    <h5>Centered Errors</h5>
    The expected value of the error term conditional on the independent variables is zero:
    $$\mathbf{E}[\epsilon|x]=0$$
    This implies that the error term does not systematically vary with the independent variables.

    <h5>Homoskedasticity</h5>
    The variance of the error terms is constant across all levels of the independent variable(s): 
    $$\text{Var}[\epsilon|x]=\sigma^2$$

    <h5>No Serial Correlation</h5>
    The error terms are uncorrelated with each other.

    $$\text{Cov}[\epsilon_i,\epsilon_j|x]=0$$
    for \(i\neq j\)

    <h5>Summary</h5>
Violation of these assumptions can lead to biased or inefficient estimators, which in turn can misguide interpretations and conclusions. For instance, heteroscedasticity (non-constant variance of error terms) or serial correlation (correlation between error terms) can result in underestimated standard errors, leading to overly optimistic p-values.
    

    <h4>Optimization</h4>

    The objective function being minimized is the sum of squared residuals (SSR), which is given by

    $$SSR=\sum_{i=1}^n(y_i-(\beta_0+\sum_j^p \beta_jx_{i,j}))^2$$
    where n is the number of observations, p is the number of independent variables, 
    \(y_i\) is the observed value of the dependent variable, 
    \(x_{ij}\) is the value fo the jth independent variable for observation i,
    and beta are the parameters to be optimized.


    <br><br>
    For unregularized linear regression, there is a closed form solution, which makes it quick to calculate.

    $$\beta=(X^TX)^{-1}X^Ty$$

            </div>
            <div id="Practice Problems">
                
                <h2>Statistics Practice Problems</h2>

                <ol>
 
                </ol>
            </div>

        </div>
        <br>
        <br>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>