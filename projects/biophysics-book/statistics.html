<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
            <title>Statistics for Biology</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability, statistics, Bayes, CDF, PDF">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="This chapter reviews basic concepts of probability and statistics that are useful for understanding the later material.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/biophysics-book/statistics"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        
        <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->
<!-- JSON-LD markup generated by Google Structured Data Markup Helper. -->
<style>#chartContainer {
    width: 100%;
    max-width: 600px;
    margin: auto;
}
button {
    margin: 10px;
    padding: 8px 16px;
    font-size: 16px;
}

#regressionInfo {
    margin-top: 20px;
    font-size: 16px;
    color: #333; /* Adjust color based on your theme */
}

</style>
<script src="./statistics-plot.js"></script>
    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Biophysical Chemistry Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Statistics">
                <h1>Statistics for Biology</h1>

                <h2>Theory</h2>

                <h3>Law of Large Numbers</h3>

The Law of Large Numbers states that the sample average approaches the distribution mean as n, the number of samples, gets large.

                <figure>
                <img src="law-of-large-numbers.png" alt="Law of Large Numbers" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Law of Large Numbers for Coin Flip</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>
                <h3>Central Limit Theorem</h3>

The Central Limit Theorem goes further and describes how sample averages of independent observations are distributed if you plotted a histogram of sample averages. 
Regardless of the original distribution, the sample average will be distributed as a normal distribution, centered at the true mean with variance \(\frac{\sigma^2}{n}\). 

    <figure>
                <img src="central-limit-theorem.png" alt="Central Limit Theorem" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Central Limit Theorem with Varying Sample Size and Number of Trials</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>

<h2>Statistical Tests</h2>

<h3>T-Tests</h3>
The t-test is a statistical method used to determine whether there is a significant difference between the means of two groups.

<h4>Assumptions of T-test</h4>
<h5>Normality of Residuals</h5>
Residuals should be normally distributed




<h5>Homogeneity of Variances</h5>
The variances among the groups should be equal.

<h5>Independence of Observations</h5>
The observations should be independent of each other.

<h4>Types of T-tests</h4>

<h5>One-Sample</h5>
Used to determine if the mean of a single group differs significantly from a known or hypothesized mean.
<h5>Two-Sample</h5>
Compares the means of two independent groups to determine if they significantly differ from each other.

<h4>Performing Two-Sample T-test</h4>
<h5>Null Hypothesis</h5>
The null hypothesis posits that there is no significant difference between the means of the groups being compared.
<h5>Alternative Hypothesis</h5>
The alternative hypothesis posits that there is a significant difference between the means of the groups.
<h5>Calculation</h5>
Calculate the t-statistic using the formula: 

$$t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{{\frac{{s_1^2}}{{n_1}} + \frac{{s_2^2}}{{n_2}}}}}$$ where 

\(\bar{x}_i\) is the sample mean for i, \(s_i^2\) is the sample variance for i, and \(n_i\) is the sample size for i.

<h5>Decision</h5>
Compare the absolute value of the t-statistic to the critical t-value from the t-distribution table based on the chosen level of significance (e.g., 0.05) and degrees of freedom. 

<br><br>
If the absolute value of the t-statistic is greater than the critical t-value, reject the null hypothesis.

<h3>Analysis of Variance (ANOVA)</h3>
Analysis of Variance, abbreviated as ANOVA, is a statistical method used to compare the means of three or more independent groups to determine if there is a significant difference among them. 

<h4>Assumptions of ANOVA</h4>
The same assumptions as the T-test
<h5>Normality of Residuals</h5>
Residuals should be normally distributed

<h5>Homogeneity of Variances</h5>
The variances among the groups should be equal.

<h5>Independence of Observations</h5>
The observations should be independent of each other.

<h4>Types of ANOVA</h4>

<h5>One-way ANOVA</h5>
Used to compare the means of three or more independent groups based on one factor or independent variable.
<h5>Two-way ANOVA</h5>
Expands on one-way ANOVA by considering two factors or independent variables, allowing for the examination of interactions between factors.


<h2>Modeling</h2>

    <h3>Ordinary Least Squares</h3>

    <h4>Introduction to OLS</h4>
    Ordinary Least Squares (OLS) regression is a fundamental statistical method used to explore the relationship between a dependent variable and one or more independent variables.
    <br><br>
    OLS regression attempts to find the line (in simple regression) or hyperplane (in multiple regression) that minimizes the sum of the squared differences (the "errors") between the observed values and the values predicted by the model.

    <h4>Single Variable</h4>

    Simple linear regression involves one independent variable and one dependent variable. The relationship is expressed as:

    $$y=\beta_0+\beta_1 x+\varepsilon$$

    where y is the dependent variable, x is the independent variable, \(\beta_0\) is the intercept, \(\beta_1\) is the slope, and epsilon is the error.

    <div id="regressionInfo">
        <p id="regressionEquation">Equation: N/A</p>
        <p id="rSquaredValue">R²: N/A</p>
    </div>
    
    <div id="chartContainer">
        <canvas id="regressionChart"></canvas>
    </div>
    <label for="granularityInput">Point Placement Granularity:</label>
<input type="number" id="granularityInput" value="0.1" step="0.1" onchange="updateGranularity()">
    <button id="addPointButton">Add Point (Click on Plot)</button>
    <button id="clearPointsButton">Clear Points</button>
    


    <h4>Multiple Linear Regression</h4>
    While tougher to visualize, the model can be expanded to have multiple indpeendent variables

    $$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\dots+\beta_n x_n +\varepsilon$$

    <h4>Assumptions</h4>
    The Gauss-Markov assumptions are fundamental prerequisites that underpin the Ordinary Least Squares (OLS) regression analysis. These assumptions ensure that the OLS estimators are BLUE (Best Linear Unbiased Estimators), which means they are the most efficient linear estimators with the least variance among the class of linear estimators. 

    <h5>Linearity in Parameters</h5>
    The relationship between the dependent and independent variables is linear in the parameters. This means that the model can be written as
    $$y=\beta_0 +\sum_i \beta_i x_i + \epsilon$$

    <h5>No Multicollinearity</h5>
    In multiple regression, the independent variables cannot be written as linear combinations of each other. This ensures that the matrix of predictors has full rank, which is necessary for the matrix to be invertible.

    <h5>Centered Errors</h5>
    The expected value of the error term conditional on the independent variables is zero:
    $$\mathbf{E}[\epsilon|x]=0$$
    This implies that the error term does not systematically vary with the independent variables.

    <h5>Homoskedasticity</h5>
    The variance of the error terms is constant across all levels of the independent variable(s): 
    $$\text{Var}[\epsilon|x]=\sigma^2$$

    <h5>No Serial Correlation</h5>
    The error terms are uncorrelated with each other.

    $$\text{Cov}[\epsilon_i,\epsilon_j|x]=0$$
    for \(i\neq j\)

    <h5>Summary</h5>
Violation of these assumptions can lead to biased or inefficient estimators, which in turn can misguide interpretations and conclusions. For instance, heteroscedasticity (non-constant variance of error terms) or serial correlation (correlation between error terms) can result in underestimated standard errors, leading to overly optimistic p-values.
    

    <h4>Optimization</h4>

    The objective function being minimized is the sum of squared residuals (SSR), which is given by

    $$SSR=\sum_{i=1}^n(y_i-(\beta_0+\sum_j^p \beta_jx_{i,j}))^2$$
    where n is the number of observations, p is the number of independent variables, 
    \(y_i\) is the observed value of the dependent variable, 
    \(x_{ij}\) is the value fo the jth independent variable for observation i,
    and beta are the parameters to be optimized.


    <br><br>
    For unregularized linear regression, there is a closed form solution, which makes it quick to calculate.

    $$\beta=(X^TX)^{-1}X^Ty$$
<script>
    document.getElementById('clearPointsButton').addEventListener('click', () => {
    regressionChart.data.datasets[0].data = [];
    regressionChart.data.datasets[1].data = [];
    regressionChart.update();
});

document.addEventListener('DOMContentLoaded', setupChart);

</script>
            </div>
            <div id="Practice Problems">
                
                <h2>Statistics Practice Problems</h2>

                <ol>
 
                </ol>
            </div>

        </div>
        <br>
        <br>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>