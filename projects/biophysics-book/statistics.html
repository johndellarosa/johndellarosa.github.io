<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Statistics for Biology</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability, statistics, Bayes, CDF, PDF">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="This chapter reviews basic concepts of probability and statistics that are useful for understanding the later material.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/biophysics-book/statistics"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        
        <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->
<!-- JSON-LD markup generated by Google Structured Data Markup Helper. -->
<script type="application/ld+json">
    [
      {
        "@context": "http://schema.org",
        "@type": "Article",
        "name": "Statistics for Biology",
        "author": {
          "@type": "Person",
          "name": "John Della Rosa"
        },
        "articleSection": "Theory",
        "articleBody": [
          "The Law of Large Numbers states that the sample average approaches the distribution mean as n, the number of samples, gets large.\n\n                <FIGURE>\n                <IMG src=\"law-of-large-numbers.png\" alt=\"Law of Large Numbers\" style=\"margin:auto; display: block;  width:clamp(300px,50vw, 800px)\" loading=\"lazy\"/>",
          "The Central Limit Theorem goes further and describes how sample averages of independent observations are distributed if you plotted a histogram of sample averages. \nRegardless of the original distribution, the sample average will be distributed as a normal distribution, centered at the true mean with variance <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"0\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MFRAC><MJX-FRAC><MJX-NUM><MJX-MSUP size=\"s\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D70E TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: 0.363em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c32\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUP></MJX-NUM><MJX-DBOX><MJX-DTABLE><MJX-LINE></MJX-LINE><MJX-ROW><MJX-DEN><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D45B TEX-I\"></MJX-C></MJX-MI></MJX-DEN></MJX-ROW></MJX-DTABLE></MJX-DBOX></MJX-FRAC></MJX-MFRAC></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><msup><mi>σ</mi><mn>2</mn></msup><mi>n"
        ]
      },
      {
        "@context": "http://schema.org",
        "@type": "Article",
        "name": "Statistics for Biology",
        "author": {
          "@type": "Person",
          "name": "John Della Rosa"
        },
        "image": "https://johndellarosa.github.io/projects/biophysics-book/central-limit-theorem.png",
        "articleSection": "Data",
        "articleBody": "Ordinary Least Squares</H3>\n\n    <H4>Introduction to OLS</H4>\n    Ordinary Least Squares (OLS) regression is a fundamental statistical method used to explore the relationship between a dependent variable and one or more independent variables.\n    <BR/><BR/>\n    OLS regression attempts to find the line (in simple regression) or hyperplane (in multiple regression) that minimizes the sum of the squared differences (the &quot;errors&quot;) between the observed values and the values predicted by the model.\n\n    <H4>Single Variable</H4>\n\n    Simple linear regression involves one independent variable and one dependent variable. The relationship is expressed as:\n\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"1\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MSUB space=\"4\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MSUB space=\"3\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\" space=\"3\"><MJX-C class=\"mjx-c1D700 TEX-I\"></MJX-C></MJX-MI></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><mi>ε</mi></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n\n    where y is the dependent variable, x is the independent variable, <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"2\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>β</mi><mn>0</mn></msub></math></MJX-ASSISTIVE-MML></MJX-CONTAINER> is the intercept, <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"3\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>β</mi><mn>1</mn></msub></math></MJX-ASSISTIVE-MML></MJX-CONTAINER> is the slope, and epsilon is the error.\n\n    <H4>Multiple Linear Regression</H4>\n    While tougher to visualize, the model can be expanded to have multiple indpeendent variables\n\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"4\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MSUB space=\"4\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MSUB space=\"3\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MSUB space=\"3\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c32\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c32\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c22EF\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MSUB space=\"3\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D45B TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D45B TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\" space=\"3\"><MJX-C class=\"mjx-c1D700 TEX-I\"></MJX-C></MJX-MI></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>+</mo><mi>ε</mi></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n\n    <H4>Assumptions</H4>\n    The Gauss-Markov assumptions are fundamental prerequisites that underpin the Ordinary Least Squares (OLS) regression analysis. These assumptions ensure that the OLS estimators are BLUE (Best Linear Unbiased Estimators), which means they are the most efficient linear estimators with the least variance among the class of linear estimators. \n\n    <H5>Linearity in Parameters</H5>\n    The relationship between the dependent and independent variables is linear in the parameters. This means that the model can be written as\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"5\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MSUB space=\"4\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MUNDER space=\"3\"><MJX-ROW><MJX-BASE><MJX-MO class=\"mjx-lop\"><MJX-C class=\"mjx-c2211 TEX-S2\"></MJX-C></MJX-MO></MJX-BASE></MJX-ROW><MJX-ROW><MJX-UNDER style=\"padding-top: 0.167em; padding-left: 0.6em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-UNDER></MJX-ROW></MJX-MUNDER><MJX-MSUB space=\"2\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\" space=\"3\"><MJX-C class=\"mjx-c1D716 TEX-I\"></MJX-C></MJX-MI></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><munder><mo data-mjx-texclass=\"OP\">∑</mo><mi>i</mi></munder><msub><mi>β</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n\n    <H5>No Multicollinearity</H5>\n    In multiple regression, the independent variables cannot be written as linear combinations of each other. This ensures that the matrix of predictors has full rank, which is necessary for the matrix to be invertible.\n\n    <H5>Centered Errors</H5>\n    The expected value of the error term conditional on the independent variables is zero:\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"6\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-TEXATOM texclass=\"ORD\"><MJX-MI class=\"mjx-b\"><MJX-C class=\"mjx-c1D404 TEX-B\"></MJX-C></MJX-MI></MJX-TEXATOM><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5B\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D716 TEX-I\"></MJX-C></MJX-MI><MJX-TEXATOM texclass=\"ORD\"><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c7C\"></MJX-C></MJX-MO></MJX-TEXATOM><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5D\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MN class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow data-mjx-texclass=\"ORD\"><mi mathvariant=\"bold\">E</mi></mrow><mo stretchy=\"false\">[</mo><mi>ϵ</mi><mrow data-mjx-texclass=\"ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>x</mi><mo stretchy=\"false\">]</mo><mo>=</mo><mn>0</mn></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n    This implies that the error term does not systematically vary with the independent variables.\n\n    <H5>Homoskedasticity</H5>\n    The variance of the error terms is constant across all levels of the independent variable(s): \n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"7\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MTEXT class=\"mjx-n\"><MJX-C class=\"mjx-c56\"></MJX-C><MJX-C class=\"mjx-c61\"></MJX-C><MJX-C class=\"mjx-c72\"></MJX-C></MJX-MTEXT><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5B\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D716 TEX-I\"></MJX-C></MJX-MI><MJX-TEXATOM texclass=\"ORD\"><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c7C\"></MJX-C></MJX-MO></MJX-TEXATOM><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5D\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MSUP space=\"4\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D70E TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: 0.413em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c32\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUP></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Var</mtext><mo stretchy=\"false\">[</mo><mi>ϵ</mi><mrow data-mjx-texclass=\"ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>x</mi><mo stretchy=\"false\">]</mo><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n\n    <H5>No Serial Correlation</H5>\n    The error terms are uncorrelated with each other.\n\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"8\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MTEXT class=\"mjx-n\"><MJX-C class=\"mjx-c43\"></MJX-C><MJX-C class=\"mjx-c6F\"></MJX-C><MJX-C class=\"mjx-c76\"></MJX-C></MJX-MTEXT><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5B\"></MJX-C></MJX-MO><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D716 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c2C\"></MJX-C></MJX-MO><MJX-MSUB space=\"2\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D716 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-TEXATOM texclass=\"ORD\"><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c7C\"></MJX-C></MJX-MO></MJX-TEXATOM><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c5D\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MN class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Cov</mtext><mo stretchy=\"false\">[</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>ϵ</mi><mi>j</mi></msub><mrow data-mjx-texclass=\"ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>x</mi><mo stretchy=\"false\">]</mo><mo>=</mo><mn>0</mn></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n    for <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"9\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c2260\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\" space=\"4\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi><mo>≠</mo><mi>j</mi></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n\n    <H5>Summary</H5>\nViolation of these assumptions can lead to biased or inefficient estimators, which in turn can misguide interpretations and conclusions. For instance, heteroscedasticity (non-constant variance of error terms) or serial correlation (correlation between error terms) can result in underestimated standard errors, leading to overly optimistic p-values.\n    \n\n    <H4>Optimization</H4>\n\n    The objective function being minimized is the sum of squared residuals (SSR), which is given by\n\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"10\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D446 TEX-I\"></MJX-C></MJX-MI><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D446 TEX-I\"></MJX-C></MJX-MI><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D445 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MUNDEROVER space=\"4\"><MJX-OVER style=\"padding-bottom: 0.192em; padding-left: 0.51em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D45B TEX-I\"></MJX-C></MJX-MI></MJX-OVER><MJX-BOX><MJX-MUNDER><MJX-ROW><MJX-BASE><MJX-MO class=\"mjx-lop\"><MJX-C class=\"mjx-c2211 TEX-S2\"></MJX-C></MJX-MO></MJX-BASE></MJX-ROW><MJX-ROW><MJX-UNDER style=\"padding-top: 0.167em; padding-left: 0.148em;\"><MJX-TEXATOM size=\"s\" texclass=\"ORD\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MN class=\"mjx-n\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-TEXATOM></MJX-UNDER></MJX-ROW></MJX-MUNDER></MJX-BOX></MJX-MUNDEROVER><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c28\"></MJX-C></MJX-MO><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2212\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c28\"></MJX-C></MJX-MO><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c30\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\" space=\"3\"><MJX-C class=\"mjx-c2B\"></MJX-C></MJX-MO><MJX-MUNDEROVER space=\"3\"><MJX-OVER style=\"padding-bottom: 0.111em; padding-left: 0.544em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D45D TEX-I\"></MJX-C></MJX-MI></MJX-OVER><MJX-BOX><MJX-MUNDER><MJX-ROW><MJX-BASE><MJX-MO class=\"mjx-lop\"><MJX-C class=\"mjx-c2211 TEX-S2\"></MJX-C></MJX-MO></MJX-BASE></MJX-ROW><MJX-ROW><MJX-UNDER style=\"padding-top: 0.167em; padding-left: 0.576em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-UNDER></MJX-ROW></MJX-MUNDER></MJX-BOX></MJX-MUNDEROVER><MJX-MSUB space=\"2\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-TEXATOM size=\"s\" texclass=\"ORD\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c2C\"></MJX-C></MJX-MO><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-TEXATOM></MJX-SCRIPT></MJX-MSUB><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c29\"></MJX-C></MJX-MO><MJX-MSUP><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c29\"></MJX-C></MJX-MO><MJX-SCRIPT style=\"vertical-align: 0.413em;\"><MJX-MN class=\"mjx-n\" size=\"s\"><MJX-C class=\"mjx-c32\"></MJX-C></MJX-MN></MJX-SCRIPT></MJX-MSUP></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>S</mi><mi>S</mi><mi>R</mi><mo>=</mo><munderover><mo data-mjx-texclass=\"OP\">∑</mo><mrow data-mjx-texclass=\"ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo stretchy=\"false\">(</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><munderover><mo data-mjx-texclass=\"OP\">∑</mo><mi>j</mi><mi>p</mi></munderover><msub><mi>β</mi><mi>j</mi></msub><msub><mi>x</mi><mrow data-mjx-texclass=\"ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></math></MJX-ASSISTIVE-MML></MJX-CONTAINER>\n    where n is the number of observations, p is the number of independent variables, \n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"11\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUB></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub></math></MJX-ASSISTIVE-MML></MJX-CONTAINER> is the observed value of the dependent variable, \n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" tabindex=\"0\" ctxtmenu_counter=\"12\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH class=\"MJX-TEX\" aria-hidden=\"true\"><MJX-MSUB><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D465 TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: -0.15em;\"><MJX-TEXATOM size=\"s\" texclass=\"ORD\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D456 TEX-I\"></MJX-C></MJX-MI><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D457 TEX-I\"></MJX-C></MJX-MI></MJX-TEXATOM></MJX-SCRIPT></MJX-MSUB></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow data-mjx-texclass=\"ORD\"><mi>i</mi><mi>j</mi></mrow></msub></math></MJX-ASSISTIVE-MML></MJX-CONTAINER> is the value fo the jth independent variable for observation i,\n    and beta are the parameters to be optimized.\n\n\n    <BR/><BR/>\n    For unregularized linear regression, there is a closed form solution, which makes it quick to calculate.\n\n    <MJX-CONTAINER class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" tabindex=\"0\" ctxtmenu_counter=\"13\" style=\"font-size: 117.4%; position: relative;\"><MJX-MATH display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\" style=\"margin-left: 0px; margin-right: 0px;\"><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D6FD TEX-I\"></MJX-C></MJX-MI><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c3D\"></MJX-C></MJX-MO><MJX-MO class=\"mjx-n\" space=\"4\"><MJX-C class=\"mjx-c28\"></MJX-C></MJX-MO><MJX-MSUP><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D44B TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: 0.413em; margin-left: 0.051em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D447 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUP><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D44B TEX-I\"></MJX-C></MJX-MI><MJX-MSUP><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c29\"></MJX-C></MJX-MO><MJX-SCRIPT style=\"vertical-align: 0.413em;\"><MJX-TEXATOM size=\"s\" texclass=\"ORD\"><MJX-MO class=\"mjx-n\"><MJX-C class=\"mjx-c2212\"></MJX-C></MJX-MO><MJX-MN class=\"mjx-n\"><MJX-C class=\"mjx-c31\"></MJX-C></MJX-MN></MJX-TEXATOM></MJX-SCRIPT></MJX-MSUP><MJX-MSUP><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D44B TEX-I\"></MJX-C></MJX-MI><MJX-SCRIPT style=\"vertical-align: 0.413em; margin-left: 0.051em;\"><MJX-MI class=\"mjx-i\" size=\"s\"><MJX-C class=\"mjx-c1D447 TEX-I\"></MJX-C></MJX-MI></MJX-SCRIPT></MJX-MSUP><MJX-MI class=\"mjx-i\"><MJX-C class=\"mjx-c1D466 TEX-I\"></MJX-C></MJX-MI></MJX-MATH><MJX-ASSISTIVE-MML unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>β</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy=\"false\">)</mo><mrow data-mjx-texclass=\"ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y"
      }
    ]
    </script>
    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Biophysical Chemistry Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Statistics">
                <h1>Statistics for Biology</h1>

                <h2>Theory</h2>

                <h3>Law of Large Numbers</h3>

The Law of Large Numbers states that the sample average approaches the distribution mean as n, the number of samples, gets large.

                <figure>
                <img src="law-of-large-numbers.png" alt="Law of Large Numbers" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Law of Large Numbers for Coin Flip</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>
                <h3>Central Limit Theorem</h3>

The Central Limit Theorem goes further and describes how sample averages of independent observations are distributed if you plotted a histogram of sample averages. 
Regardless of the original distribution, the sample average will be distributed as a normal distribution, centered at the true mean with variance \(\frac{\sigma^2}{n}\). 

    <figure>
                <img src="central-limit-theorem.png" alt="Central Limit Theorem" style="margin:auto; display: block;  width:clamp(300px,50vw, 800px)" loading="lazy" />
                <figcaption>Central Limit Theorem with Varying Sample Size and Number of Trials</figcaption>
            </figure><br>
                <button type="button" class="collapsible">Python Code</button>
<div class="content">
<code style="margin: auto; width: 50%;"><pre>
fig, axes = plt.subplots(3,2,figsize=(9,9))

def illustrate_clt(n, k, x,y, axes):
    """Illustrate the Central Limit Theorem using uniform distribution.
    
    Parameters:
    - n: number of samples taken in each experiment
    - k: number of experiments
    """
    # Collect the averages from k experiments
    averages = [np.mean(np.random.uniform(0, 1, n)) for _ in range(k)]

    # Plotting the histogram of the averages
    axes[x,y].hist(averages, bins=50, density=True, color='skyblue', edgecolor='black')
    
    axes[x,y].set_title(f'Distribution of Averages from {k} Experiments\nEach with {n} Random Uniform Samples',size=8)
    axes[x,y].set_xlabel('Sample Average')
    axes[x,y].set_ylabel('Density')
    axes[x,y].set_xlim(0,1)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)


n_vals = [3,10,30]
k_vals = [250,10000]
for x in range(len(n_vals)):
    for y in range(len(k_vals)):
        
        illustrate_clt(n=n_vals[x], k=k_vals[y],x=x,y=y,axes=axes)
plt.tight_layout()
plt.show()
    
</pre>
</code></div>

<h2>Data</h2>

    <h3>Ordinary Least Squares</h3>

    <h4>Introduction to OLS</h4>
    Ordinary Least Squares (OLS) regression is a fundamental statistical method used to explore the relationship between a dependent variable and one or more independent variables.
    <br><br>
    OLS regression attempts to find the line (in simple regression) or hyperplane (in multiple regression) that minimizes the sum of the squared differences (the "errors") between the observed values and the values predicted by the model.

    <h4>Single Variable</h4>

    Simple linear regression involves one independent variable and one dependent variable. The relationship is expressed as:

    $$y=\beta_0+\beta_1 x+\varepsilon$$

    where y is the dependent variable, x is the independent variable, \(\beta_0\) is the intercept, \(\beta_1\) is the slope, and epsilon is the error.

    <h4>Multiple Linear Regression</h4>
    While tougher to visualize, the model can be expanded to have multiple indpeendent variables

    $$y=\beta_0+\beta_1 x_1+\beta_2 x_2+\dots+\beta_n x_n +\varepsilon$$

    <h4>Assumptions</h4>
    The Gauss-Markov assumptions are fundamental prerequisites that underpin the Ordinary Least Squares (OLS) regression analysis. These assumptions ensure that the OLS estimators are BLUE (Best Linear Unbiased Estimators), which means they are the most efficient linear estimators with the least variance among the class of linear estimators. 

    <h5>Linearity in Parameters</h5>
    The relationship between the dependent and independent variables is linear in the parameters. This means that the model can be written as
    $$y=\beta_0 +\sum_i \beta_i x_i + \epsilon$$

    <h5>No Multicollinearity</h5>
    In multiple regression, the independent variables cannot be written as linear combinations of each other. This ensures that the matrix of predictors has full rank, which is necessary for the matrix to be invertible.

    <h5>Centered Errors</h5>
    The expected value of the error term conditional on the independent variables is zero:
    $$\mathbf{E}[\epsilon|x]=0$$
    This implies that the error term does not systematically vary with the independent variables.

    <h5>Homoskedasticity</h5>
    The variance of the error terms is constant across all levels of the independent variable(s): 
    $$\text{Var}[\epsilon|x]=\sigma^2$$

    <h5>No Serial Correlation</h5>
    The error terms are uncorrelated with each other.

    $$\text{Cov}[\epsilon_i,\epsilon_j|x]=0$$
    for \(i\neq j\)

    <h5>Summary</h5>
Violation of these assumptions can lead to biased or inefficient estimators, which in turn can misguide interpretations and conclusions. For instance, heteroscedasticity (non-constant variance of error terms) or serial correlation (correlation between error terms) can result in underestimated standard errors, leading to overly optimistic p-values.
    

    <h4>Optimization</h4>

    The objective function being minimized is the sum of squared residuals (SSR), which is given by

    $$SSR=\sum_{i=1}^n(y_i-(\beta_0+\sum_j^p \beta_jx_{i,j}))^2$$
    where n is the number of observations, p is the number of independent variables, 
    \(y_i\) is the observed value of the dependent variable, 
    \(x_{ij}\) is the value fo the jth independent variable for observation i,
    and beta are the parameters to be optimized.


    <br><br>
    For unregularized linear regression, there is a closed form solution, which makes it quick to calculate.

    $$\beta=(X^TX)^{-1}X^Ty$$

            </div>
            <div id="Practice Problems">
                
                <h2>Statistics Practice Problems</h2>

                <ol>
 
                </ol>
            </div>

        </div>
        <br>
        <br>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>