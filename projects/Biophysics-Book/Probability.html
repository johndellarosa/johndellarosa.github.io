<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Probability, Statistics, & Entropy</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability, statistics, Bayes, CDF, PDF">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="This chapter reviews basic concepts of probability and statistics that are useful for understanding the later material.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
            
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        
        <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/index.html">Home</a>|
            <a href="https://johndellarosa.github.io/resume.html">Resume</a>
            <!-- <a href="./index.html#education">Education</a>
            <a href="./index.html#experience">Experience</a>
            <a href="./index.html#skills">Skills</a> -->
            |
            <a href="https://johndellarosa.github.io/biography.html">About</a>|
            <a href="https://johndellarosa.github.io/projects.html">Projects</a>|
            <a href="https://johndellarosa.github.io/Miscellaneous.html">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "Table-of-Contents.html">Biophysical Chemistry Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Probability">
                <h1>Probability, Statistics, and Entropy</h1>
                <h2>Probability</h2>
                <h3>Defining terms</h3>
                Before we show any equations, we must define some terms. Let A and B be events.
                We will also denote \(\Omega\) to be the set of all possible outcomes. 
                <ul>
                    <li>P(A) is the probability of event A occurring</li>
                    <li>\(P(A\cup B)\) is the probability of A or B or both occurring. The \(\lor\) can also be seen instead of \(\cup\)</li>
                    <li>\(P(A\cap B)\) is the probability of A <b>and</b> B both occurring. The \(\land\) can also be seen instead of \(\cap\)</li>
                    <li>\(P(\neg A)\) is the probability of A not occurring. This can also be written as a bar above the A</li>
                </ul>



                <h3>Basic Rules</h3>
                The sum of the probabilities of all possible outcomes must equal 1. For discrete distributions this can be stated as 
                $$\sum P(x_i)=1$$
                and for continuous distributions, we can write
                $$\int_{-\infty}^{\infty}p(x)dx=1$$
                <h4>Inclusion-Exclussion Principle</h4>
                $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
                The Inclusion-Exclussion Principle gives a relationship between two events, the probability of both happening, and the probability of either happening. This essentially makes sure we don't double count as \(A\cap B\) is included as an outcome in P(A) and P(B). 
                <h4>De Morgan's Laws</h4>
                De Morgan's laws describe how negation interactions with grouping of events. The statements can be rather formal, but verbally seem obvious. 
                <br>
                "If neither A nor B happened, then that means A didn't happen and B didn't happen"
                $$\neg(A \lor B)=(\neg A)\land (\neg B)$$
    
                "If A and B didn't both happen, then that means either A didn't happen, B didn't happen, or neither happened"
                $$\neg(A \land B)=(\neg A)\lor (\neg B)$$
                <h3>Cumulative Density Function</h3>
                The cumulative density function states the probability of getting the outcome being less than or equal to a given x. 
                We will define a <b>Cumulative Density Function</b> (CDF) formally as 
                
                $$F(x)=P(X\leq x)$$
                where X is the value of the random variable and x is a chosen value or cutoff point of interest.

                For discrete variables, we can formulate this as 

                $$F(x)=\sum_{x_i\leq x}p(x_i)$$
                where \(x_i\) is a possible value that X takes. We will call the function p(x) the <b>probability mass function</b> (PMF) for discrete variables. 

                <h4>Example with Binomial Distribution</h4>
                    <img src="BinomialPMFCDF.png" alt="Binomial PMF and CDF" style="margin:auto; display: block;  width:clamp(400px,50vw, 800px)"/>
                    Python code:
                    <code style="margin: auto; width: 50%;"><pre>
def combinations(n,k):
    return math.factorial(n)\
    /(math.factorial(k)*math.factorial(n-k))
def binomial_PMF(k,n,p):
    return combinations(n,k)*(p**k)*(1-p)**(n-k)
def binomial_CDF(k,n,p):
    accum = 0
    for i in range(0,k+1):
        accum += binomial_PMF(i,n,p)
    return accum
n = 8
x_values = [i for i in range(0,n+1)]
p = 0.6
fig = plt.figure()
plt.scatter(x_values,
            [binomial_PMF(k,n,p) for k in x_values])
plt.scatter(x_values,
            [binomial_CDF(k,n,p) for k in x_values],
            color='orange')
plt.legend(['PMF: $P(X=x_i)$','CDF: $P(X\leq x_i)$'])
plt.title("PMF and CDF for Binomial (p=0.6, n=8)")
plt.xlabel('k (number successes)')
plt.ylabel('Probability')
plt.show()</pre></code>


                <br><br>
                For continuous variables, we have 

                $$F(x)=\int_{-\infty}^xf(y)dy$$

                where f(y) is the probability density at y. We can think of this as the probability of  We will call the density function f(y) the <b>probability density function</b> (PDF) for continuous variables as opposed to PMF. 

                <br><br>
                We can consequently calculate the probability of being between two values as follows:

                $$P(a\lt X\leq b) = F(b)-F(a)$$
                Which also means
                $$P(a \lt X \leq b)=\int_{a}^bf(y)dy$$

                Another common definition is that of the <b>survival function</b>. We define this as 
                $$\bar{F}(x)=1-F(x)$$
Thus the survival function is also equivalent to 
$$\bar{F}(x)=P(X\gt x)$$

<h5>Example</h5>
Let X be an exponentially distributed variable with \(\lambda=2\), with the following density:

$$f(x)= \begin{cases}
                2\exp(-2x) & x\geq 0 \\
                0 & x \lt 0

\end{cases}$$

The probability that \(1\lt X \leq 2\) is given by

$$P(1\lt X \leq 2)=F(2)-F(1)$$
$$=\int_1^2 2\exp(-2x)dx$$

<img src="AreaUnderCurve.png" alt="Probability Continuous Variable" style="margin:auto; display: block;  width:clamp(400px,50vw, 800px)"/>
                Python code:
                <code style="margin: auto; width: 50%;"><pre>
lam = 2

def pdf_exp(x,lam):
    return np.where(x>=0, lam*np.exp(-lam*x),0)

x_values = np.linspace(0,6,1000)
plt.figure(figsize=(9,6))
plt.plot(x_values, pdf_exp(x_values,lam))
fill_x_values = np.linspace(1,2,300)
plt.fill_between(fill_x_values,pdf_exp(fill_x_values,lam))

plt.plot([0, 0], [0, 2], color='black')
plt.plot([0,6],[0,0],color='black')
plt.xlabel('x')
plt.ylabel('Density of X')
plt.title("Area Under Curve For Probability")
plt.legend(['Exp(2)'])
plt.show()</pre>
                </code>

                
    
    
                <h3>Conditional Probability</h3>
                Let A and B be two events. We denote the conditional probability of A occurring given B having occurred as \(P(A|B)\)
    
                This is defined mathematically as:
    
                $$P(A|B)\equiv \frac{P(A \cap B)}{P(B)}$$
                where \(P(A\cap B)\) is the probability of both A and B occurring.
                
    
    
                <h4>Bayes' Theorem</h4>
                Bayes' Theorem relates two events and the conditional probabilities of one upon another. 
                Let A and B two events with non-zero probabilities
                $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
    
                <h3>Expected Value and Variance</h3>
                Given a probability distribution, what properties can we discern? 

                Let E[f(X)] denote the "Expected value," which can be thought of as the density-weighted mean, for a function f(x) on a distribution X. 

                We can calculate this as 

                $$E[f(X)]=\sum_{x_i}f(x)\times p(X=x_i)$$
                or in the continuous case
                $$E[f(X)]=\int_{-\infty}^{\infty}f(x)p(x)dx$$

                If we wanted to calculate the mean of a distribution, we can use f(X)=x and calculate the expectation.

                $$E[X]=\int_{-\infty}^{\infty}xp(x)dx$$

                Another common statistic that is used to describe a distribution is the variance, which is a measure of deviation from the mean.

                $$\text{Var}(X)=E[X^2]-(E[X])^2$$

                We can calculate this by separately calculating the expectations for \(E[X^2]\) and E[X]. The standard deviation is the square root of the variance. 



                <h5>Example</h5>
                The Bernoulli Distribution is given by 
                $$P(X=1)=p$$
                $$P(X=0)=1-p$$

                Calculate the mean and variance of the distribution.

                $$E[X]=\sum x_i p(x_i)$$
                $$=1\times p + 0 \times (1-p)$$
                $$=p$$

                $$E[X^2] = \sum x_i^2 p(x_i)$$
                $$=1^2 \times p + 0^2 \times (1-p)$$
                $$=p$$

                $$\text{Var}(X)=E[X^2]-(E[X])^2$$
                $$=p-p^2$$
                $$p(1-p)$$



                <h3>Multivariate Distributions</h3>

                

                A distribution can output a set of random variables.
                $$F(x_1,\dots,x_n)=P(X_1\leq x_i,\dots,X_n\leq x_n)$$
                We say that \(X_1,\dots,X_n\) are independent if

                
                
                $$F(x_1,\dots,x_n)=\prod_{x_i}P(X_i\leq x_i)$$

                We can use the notation

                $$F_{X_i}(x_i)\equiv P(X_i\leq x_i)$$

                We can define the density of a multivariate distribution as 

                $$f(x_1,\dots,x_n)\equiv\frac{\partial^n}{\partial_1\cdots\partial_n}F(x_1,\dots,x_n)$$

                We can also define a function called the marginal density. Let us consider a bivariate distribution given by \(x_1\) and \(x_2\). 
                $$F_{X_1}(x_1)=\int_{-\infty}^{\infty}f(x_1,x_2)dx_2$$

                Using the marginal densities, we can arrive at an alternative definition of independence. Two random variables are independent if
                $$f(x_1,x_2)=f_{X_1}(x_1)f_{X_2}(x_2)\forall(x_1,x_2)$$
    
                Let us define the conditional distribution of \(X_1\) given \(X_2=x_2\)

                $$F_{X_1|X_2}(x_1|X_2)\equiv P(X_1\leq x_1|X_2=x_2)$$
                The marginal density is more useful in practice
                $$f_{X_1|X_2}(x_1|x_2)=\frac{\partial}{\partial x_1}F_{X_1|X_2}(x_1|x_2)=\frac{f(x_1,x_2)}{f_{X_2}(x_2)}$$

                Equivalently,

                $$f(x_1,x_2)=f_{X_1|X_2}(x_1|x_2)f_{X_2}(x_2)$$

                <h3>Covariance and Correlation</h3>
                How do we quantify how two variables co-move?
                <br><br>
                Let us define the Covariance between \(X_1\) and \(X_2\) as

                $$\text{Cov}(X_1,X_2)=E[X_1X_2]-E[X_1]E[X_2]$$

                The special case of the covariance of a random variable with itself is equal to the variance. 
                <br><br>
                We can normalize this into the <b>Pearson correlation</b> (often just called correlation) as follows:

                $$\rho(X_1,X_2)=\frac{\text{Cov}(X_1,X_2)}{\sqrt{\text{Var}[X_1]\text{Var}[X_2]}}$$

                What must be emphasized is that a correlation of 0 does not mean that two variables are independent. 
                A correlation of 0 is a <b>necessary but not sufficient</b> condition. For example, take a random variable that is distributed like \(X\sim N(0,1)\) and another variable Y=abs(X). The correlation is 0, but they are not independent.
<br><br>
                Also, Pearson correlation only captures linear behavior between two variables. Another metric for monotonic behaviour between two random variables is <b>Spearman correlation</b>. Kendall's tau coefficient is a third way. 

                <h2>Entropy</h2>
                <h3>Multiplicity</h3>
    
                If we flip a coin twice, the most likely outcome for the number of heads we get is 1. 
                How is this possible if the coins are fair and independent? The answer has to do with that there are two possible sequences which will give you 1 head, TH and HT, while there are only 1 sequence each for 0 heads and 2 heads. 
                We will call this phenomenon of their being different number of sequences for each outcome "multiplicity."
                To further use the coin analogy, we will call the specific sequence the <b>microstate</b> and the total number of heads to be the <b>state</b> or <b>macrostate</b>. 
                Each microstate is equally likely, but there are 2 microstates that correspond to the 1 heads macrostate. 
                
                <br><br>
                Given M coin flips, the number of ways to get N heads would be given as:
                $$W=\frac{M!}{N!(M-N)!}$$
                
                However, this restricts us to only 2 outcomes. What if we require more? <br><br>
    
                A generalized multiplicity formula for arranging Z types of things in M spaces is
                $$W=\frac{M!}{\prod N_{i}!}=\frac{M!}{N_{1}!*N_{2}!*...*N_{Z}!}$$
                If there are only two possible types and M total and N of type 1, then \(N_2=M-N\) and it reduces to
                $$W=\frac{M!}{N!(M-N)!}$$
    
                <h3>Definitions of Entropy</h3>
                Entropy is typically described as being a measure of "disorder," but how does that manifest? 
                With words, we can describe entropy for now as a measure of how mixed a system is or how many microstates a given macrostate has. 
                <br><br>
    
                There are 3 main formulas that we will be considering:
                $$S=k_B\ln(W)$$
                $$S=-k_B\sum p\ln(p)$$
                $$dS=\frac{dq}{T}$$
                These will be called the statistical definition, the probabilistic definition, and thermodynamic definition respectively. 
    
    
                <h3>Extensive and Intensive Properties</h3>
                A property of a system is said to be <b>extensive</b> if it is proportional to the size of the system. One example is mass; if the system is 1L of water, going to 2L of water will double the mass. These properties are often not ratios and can often be intuited via thought experiment. 
    <br><br>
                A property of a system is said to be <b>intensive</b> if it remains constant when a system is scaled. Density is an example of an intensive property. In the previous example, the density of the system does not change when you go from 1L of water to 2L of water. Mass and volume of the system both double, which cancel out when you take the ratio to get density. 
    <br><br>
                Some properties are neither extensive not intensive if they scale but in a non-linear proportional way. 
                The most notable example is multiplicity. This can be shown if every number in the above calculation is doubled due to the factorial operator. 
                Entropy at large scales behaves like an extensive property, but at very small scales, this does not always hold. 
                
            
                <h3>Normal Distribution</h3>

                Also called a Gaussian or Bell-curve, the Normal distribution is an essential distribution to know. 
                It is parameterized by two values:
                <ul>
                    <li>The mean, \(\mu\), which states the center of the distribution</li>
                    <li>The variance, \(\sigma^2\), which describes the width of the distribution</li>
                </ul>

                Some conventions will parameterize it using the standard deviation rather than its variance. 
                These are equivalent as the standard deviation is the square root of the variance and there is a 1-to-1 mapping. 
                In this book, we will use the following notation to state that a variable X is distributed normally with mean, \(\mu\) and variance \(\sigma^2\):

                $$X\sim N(\mu,\sigma^2)$$

                Again, in some sources, the second parameter will be the standard deviation instead, but in this text, it will always be the variance unless stated otherwise.

                <br><br>

                This is a continuous probability distribution whose density is given by

                $$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

                If the normal distribution has parameters \(\mu=0\) and \(\sigma^2=1\) then we refer to it as a <b>standard normal distribution</b>.
                <br><br>

                <h4>Properties</h4>
                
                <ul>
                    <li>Has non-zero probability density everywhere</li>
                    <li>Symmetric</li>
                    <li>Antiderivative does not have a closed-form</li>
                </ul>

                <h3>Law of Large Numbers</h3>

                <h3>Central Limit Theorem</h3>



            </div>
            <div id="Practice Problems">
                
                <h2>Practice Problems</h2>

                <ol>
                    <li>The exponential distribution is given by \(f(x)=ae^{-ax}\) for \(x\geq 0\) and 0 for \(x \lt 0\). Determine the mean and variance.
                        The following identities may be useful:
                        <ol>
                            <li>\(\int xe^{-ax}dx=-\frac{e^{-ax}(ax+1)}{a^2}+c\)</li>
                            <li>\(\int x^2e^{-ax}dx=-\frac{e^{-ax}(a^2x^2+2ax+2)}{a^3}+c\)</li>
                        </ol>
                    </li>

                </ol>
            </div>

        </div>
        <br>
        <br>
</body>
</html>