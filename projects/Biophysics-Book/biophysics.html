<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RDM65P2C47"></script>
        <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SYJL7ZYKXB');
    </script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-RDM65P2C47');
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Biophysics E-book</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Biophysics chemistry textbook">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Biophysical chemistry textbook">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
            
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        </script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script>

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/index.html">Home</a>|
            <a href="https://johndellarosa.github.io/resume.html">Resume</a>
            <!-- <a href="./index.html#education">Education</a>
            <a href="./index.html#experience">Experience</a>
            <a href="./index.html#skills">Skills</a> -->
            |
            <a href="https://johndellarosa.github.io//biography.html">About</a>|
            <a href="https://johndellarosa.github.io//projects.html">Projects</a>|
            <a href="https://johndellarosa.github.io//Miscellaneous.html">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
<h1>Biophysical Chemistry Textbook (Work in Progress)</h1>
<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">

            <div id="Table-of-Contents">
                <ol>
                    <li><a href="#Introduction">Introduction</a></li>
                    <li><a href="#Probability">Probability, Statistics, & Entropy</a></li>
                    <li><a href="#Thermodynamics">Thermodynamics</a></li>
                    <li><a href="#Advanced-Thermodynamics">Advanced Thermodynamics</a></li>
                    <li><a href="#Biochemistry">Biochemistry</a></li>
                    <li><a href="#Kinetics">Kinetics</a></li>
                    <li><a href="#Stochastic-Processes">Stochastic Processes</a></li>
                
                
                
                </ol>


            </div>


            <div id="Introduction">
            <h2>Introduction</h2>
            <h3>Who this text is for</h3>
            This text is aimed at students at junior level or higher who are interested in biophysical chemistry or want a free supplemental text on the subject.
            <h3>Requirements</h3>
            <h4>Math</h4>
            <ul>
            <li>This textbook will assume knowledge of single variable calculus.</li> 
            <li>Multivariable calculus will be used for derivations, especially in the physics-focused chapters.</li> 
            <li>Properties of logarithms and exponentiatials will be used</li> 
            <li>Differential equations will appear, but will not require understanding for how to solve, merely recognizing what the equation mean. </li>
            <li>Linear Algebra will be used in the some of the later probability-focused chapters, such as Markov Chains</li>
            <li>Basics of probability and statistics will be assumed</li>
        </ul>
            <h4>Physics</h4>
            <ul>
                <li>
                Not much physics is required, just basic understanding the concept of energy, kinetic energy, potential energy, force, etc.
            </li>
            </ul>

            <h4>Chemistry</h4>
            <ul>
                <li>General chemmistry knowledge will be assumed</li>
                <li>Basics of organic chemistry will be assumed.
                    <ul>
                        <li>Ability to understand shorthand for structures</li>
                        <li>Nucleophile, electrophile concepts</li>
                    </ul>
                </li>
            </ul>


            <h4>Biology</h4>
            <ul>
                <li>Familiarity with central dogma of biology</li>
                <li>Basic knowledge of nucleic acids</li>
                <li>Basic knowledge of proteins</li>
            </ul>
            <h4>Programming</h4>
            Programming knowledge is <b>not</b> a prerequisite for understanding the material in the book. <br><br>
            In order to generate many of the figures, I have used programming, mainly Python. Code will be sometimes be displayed if you wish to run simulations yourself similar to the figures; however, it is not crucial. 
            </div>
            <div id="Probability">
            <h2>Probability, Statistics, and Entropy</h2>
            <h3>Probability</h3>
            <h4>Defining terms</h4>
            Before we show any equations, we must define some terms. Let A and B be events.

            <ul>
                <li>P(A) is the probability of event A occurring</li>
                <li>\(P(A\cup B)\) is the probability of A or B or both occurring. The \(\lor\) can also be seen instead of \(\cup\)</li>
                <li>\(P(A\cap B)\) is the probability of A <b>and</b> B both occurring. The \(\land\) can also be seen instead of \(\cap\)</li>
                <li>\(P(\neg A)\) is the probability of A not occurring. This can also be written as a bar above the A</li>
            </ul>

            <h4>De Morgan's Laws</h4>
            De Morgan's laws describe how negation interactions with grouping of events. The statements can be rather formal, but verbally seem obvious. 

            "If neither A nor B happened, then that means A didn't happen and B didn't happen"
            $$\neg(A \lor B)=(\neg A)\land (\neg B)$$

            "If A and B didn't both happen, then that means either A didn't happen, B didn't happen, or neither happened"
            $$\neg(A \land B)=(\neg A)\lor (\neg B)$$


            <h4>Conditional Probability</h4>
            Let A and B be two events. We denote the conditional probability of A occurring given B having occurred as \(P(A|B)\)

            This is defined mathematically as:

            $$P(A|B)\equiv \frac{P(A \cap B)}{P(B)}$$
            where \(P(A\cap B)\) is the probability of both A and B occurring.
            


            <h4>Bayes' Theorem</h4>
            Bayes' Theorem relates two events and the conditional probabilities of one upon another. 
            Let A and B two events with non-zero probabilities
            $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$



            <h3>Entropy</h3>
            <h4>Multiplicity</h4>
            A generalized multiplicity formula for arranging Z types of things in M spaces is
            $$W=\frac{M!}{\prod N_{i}!}=\frac{M!}{N_{1}!*N_{2}!*...*N_{Z}!}$$
            If there are only two possible types and M total and N of type 1, then \(N_2=M-N\) and it reduces to
            $$W=\frac{M!}{N!(M-N)!}$$
            <h4>Extensive and Intensive Properties</h4>
            A property of a system is said to be <b>extensive</b> if it is proportional to the size of the system. One example is mass; if the system is 1L of water, going to 2L of water will double the mass. These properties are often not ratios and can often be intuited via thought experiment. 
<br><br>
            A property of a system is said to be <b>intensive</b> if it remains constant when a system is scaled. Density is an example of an intensive property. In the previous example, the density of the system does not change when you go from 1L of water to 2L of water. Mass and volume of the system both double, which cancel out when you take the ratio to get density. 
<br><br>
            Some properties are neither extensive not intensive if they scale but in a non-linear proportional way. 
            The most notable example is multiplicity. This can be shown if every number in the above calculation is doubled due to the factorial operator. 
            Entropy at large scales behaves like an extensive property, but at very small scales, this does not always hold. 
            </div>
            <div id="Thermodynamics">
            <h2>Introductory Thermodynamics</h2>
            <h3>Laws of Thermodynamics</h3>
            <h4>First Law of Thermodynamics</h4>

            The first law can be formulated in different ways. A common source of confusion for students is the seemingly inconsistent formula. 
<br><br>
            Variant 1:
            $$\Delta U=q+w$$
            Variant 2:
            $$\Delta U=q-w$$
            So why will you see both? This is due to differences between authors in how they define the quantities in the equation. 

            Let's start by defining q, which we will call "heat," since this is less controversial. q will be the heat added to the system. Heat flowing (positive q) into the system will increase the amount of internal energy intuitively, so a positive q will contribute to an increase in energy. 
            Heat flowing out of the system (negative q) will decrease the amount of internal energy.
<br><br>
            Now, for defining w, which is the "work" term. Here are the two equations again with their definition
            <ol>
                <li>\(\Delta E=q+w\), positive w here is defined as work done <b>to</b> the system and negative w is defined as work done <b>by</b> the system</li>
                <li>\(\Delta E=q-w\), positive w here is defined as work done <b>by</b> the system and negative w is defined as work done <b>to</b> the system</li>
            </ol>

            Regardless of your definition of w, if work is being performed on the system, energy is being transferred to it, but if the system is expending energy to do work, then it will lose energy. 
            From here on out, the textbook will use variant 1.
            <br><br>
            Now, the terms work and heat have not been preicsely defined yet. 
            Working definitions for the terms at this point will be:
            <ul>
                <li>Work is controlled, harnessed transfer of energy</li>
                <li>Heat is uncontrolled transfer of energy</li>
            </ul>

            Work can take various forms, but the ones we will be considering will be 
            <ol>
                <li>Expansion work or "pV work," such as a piston moving due to a pressure difference.
                </li>
                <li>Chemical work, which harnesses energy from chemical reactions</li>
            </ol>

            There is a differential form of the equation which is expressed as:

            $$dE=\delta q + \delta w$$

            The heat and work variables have \(\delta\) instead because they are <b>inexact differentials</b>. That is to say, they have path-dependent integration. 
            While the change in energy will be the same, the contribution from work vs heat depends on how the process is done. 

            <h4>Second Law of Thermodynamics</h4>
            The second law of thermodynamics states that total entropy is non-decreasing over time. 
            $$\Delta S_{universe}\geq 0$$
            This is sometimes put into formula by breaking down the universe into the "system" or thing of interest and the "surroundings" or everything else. 
            $$\Delta S_{sys}+\Delta S_{surr} \geq 0$$
            A common mistake that students make is interpreting it as saying that entropy cannot decrease locally. 
            This is <b>not</b> true. Entropy can very well decrease in a system, but it will be offset by a greater increase in the surroundings. 
            In fact, much of biology involves this very trade-off.

            <h3>Energy</h3>
            <h4>Forms of Energy</h4>
            Energy can take different forms. The two main categories are Kinetic and Potential energy. 
            As its name suggests, kinetic energy is associated with movement. Here are 3 forms which we will consider throughout the book:
            <ol>
                <li>Translational: a molecule's movement through space</li>
                <li>Rotational: a molecule rotating about some axis</li>
                <li>Vibrational: bonded atoms vibrating</li>
            </ol>
            Potential energy is associated with position rather than movement. This can also take several forms, including but not limited to:
            <ol>
                <li>Chemical bonds</li>
                <li>Gravitational</li>
                <li>Concentration gradients</li>
                <li>Electrostatic interactions</li>
                <li>Compressed spring</li>
            </ol>
            <h4>Physics Equations</h4>
            As one formulation of the First Law states, there is conservation of energy with no input or removal from the system. 
            However, energy can be converted from one form to another. Imagine a ball rolling off of a table; its gravitational potential energy gets converted into kinetic energy, sending it downward. 
            U sometimes refers to potential energy, but also internal energy in general. In this section, U will refer to <b>Potential energy</b> and E will refer to <b>Total energy</b>.
            
            <br><br>
            In this section, positive work will refer to work <b>done</b> by the system which is opposite what we had before, but it is common convention. 

            $$W=-\Delta U$$

            How is are work required and potential energy related to forces?

            $$W=\int F\cdot dx$$

            The work done is the integral of the force, F, applied over the distance. This force may be a function of the position, x. 

            <h4>Hooke's Law</h4>
            Hooke's law describes the restoring force in a spring. In plain English, Hooke's law states that the restoring force a spring has is proportional to the displacement from the rest length.
            $$F=-k(x-x_0)$$
            where x is the length of the spring, \(x_0\) is the rest length of the spring, k is the spring constant, and F is the force. The sign is negative as the force counteracts the stretching and applies a force in the opposite direction. 
            We can also call \(x-x_0\) to be simply x by having our coordinate system have x_0=0; longer is positive x; compressed is negative x.

            Using our formula for work, we can calculate the work required to stretch a spring.
            $$W=\int_0^z (-k x) \cdot dx$$
            Because the force is in the opposite direction as the dx vector, we get an additional negative sign which cancels things out. 
            $$W=\int_0^z(kx)dx$$
            $$=\frac{1}{2}kz^2$$
            The work required is \(\frac{1}{2}kz^2\) which will be converted into potential energy.

            <h4>Electrostatics</h4>
            Another common force is electrostatic. The electrostatic force between two point charges is described by Coulomb's Law. 
            $$F=\frac{1}{4\pi D\varepsilon_0}\frac{q_1q_2}{r^2}$$
            where F is the force, D is the dielectric constant for the medium, \(\varepsilon_0\) is the vacuum permittivity (a constant), \(q_i\) is the charge of particle i, and r is the distance between the two charged particles. 
            In this form of Coulomb's Law, a negative value for F means an attractive force and a positive F means a repulsive force. 

            <h4>Morse Potential</h4>
            The harmonic oscillator/Hooke's law model is treating a molecule as if it were two nuclei connected by a spring in a classical sense. The potential energy is 
            $$U=\frac{1}{2}k(r-r_e)^2$$
            where U is your potential energy of the spring, k is your spring constant, r is the distance between the atoms and \(r_e\) is the equilibrium bond distance. So this energy function looks like a parabola with a minimum at \(r=r_e\).
<br><br>

            This model is pretty decent at describing behavior close to the equilibrium bond distance, but as r gets further away from \(r_e\), we need a better model. This is where the Morse potential comes in. This is an improved version which takes into account things like nuclear-nuclear repulsion from Pauli's exclusion principle and bond dissociation. When r is too small, the Morse potential jumps in potential energy quicker than with a simple Hooke's law approximation. Bonds can't stretch infinitely; bonds break if separation grows too large. The Morse potential takes this into account by having U asymptotically approach a value in the limit as r goes to infinity instead of continuing to increase quadratically. The limit as r approaches infinity is your bond dissociation energy.
The equation for the morse potential is given by 
$$U(r)=D_e (1-e^{-a(r-r_e)})^2$$
where \(D_e\) is related to the dissociation energy. 


<h4>Lennard-Jones Potential</h4>
The Lennard-Jones potential describes Van der Waals interactions between two molecules. 
The potential is given by

$$U(r)=4\varepsilon \left[(\frac{\sigma}{r})^{12}-(\frac{\sigma}{r})^6\right]$$


            </div>
            <div id="Advanced-Thermodynamics">
                <h2>Advanced Thermodynamics</h2>
                <h3>Introduction</h3>
                This section will delve deeper into thermodynamics. Multivariable calculus will be used to expand upon topics covered earlier as well as introduce new topics. 
                The <b>Fundamental Thermodynamic Relation</b> will be covered as an extension of the first law.
                <h3>Laws of Thermodynamics Revisited</h3>
            <h4>Total derivatives</h4>

            While the derivation is outside the scope of this textbook, the first law can be expressed as the following total derivative:
            $$dE=(\frac{\partial E}{\partial S})_{V,N}dS+(\frac{\partial E}{\partial V})_{S,N}dV+\sum(\frac{\partial E}{\partial N_i})_{S,V}dN_i$$
            $$=TdS-p_{ext}dV+\sum\mu_idN_i$$
            where T is temperature, \(p_{ext}\) is external pressure, V is volume, \(\mu_i\) is the chemical potential for a chemical species i, and \(N_i\) is the number of species i

            <h4>Temperature and Entropy</h4>
            From the Fundamental Thermodynamic Relation, we have:
            $$(\frac{\partial S}{\partial E})_{N,V}=\frac{1}{T}$$
            If \(T_{A}>T_{B}\), then \(\frac{1}{T_{A}}<\frac{1}{T_{B}}\)
            $$(\frac{\partial S_{A}}{\partial E_{A}})_{N,V}<(\frac{\partial S_{B}}{\partial E_{B}})_{N,V}$$
            by substituting in the thermodynamic definition for temperature, we see that the lower temperature system, B, would have a greater change in entropy for the same change in energy. This means that there would be a net entropy increase if energy were to flow from A to B. 
            So when would this heat flow stop? When entropy is maximized/equilibrium. The termination condition would be when there is no net increase in entropy due to heat flow or in mathematical terms:
            $$(\frac{\partial S_{A}}{\partial E_{A}})_{N,V}=(\frac{\partial S_{B}}{\partial E_{B}})_{N,V}$$
            which is equivalent to saying
            $$\frac{1}{T_{A}}=\frac{1}{T_{B}}$$
            or alternatively
            $$T_{A}=T_{B}$$

            In plain English to summarize: For the same amount of energy added to a system, the one with the lower temperature would increase in entropy more. 
            Conversely, for the same amount of energy taken out of a system, the one with a higher entropy would decrease in energy less. Therefore, if you take energy from the higher temperature system and put it into the lower temperature system, there will be a net increase in entropy. 
            This will change the temperatures of the systems, bringing them closer together. This exchange in energy will stop when the temperatures are the same, or when there is no longer a possible increase in energy from taking energy from one system and putting it in another.



            <h3>Enthalpy and the Thermodynamic Definition of Entropy</h3>
            <h4> Deriving the Thermodynamic Definition of Entropy</h4>
            By the differential form of the first law of Thermodynamics, we have
            $$dE=(\frac{\partial E}{\partial S})_{V,N}dS+(\frac{\partial E}{\partial V})_{S,N}dV+\sum(\frac{\partial E}{\partial N_i})dN_i$$
            $$=TdS-p_{ext}dV+\sum\mu_idN_i$$
            *Negative sign for pressure since our dV is pointing in opposite direction of \(p_{ext}\).
            Let us define a quantity called Enthalpy, denoted by H, as follows:
            $$H\equiv E+PV$$
            Through the <a href="https://en.wikipedia.org/wiki/Product_rule">product rule</a> for differentiation:
            $$dH=dE+d(PV)=dE+(pdV+Vdp+dVdp)$$
            Substituting in the total differential form of the first law from above for dU:
            $$=(TdS-pdV+\sum\mu_idN_i)+pdV+Vdp+dVdp$$
            The dVdp term goes to 0 in the limit. The pdV term from work cancels out with one of the terms from the product rule for d(PV).
            $$=TdS+Vdp\sum\mu_idN_i$$
            If we are working in a constant temperature environment and no chemical reactions are taking place, we can let dp=0 and \(dN_i=0\forall i\), yielding the formula
            $$dH=TdS=\delta q$$

            $$dS=\frac{\delta q_{rev}}{T}$$
            This is the so-called "Thermodynamic definition of Entropy" which holds for a reversible process at constant pressure<br>


<h4>Irreversible Processes</h4>

            However, what about for irreversible processes? The first law still holds such that 
            $$U=q+w$$
            but
            \(q_{rev}\leq q_{irr}\) 
            and 
            \(w_{rev}\geq w_{irr}\).
            A more general statement which holds for irreversible processes is
            $$dS\geq \frac{\delta Q}{T}$$
            with equality when the process is reversible. This is known as the Clausius Inequality, which is another way of formulating the 2nd law.<br>
            
            <h4>An Aside on Legendre Transforms</h4>
            In the first law, we had
            $$dU=TdS-pdV+\sum\mu_idN_i$$
            and when we defined Enthalpy as \(H\equiv U+PV\), we ended up getting
            $$dH=TdS+Vdp+\sum\mu_idN_i$$
            By adding PV, we were able to switch the positions of the Volume and Pressure variables (along with a sign change). 
            By doing this, we can now have a quantity that is more convenient to work with when we assume different conditions (e.g. constant pressure instead of constant volume). 
            This is from which we got heat being the change in energy at a fixed volume and the change in enthalpy at fixed pressure equivalencies (set dV=0 or dP=0 in the above equations). 
        <br><br>
            This is a theme that will recur for the Helmholtz and Gibbs free energies where instead we will switch temperature and entropy, as dT=0 is more feasible than dS=0. 
            These paired variables are known as "conjugate variables," and the process of switching the differential between the extensive and intensive variable is known as a Legendre transformation.

            <h4>Helmholtz Free Energy and Gibbs Free Energy</h4>

            Let us define a quantity Helmholtz Free Energy, denoted by A (The letter A was chosen for the German word for work as H was already being used for Enthalpy; some books use F for free energy):
            $$A\equiv U-TS$$
            Doing a similar trick as we did for Enthalpy - taking the derivative of both sides and using the linearity of the derivative operator:
            $$dA=dU-d(TS)$$
            Substituting in for dU using the first law; product rule for d(TS)
            $$dA=(TdS-pdV+\sum\mu_idN_i)-(TdS+SdT+dSdT)$$
            Canceling out TdS terms and dSdT goes to 0
            $$dA=-pdV-SdT+\sum\mu_idN_i$$

            If we are working in a constant temperature system (dT=0) and constant Volume, then we find that the change in Helmholtz free energy is the amount of chemical work being done. 
            <br><br>
            It is also possible to do what we did for Enthalpy and Helmholtz for a single quantity. 
            Let us define a quantity Gibbs Free Energy, denoted by G:

            $$G\equiv U+PV-TS=A+PV=H-TS$$
            In general chemistry, Gibbs free energy is often introduced as
            $$G=H-TS$$
            so the derivation will go from that point.

            $$dG=dH-d(TS)$$
            Substituting in our above result for dH; product rule for d(TS)
            $$dG=(TdS+Vdp+\sum\mu_idN_i)-(TdS+SdT+dSdT)$$
            Cancelling out TdS terms; dSdT goes to 0
            $$dG=Vdp-SdT+\sum\mu_idN_i$$

            If we are working in a constant temperature system and constant pressure, then the change in Gibbs free energy is the amount of chemical work. 


        </div>
        <div id="Statistical-Mechanics">
            <h2>Statistical Mechanics</h2>
            <h3>Boltzmann Distribution</h3>

            A system at equilibrium obeys what is known as a <b>Boltzmann Distribution</b>. 
            This distribution maximizes <b>total entropy</b>, not the entropy of the system. 




        </div>
        <div id="Biochemistry">
            <h2>Biochemistry</h2>
            <h3>Acid-Base Chemistry</h3>
            <h4>Definitions of Acid and Base</h4>
            There are 3 main basic definitions of acid-base chemistry:
            <ul>
                <li>Arrhenius: increases concentration of H+ (acid) or OH- (base)</li>
                <li>Bronsted-Lowry: proton donor (acid) and proton acceptor (base)</li>
                <li>Lewis: electron pair acceptor (acid) and electron pair donor (base)</li>
            </ul>
            The Arrhenius definition has been supplanted by the Bronsted-Lowry one, as it's more general. 
            The Lewis definition is also very important and its acids and bases are also referred to as electrophiles and nucleophiles respectively. 
            For clarity, future uses of the words "acid" and "base" will refer to the Bronsted-Lowry definition; Lewis acids and bases will be referred to by their aforementioned organic chemistry terms.

            <h4>Equilibrium Constants</h4>

            To be filled later

            <h4>Henderson-Hasselbach Equation and pKa</h4>

            Let us start with a generic acid dissociation equation from before with an acid dissociation constant \(K_A\):

            $$HA+H_2O\rightleftharpoons A^-+H_3O^+$$

            The associated equilibrium equation is 

            $$K_A=\frac{[A^-][H_3O^+]}{HA}$$
            There are various ways you can algebraically manipulate the equation and take logs in order to get to the final equation, but I will get things on the right side prior to taking the log
            $$\frac{1}{[H_3O^+]}=\frac{1}{K_A}\times\frac{[A^-]}{[HA]}$$
            Taking the log of both sides.
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A}\times\frac{[A^-]}{[HA]})$$
            Using the multiplication and division rule for the argument:
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A})+\log(\frac{[A^-]}{[HA]})$$
            Using a specific case of the log power rule, there is the relationship \(\log(1/a)=-\log(a)\), we get
            $$-\log([H^+])=-\log(K_A)+\log(\frac{[A^-]}{[HA]})$$
            By definition, \(pKa\equiv -\log(K_A)\) and \(pH\equiv -\log([H^+])\). Thus, we arrive at the Henderson-Hasselbach equation:
            $$pH=pKa+\log(\frac{[A^-]}{[HA]})$$

            This equation allows us to relate the pH of an environment, the pKa of an acid, and the ratio of deprotonated to protonated occurences of that acid

            

            <h4>Amino Acids and Acid-Base Chemistry</h4>
            As amino acids have both amine and carboxylic acid groups, they can all act as both acids and bases to some extent. 
            In fact, at a neutral pH, amino acids will be predominantly Zwitterions (molecules that have both positive and negative charges); the amine group next to the alpha carbon will be protonated and the carboxylic acid group next to the alpha carbon will be deprotonated. 
            <br>
            However, a number of the proteogenic amino acids can also participate in acid-base chemistry through their sidechains. 

            <p style="width: 90%; text-align: center; margin: auto;"><a href="https://commons.wikimedia.org/wiki/File:Proteinogenic_Amino_Acid_Table.png#/media/File:Proteinogenic_Amino_Acid_Table.png"><img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Proteinogenic_Amino_Acid_Table.png" alt="Proteinogenic Amino Acid Table.png" style="max-width: 100%;"></a><br>By <a href="https://commons.wikimedia.org/w/index.php?title=User:Thomas.ryckmans68&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="User:Thomas.ryckmans68 (page does not exist)">Thomas.ryckmans68 <span class="int-own-work" lang="en"&gt;Own work&lt;/span&gt;, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=117084381">Link</a></p>
            <br>
            <br>

            One might ask - why do the amide groups in Asparagine and Glutamine not participate in acid-base chemistry? 
            The answer has to do with conjugated pi systems. The lone pair on the nitrogen is not "available" to grab a proton because it prefers to be in a conjugated system with the carbonyl. 
            For the same reason, in proteins, you will not see the amide backbone being charged, only the N and C termini.

        </div>

        <div id="Kinetics">
            <h2>Kinetics</h2>
            <h3>Refresher on Kinetics</h3>
                <h4>Elementary Reactions</h4>

                Let us consider elementary reactions - that is they have no intermediate steps:
                Let A, B be the reactants and P be the product (not Phosphorus). 



                For <b>single step</b> unidirectional reactions, we can write an equation that describes the rate of change in chemical species:

                $$A\rightarrow P$$

                Instinctively, if this is the only reaction taking place, the concentration of A should be going down by the same rate by which the concentration of P goes up.


                $$\frac{d[A]}{dt}=-\frac{d[P]}{dt}$$

                It would also be intuitive to think that the higher the concentration of A, the faster the reaction would go. 
                Probabilitistically, a justification would be that each A molecule has a chance per unit time of converting to P. 
                Then if each conversion is independent, you would expect the rate of conversion to be directly proportional to the number of A. 
                Thus, we also get the relationship.

                $$\frac{d[P]}{dt}=k[A]$$

                where k is a constant of proportionality which is specific to the reaction. We will call this the <b>rate constant</b>.

                Let us now consider a bimolecular, <b>single-step</b> unidirectional reaction:

                $$A+B\rightarrow P$$

                By similar logic, we can write the following relationships:

                $$\frac{d[A]}{dt}=\frac{d[B]}{dt}=-\frac{d[P]}{dt}$$

                $$\frac{d[P]}{dt}=k[A][B]$$

                <h4>Multiple Step Reactions</h4>


                <h4>Relationship with Equilibrium Constant</h4>

                <h4>Integrated Rate Laws</h4>

                This section will involve solving a few simple differential equations. 
                As thsi section is more concerned with the results and logic rather than the math, the derivations will not be especially rigorous and will be playing fast and loose with some math.


                Let us consider the simplest case, a reaction that has a rate-limiting step that is independent of concentration.

                $$\frac{d[P]}{dt}=k$$
                
                If the mathematicians aren't watching, you can "multiply" by dt to get

                $$d[P]=kdt$$
                While not good notation, we can itegrate from time t=0 to a a generic time t=t.

                $$\int_0^td[P]=\int_0^tkdt$$
                $$[P]_t-[P]_0=kt$$

                We would expect this as it is a constant rate process, so it should just be the rate multiplied by the amount of time.

                <br><br>
                Now for a more realistic example.

                Consider a reaction with the rate law:

                $$\frac{d[A]}{dt}=-k[A]$$

                Again we will want each variable to be on its own side. Using some algebra and abuse of notation, we get

                $$\frac{d[A]}{[A]}=-kdt$$

                We can treat it as if we were integrating each side seperately.

                $$\int \frac{d[A]}{[A]}=\int-kdt$$

                While it may appear intimidating, we are essentially just integrating \(\frac{1}{x}\) and a constant.

                $$ln([A]_t)-ln([A]_0)=-kt$$

                In these sorts of problems, once we integrate, we then try to isolate the concentration at time t.

                $$ln([A]_t)=ln([A]_0)-kt$$
                Exponentiating things, we get
                $$[A]_t=[A]_0e^{-kt}$$
                This is the <b>first order integrated rate law</b>
                Now, we have an equation that describes the concentration over time, not just the rate.

                For completeness, consider a rate law corresponding to a reaction with the rate law:

                $$\frac{d[A]}{dt}=-k[A]^2$$

                $$\frac{d[A]}{[A]^2}=-kdt$$

                $$\int\frac{d[A]}{[A]^2}=\int-kdt$$

                $$\frac{-1}{[A]_t}-\frac{-1}{[A]_0}=-kt$$
                $$\frac{1}{[A]_t}=\frac{1}{[A]_0}+kt$$
                $$[A]_t=\frac{1}{\frac{1}{[A]_0}+kt}$$

                <img src="Rate_Law_Comparison.png" style="margin:auto; display: block;  width:clamp(300px,30vw, 600px)"/>

                So far, the integrated rate laws have been univariate. Differential equations with multiple variables are possible and are called "Partial Differential Equations" or PDEs. 

            <h3>Michaelis-Menten Kinetics</h3>
            <h4>Binding and Unbinding</h4>

            Let us picture a an enzymatic reaction. We have a substrate, S, which binds to an Enzyme, E, resulting in a bound complex, \(E\cdot S\). After some amount of time, the substrate is transformed into product, P, and the enzyme unbinds. 
            We will have the formation of the Enzyme-substrate complex be reversible, but consider the product formation to be irreversible.

            $$S+E\rightleftharpoons_{k_{-1}}^{k_1} E\cdot S \rightarrow^{k_{cat}} E+P$$

            

            <h4>Derivation</h4>

            There is a fixed amount of enzyme - either the enzyme is bound or unbound. Let us call the total amount of Enzyme, \(E_0\)

            $$E_0=[E]+[E\cdot S]$$
            We can write general, complex rate laws by looking at the flux into and out of each state
            $$\frac{d[X]}{dt}=\sum R_i-\sum R_{j}$$
            where R_i is the rate for a reaction step that produces X and R_j is the rate for a reaction step that depletes X. 

            $$\frac{d[S]}{dt}=k_{-1}[E\cdot S]-k_1[E][S]$$
            $$\frac{d[E]}{dt}=k_{-1}[E\cdot S]+k_{cat}[E\cdot S]-k_{1}[E][S]$$
            $$=(k_{-1}+k_{cat})[E\cdot S]-k_1[E][S]$$
            $$\frac{d[P]}{dt}=k_{cat}[E\cdot S]$$
            $$\frac{d[E\cdot S]}{dt}=k_1[E][S]-k_{-1}[E\cdot S]-k_{cat}[E\cdot S]$$

            A "steady-state approximation" is then made; that is, we treat the intermediate complex's concentration as a constant. 
            $$\text{Let }\frac{d[E\cdot S]}{dt}=0$$
            Using our above equations, we can then get
            $$k_{1}[E][S]=(k_{-1}+k_{cat})[E\cdot S]$$
            Since we are ultimately interested in the rate of product formation, we should look to that equation for a possible next step. 
            $$\frac{d[P]}{dt}=k_{cat}[E\cdot S]$$
            Since we see that there is \([E\cdot S]\), we should manipulate the previous equation to find \([E\cdot S]\) in terms of [E] and [S], especially since we do not directly know the concentration of the enzyme-substrate complex. We know how much substrate and enzyme there are initially. We also do not necessarily know how much unbound enzyme there is either as that would also require knowledge of how much bound enzyme there was. So let's try to reduce it from 2 unknowns to 1 unknown. Another assumption that we will make is that \([S]\gg E_0\) since technically \(S_0=[S]+[E\cdot S]\), but we shall say \([S]+[E\cdot S]\approx [S]\). 
            By using our total enzyme equation above, we can get
            $$k_{1}(E_0-[E\cdot S])[S]=(k_{-1}+k_{cat})[E\cdot S]$$
            Let's now algebraically manipulate the equation to isolate \(E\cdot S\)..
            $$k_{1}E_0[S]-k_{1}[E\cdot S][S]=(k_{-1}+k_{cat})[E\cdot S]$$
            $$k_{1}E_0[S]=(k_{1}[S]+(k_{-1}+k_{cat}))[E\cdot S]$$
            $$[E\cdot S]=\frac{k_1E_0[S]}{k_1[S]+(k_{-1}+k_{cat})}$$
            For simplicity, we will divide the numerator and denominator of the right side by \(k_1\)
            $$[E\cdot S]=\frac{E_0[S]}{\frac{k_{-1}+k_2}{k_1}+[S]}$$
            We can then plug \(E\cdot S\) in to the rate of product formation equation
            $$\frac{dP}{dt}=k_{cat}[E\cdot S]$$
            $$=\frac{k_{cat}E_0[S]}{\frac{k_{-1}+k_2}{k_1}+[S]}$$

            While this result may look unruly, it does tell us some nice properties. 
            Let us define a constant \(k_M\)
            $$k_M\equiv\frac{k_{-1}+k_{cat}}{k_1}$$
            $$\frac{dP}{dt}=\frac{k_{cat}E_0[S]}{k_M+[S]}$$
            When the susbtrate concentration is equal to \(k_M\), the rate of production is at 50% of max rate.

            $$V_{max}\equiv k_{cat}E_0$$

            $$v=\frac{V_{max}[S]}{k_M+[S]}$$

            <img src="MichaelisMentenGraph.png" style="margin:auto; display: block;  width:clamp(400px,50vw, 1000px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
k_M = 4e-4
k_cat = 10
E_0 = 1e-12
S_array = np.geomspace(1e-5,1e-2,100)
v_array = k_cat*E_0*S_array/(k_M+S_array)
noise = np.random.normal(size=(100))*5e-14
v_array = v_array + noise
plt.figure(figsize=(14,8))
plt.scatter(S_array, v_array,s=5)
plt.title("Michaelis-Menten plot")
plt.xlabel("Substrate Concentration (M)")
plt.ylabel(f"$v_0$ (M/s)")
plt.xticks([i*5e-4 for i in range(0,21)])
plt.show()</pre>
            </code>

            As shown in the generated data above, the value of [S] at which the velocity is half of the asymptotic max is roughly 40 mM. 

            <h5>Lineweaver-Burk plots</h5>

            Since trying to estimate the half-max of a curve can be difficult, we might choose to linearize the data and use regression to find the best fit line and calculate the parameters from that. 
            One common method is Lineweaver-Burk plots, which are also called double-recipricol plots. As the name suggests, 1/[S] is plotted against 1/v. 
            The y-intercept is v_max. This is evident since as [S] goes to infinity, 1/[S] goes to 0. The slope is \(k_M/V_\text{max}\). The x-intercept is \(-1/k_M\). 

            <img src="LineweaverBurkPlot.png" style="margin:auto; display: block;  width:clamp(400px,50vw, 1000px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
k_M = 4e-4
k_cat = 10
E_0 = 1e-12
S_array = np.geomspace(1e-5,1e-2,100)
v_array = k_cat*E_0*S_array/(k_M+S_array)
noise = np.random.normal(size=(100))*5e-14
v_array = v_array + noise
S_array_inverse = 1/S_array
v_array_inverse = 1/v_array
plt.figure(figsize=(14,8))
plt.scatter(S_array_inverse, v_array_inverse,s=5)
model = sm.OLS(v_array_inverse, sm.add_constant(S_array_inverse))
results = model.fit()
plt.plot(S_array_inverse, results.predict(sm.add_constant(S_array_inverse)),color='orange')
plt.title("Lineweaver-Burk plot")
plt.xlabel("1/[S] ($M^{-1}$)")
plt.ylabel(f"$1/v_0$ (s/M)")
plt.legend(['Data',f'OLS: y={results.params[0]}x+{results.params[1]}'])
plt.show()</pre>
            </code>

            One thing to note is the heteroskedasticity in the graph. Ordinary Least Squares (OLS) is the best, unbiased linear model when the Gauss-Markov conditions are satisfied - one of which is homoskedasticity (variance of errors is constant over X). 
            In practice, this can lead to bad estimates as the noisy values on the right can drastically affect the best-fit parameters.

<h3>Inhibition</h3>

<figure><a href="https://commons.wikimedia.org/wiki/File:Inhibition-types.jpg#/media/File:Inhibition-types.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/d/dc/Inhibition-types.jpg" alt="Inhibition-types.jpg" style="max-height: 300px;"></a><br>By <a href="//commons.wikimedia.org/w/index.php?title=User:Athel_cb&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="User:Athel cb (page does not exist)">Athel cb</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=130486959">Link</a></figure>

            <h4>Competitive Inhibition</h4>
            We had previously just considered the scenario where the substrate had no competition for binding to the active site, but the equation can be extended easily to include the presence of another moelecule that can bind to the same active site as the substrate, but will not undergo the catalytic reaction. 

            Let us assume the same dynamics, but with the addition of a side reaction:
            $$S+E\rightleftharpoons_{k_{-1}}^{k_1} E\cdot S \rightarrow^{k_{cat}} E+P$$

            $$E+I\rightleftharpoons_{k_2}^{k_{-2}} E\cdot I$$
            Since there is a new state can consume E to enter and product E when it leaves, this will change our equation for \(\frac{d[E]}{dt}\) as well as create new equations for \(\frac{d[E\cdot I]}{dt}\) and \(\frac{d[I]}{dt}\). 
            We can denote the equilibrium constant for the association of the Enzyme-Inhibitor complex to be \(K_I\).

            $$K_I=\frac{k_{-2}}{k_2}$$
            $$\frac{[E][I]}{[E\cdot I]}=K_I\sim\frac{[E]I_0}{[Y]}$$
            $$E_0=[E]+[E\cdot S] +[E\cdot I]$$
            $$[E]=E_0-[E\cdot S]-[Y]$$
            $$[E]=E_0-[E\cdot S]-\frac{[E]I_0}{K_I}$$
            $$[E](1+\frac{I_0}{K_I})=E_0-[E\cdot S]$$



            $$[E]=\frac{E_0-[E\cdot S]}{1+\frac{I_0}{K_I}}$$
            Let us define a constant \(\alpha\) such that
            $$\alpha\equiv 1+\frac{I_0}{K_I}$$
            $$[E]=\frac{E_0-[E\cdot S]}{\alpha}$$

            $$0=\frac{d[E\cdot S]}{dt}=k_1[E][S]-k_{cat}[E\cdot S]-k_{-1}[E\cdot S]$$
            $$[E\cdot S](k_{cat}+k_{-1})=k_1[E][S]$$
            $$[E\cdot S]=\frac{1}{K_M}[E][S]$$
            $$\approx \frac{1}{k_M}[E]S_0$$
            $$[E\cdot S]_{ss}=\frac{S_0}{k_M}\times\frac{E_0-[E\cdot S]_{ss}}{\alpha}$$
            $$[E\cdot S]_{ss}(1+\frac{S_0}{k_M\alpha})=\frac{S_0E_0}{\alpha k_M}$$
            $$[E\cdot S]_{ss}=\frac{\frac{S_0E_0}{k_M\alpha}}{1+\frac{S_0}{k_M\alpha}}$$
            $$=\frac{E_0}{1+\frac{\alpha k_M}{S_0}}$$

            Now finally plugging in the concentration into the product formation kinetic equation:

            $$v_0=k_{cat}[E\cdot S]_{ss}=\frac{k_{cat}E_0}{1+\frac{\alpha k_M}{S_0}}$$

            <img src="LineweaverBurkInhib.png" style="margin:auto; display: block;  width:clamp(400px,50vw, 1000px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
k_M = 4e-4
k_cat = 10
E_0 = 1e-12
S_array = np.geomspace(5e-3,1e-2,200)
test_points = 1/np.array([-0.1,5e-3])

alpha = 1
v_array = k_cat*E_0/(1+k_cat*alpha/S_array)
noise = np.random.normal(size=(200))*.5e-16
v_array = v_array + noise
S_array_inverse = 1/S_array
v_array_inverse = 1/v_array
ax = plt.figure(figsize=(14,8))
plt.scatter(S_array_inverse, v_array_inverse,s=5)
model = sm.OLS(v_array_inverse, sm.add_constant(S_array_inverse))
results = model.fit()
plt.plot(test_points, results.predict(sm.add_constant(test_points)),color='green')

alpha = 1.5
v_array = k_cat*E_0/(1+k_cat*alpha/S_array)
noise = np.random.normal(size=(200))*.5e-16
v_array = v_array + noise
S_array_inverse = 1/S_array
v_array_inverse = 1/v_array
plt.scatter(S_array_inverse, v_array_inverse,s=5,color='red')
model2 = sm.OLS(v_array_inverse, sm.add_constant(S_array_inverse))
results2 = model2.fit()
plt.plot(test_points, results2.predict(sm.add_constant(test_points)),color='orange')


plt.title("Lineweaver-Burk plot")
plt.xlabel("1/[S] ($M^{-1}$)")
plt.ylabel(f"$1/v_0$ (s/M)")
plt.legend(['Data (No inhib)',f'OLS: y={results.params[0]:.2E}x+{results.params[1]}','Data (Inhib)', f'OLS: y={results2.params[0]}x+{results2.params[1]}'])
plt.show()</pre>
            </code>

While it is not apparent in the figure, as the inhibitor is added, the line rotates counter-clockwise, with the y-intercept remaining the same. 
The following are the effects of competitive inhibition:
<ol>
    <li>v_max stays the same</li>
    <li>k_M <b>increases</b> as more substrate is required to reach half max velocity</li>
</ol>

        <h4>Uncompetitive Inhibition</h4>

        Uncompetitive inhibition involves the following dynamics:

        $$S+E\rightleftharpoons_{k_{-1}}^{k_1} E\cdot S \rightarrow^{k_{cat}} E+P$$

        $$E\cdot S+I\rightleftharpoons_{k_2}^{k_{-2}} E\cdot S\cdot I$$

        With this form of inhibition, the inhibitor binds to the \(E\cdot S\) complex, not E, and prevents enzyme activity as well as substrate dissociation. In a Lineweaver-Burk plot, this would appear as a parallel shift downward. 

        The following are the effects of uncompetitive inhibition:
        <ol>
            <li>v_max decreases</li>
            <li>k_M <b>decreases</b></li>
        </ol>

        <h4>Non-competitive Inhibition</h4>
        While similar in name to uncompetitive, non-competitive inhibition is distinct from it. 
        Non-competitive inhibition does not involve the inhibitor binding to the active site, instead it binds to an "allosteric site." 
        The inhibitor can bind to both E and \(E\cdot S\). Similarly, the substrate is able to bind and unbind to both E and \(E\cdot I\). 
<br><br>
This has the effect of essentially reducing the amount of enzyme present, as if less had been added in the first place. 
The following are the effects of non-competitive inhibition:
<ol>
    <li>v_max decreases</li>
    <li>k_M is unaffected</li>
</ol>



        </div>

        <div id="Stochastic-Processes">
            <h2>Stochastic Processes</h2>
            <h3>Discrete Time Stochastic Processes</h3>

            <h4>Discrete-Time Markov Chains</h4>

            Imagine a system where a protein is in one of two states: A or E. Each minute you observe its state, and it has the ability to go from state A to E or E to A, but it does not necessarily switch. We will denote these observational periods at \(t=0, 1, \ldots\)
            <br><br>
            An important assumption that we will make is that only the current state determines the probability of switching conformations by the second; the state that it was in prior to this observation have <b>no</b> impact on the chances. 
            E.g. if we are at t=n, the state that the protein was in at t=n-1 has no impact on the probability for t=n+1. 
            This is called the <b>Markov property</b>.
            
            <br><br>
            
            For simplicity's state, we will assume that these observations are discrete events and not worry about what happens outside of our regular 1-minute interval observations. 
            

            <figure><a href="https://commons.wikimedia.org/wiki/File:Markovkate_01.svg#/media/File:Markovkate_01.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Markovkate_01.svg" alt="Markovkate 01.svg" style="max-height: 300px;" ></a><br>By <a href="//commons.wikimedia.org/wiki/User:Joxemai4" title="User:Joxemai4">Joxemai4</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=10284158">Link</a></figure>

            We are given the dynamics of the protein conformational changes: 
                        <ul>
                <li>If the protein is in state A, it has a 40% chance of switching to the E state by the next observation and a 60% chance of remaining in the A state. 
                </li>
                <li>If the protein is in state E, it has a 70% chance of switching to the A conformation by the next observation and a 30% chance of remaining in the E state</li>
                        </ul>

            Given a start state, we can determine the probability, not just of the next minute, but subsequent observations as well. 
            
            This can be represented naturally using Linear Algebra. 

            $$\begin{bmatrix}
            0.6 & 0.4\\ 
            0.7 & 0.3
            \end{bmatrix}$$

            Where the row is the starting state and the column is the final state. 
<br><br>
            
            For our starting state vector, if we are in the A state, we can put a 1 in the first column and a 0 elsewhere:

            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}$$
            It is not actually necessary to know the starting state to make statements on the probability of the state over time. We can have our start be a probability distribution instead. 
            
            To get the probability of being in each state after 1 interval, we can multiply the two:
            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}=\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}$$

            We observe a 60% chance of remaining in state A and a 40% chance of transitioning to state E as expected at t=1. 
            
            We can use this result to see what the probability of being in each state are at t=2

            $$\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}$$
                        $$=\begin{bmatrix}
            0.6*0.6+0.7*0.4 & 0.6*0.4+0.4*0.3
            \end{bmatrix}$$
            $$=\begin{bmatrix}
            0.64 & 0.36
            \end{bmatrix}$$
            We can view this as plugging in a probability distribution into a Markov chain rather than knowing the initial state.
            A general formula for the probability distribution at time t=n would thus be:

            $$\begin{bmatrix}
            p_{A,t=0} & p_{E,t=0}
            \end{bmatrix}\begin{bmatrix}
                        p_{A\rightarrow A} & p_{A\rightarrow E}\\ 
                        p_{E\rightarrow A} & p_{E\rightarrow E}
                        \end{bmatrix}^n$$
            But what are the long-run probabilities? What if we just came across this natively (assuming the same dynamics) - what would we expect our chances of finding it in a given state are? We can figure out what the distribution would be by taking the limit as \(n\rightarrow\infty\).

            <br><br>
            <img src="Markov_states.png" style="margin:auto; display: block;  width:clamp(300px,30vw, 600px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
initial_prob_vector = np.array([1,0])
transition_matrix = np.array([[0.6,0.4],[0.7,0.3]])
time_points = np.arange(10)
markov_df = pd.DataFrame(index=time_points,columns=['A','E'])
current_prob = initial_prob_vector
for i in range(len(time_points)):
    markov_df.loc[i] = current_prob
    current_prob = np.matmul(current_prob, transition_matrix)
markov_df.plot(xlabel="Iteration",
    ylabel="Fraction",
    title="Evolution of State Makeup Over Time")
plt.show()</pre>
            </code>

            What happens if the initial state were [0,1] instead of [1,0]? This can be checked using the code provided, but it will end up converging to the same values.


            Additionally, we are not restricted to just 2 states, our probability vector could have m columns for m possible states and our transition matrix would be mxm. 
            Markov chains can even be abstracted to infinite-state versions, although some properties change, which are beyond the scope of this textbook.

            Furthermore, the concept can be extended to continuous time processes. 
            <br><br>


            <h4>Random Walks and Diffusion</h4>
            Let us consider a number line containing all integers, centered at 0. 
            Place a particle at x=0. At every time increment, there is a change to its x position according to the following dynamics:
            $$P(\Delta x_t=1)=0.5$$
            $$P(\Delta x_t=-1)=0.5$$
            What will the resulting distribution look like? 
            If you are familiar with probability theory and remember the Central Limit Theorem, you might be able to predict that it will begin to approach a Gaussian distribution. 
            Well, the maximum possible deviation should be t if all the \(\Delta x_t\) are the same. 
            If each step is independent and the probability of a left movement is the same as a right movement, 
            then we can conclude that every individual path is equally likely. However, the number of paths you can take to get to x=0 after 10 steps is not the same number of paths where you'd end up at x=10. 
            We can alternatively parameterize the paths in terms of lefts and rights, similar to a coin flip. This ends up being a Bernoulli distribution and the multiplicity of paths is given by the binomial distribution. 
            
            <br><br>
            This is an application of Markov chains where each state is connected to the two adjacent numbered states. 
            

            <br><br>

            The following depicts a simulation of 5000 particles at various time points using Python.
            <img src="Diffusion.png" style="margin:auto; display: block;  width:clamp(300px,60vw, 900px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
particles = np.zeros(5000)
fig, axes = plt.subplots(4,figsize=(16,12))
bins = [-31+2*i for i in range(0,31)]
for i in range(31):
    if (i % 10 == 0):
        axes[i//10].hist(particles, bins = bins,density=True,histtype='step')
        axes[i//10].set(ylabel='Density',title=f't={i}')
    dice_rolls = np.random.rand(5000)
    movement = np.where(dice_rolls < 0.5,1,-1)
    particles += movement
axes[-1].set(xlabel="x")
plt.show()</pre>
            </code>
            <br>

            We can easily extend this model to a higher number of dimensions by having independent random walks along each axis. 

<img src="Diffusion2D.png" style="margin:auto; display: block;  width:clamp(300px,60vw, 900px)"/>
Python code:
<code style="margin: auto; width: 50%;"><pre>
particles_x = np.zeros(5000)
particles_y = np.zeros(5000)
bins = [-31+2*i for i in range(0,31)]
fig, axes = plt.subplots(2,2,figsize=(16,12))
axes = axes.flatten()
j=0
for i in range(51):
    if (i in [0,10,25,50]):
        ax = axes[j].hist2d(particles_x,particles_y, bins = bins,density=True)
        axes[j].set(ylabel='y',title=f't={i}')
        fig.colorbar(ax[3])
        j+=1
    dice_rolls = np.random.rand(5000)
    movement = np.where(dice_rolls < 0.5,1,-1)
    particles_x += movement
    dice_rolls = np.random.rand(5000)
    movement = np.where(dice_rolls < 0.5,1,-1)
    particles_y += movement
axes[-1].set(xlabel="x")
plt.show()</pre>
</code>
<br>


            Another area is the concept of a biased walk; i.e. one where the jumps in one direction are favored. We used 50% probabilities for each direction in our initial model, but there's no need for it to be this way. 
            We could have just as easily set \(P(\Delta x=+1)=0.75\) instead. We would observe a fraction still winding up in the negative integers.

            <img src="Biased_Diffusion.png" style="margin:auto; display: block;  width:clamp(300px,60vw, 900px)"/>
            Python code:
            <code style="margin: auto; width: 50%;"><pre>
particles = np.zeros(5000)
fig, axes = plt.subplots(4,sharex=True,figsize=(16,12))
bins = [-31+2*i for i in range(0,31)]
for i in range(31):
    if (i % 10 == 0):
        axes[i//10].hist(particles, bins = bins,density=True,histtype='step')
        axes[i//10].set(ylabel='Density',title=f't={i}')
    dice_rolls = np.random.rand(5000)
    movement = np.where(dice_rolls < 0.75,1,-1) # changed from 0.5 to 0.75
    particles += movement
axes[-1].set(xlabel="x")
plt.show()</pre>
            </code>
            <br>

            Of course, this is not a realistic model, as particles don't discretely jump, but it becomes a better approximation as the limit as the scale of distance and time go to 0. 
            This will be more precisely defined later.

            <h3>Continuous-Time Stochastic Processes</h3>

            <h4>Brownian Motion</h4>
            Brownian motion can be thought of as the continuous analogue of random walks. 
            It is defined by the following properties (<a href="https://www.math.uchicago.edu/~lawler/finbook.pdf">https://www.math.uchicago.edu/~lawler/finbook.pdf</a>):
            <ol>
                <li>\(B_0=0\)</li>
                <li>\(B_t\) is almost surely continuous</li>
                <li>\(B_t\) has independent increments</li>
                <li>\(B_t-B_s\sim N(\mu=0,\sigma^2=t-s)\text{where }0\leq s\leq t\)</li>
            </ol>

            Now to explain the properties in further detail.

            $$B_0=0$$
            This is mainly a convention, as it is possible to have a process undergoing Brownian Motion starting elsewhere, but you would just have to add a constant.

            $$B_t\text{ is almost surely continuous}$$
            This is more of a theoretical property which we need not be concerned with. For the intents and purposes of this book, 
            we can just interpret this as stating that Brownian motion is a continuous path with no jumps. 

            $$B_t\text{ has independent increments}$$
            Let \(0< s< t\).
            The value of \(B_s-B_0\) is independent of \(B_t-B_s\). It is important that when we select the two intervals that they do not overlap. 
            It is fine to have the endpoint of one be the start of the other however.

            $$B_t-B_s\sim N(\mu=0,\sigma^2=t-s)\text{where }0\leq s\leq t$$

            A segment of Brownian motion is normally distributed with a variance equal to the time between the start and end points with mean 0. 
            



            <h4>Poisson Process</h4>
            When we were working with discrete-time Markov chains, we had a matrix or depiction of
            $$P(X_{i,t=n+1}|X_{j,t=n})$$
            where \(X_i\) is the new possible state and \(X_j\) is the current state. 
            These probabilities are implicitly given based on the time interval. We do not consider what happens in between observation periods; it is possible that we observe the same state in consecutive observations, but the protein went from A to E then back to A in between.
            To get to continuous-time stochastic processes, we can take the limit as the time between observation periods goes to 0.
<br><br>
            For a full derivation, see <a href="https://www.math.uchicago.edu/~lawler/finbook.pdf">Lawler's Stochastic Calculus book</a> chapter 6.
            
<br><br>
            Let us first consider a process that can only increase. In a biology context, we can say that this is the total amount of mRNA produced over a period (not the mRNA level since mRNA can degrade and would go down). 
            Let assume that there is an mRNA production rate \(\lambda\) and define \(p(s)\) to be the probability that at least 1 mRNA moecule is produced during the interval [t,t+s]. 
            Then the probability of 1 mRNA molecule being produced in a short interval is approximately the rate times the duration of time. This can be formalized as 
            $$p(\Delta t)=\lambda \Delta t+o(\Delta t)$$

            Let \(X_s\) be the total number of molecules of mRNA produced by time t=s, T be the first time that a molecule of mRNA is produced, or formally:

            $$T=\text{inf}\{s:X_s=1\}$$

            From this, it can be shown that the probability that no molecules have been produced during an interval of length t is
            
            $$P\{T>t\}=e^{-\lambda*t}$$
            Thus, the time for between mRNA production is exponentially distributed like 
            $$f(t)=\lambda e^{-\lambda t}$$
            
            This distribution is nice as it has the "memory-less" property that our discrete-time Markov chains had.
            Now, this has only given us a distribution of how long it takes between transcription events, not a total amount. 
<br><br>
            Without deriving it, it can be shown that the distribution for the total number of "events" with exponentially distributed wait times has the following distribution:

            $$P(X_{t+s}-X_s=k)=e^{-\lambda t}\frac{(\lambda t)^k}{k!}$$

            This is known as a Poisson distribution, which is parameterized by \(\lambda\), the rate constant. 
            The distribution has the following properties:
            <ol>
                <li>Mean of \(\lambda t\) </li>
                <li>Variance of \(\lambda t\)</li>
            </ol>
            The Poisson process can be further generalized to allow for variable-sized jumps rather than just jumps of 1. 
            This is known as a "Compound Poisson Process." 


            <h4>Advanced Applications in Biology</h4>
            The classical example of stochastic processes in biology is mRNA expression levels - as it is a quantity that can both degrade and be produced at irregular intervals constantly. 

            
        </div>




        </div>

        <h2>Works Cited</h2>
        <ol>
            <li><a href="https://www.math.uchicago.edu/~lawler/finbook.pdf">https://www.math.uchicago.edu/~lawler/finbook.pdf</a></li>
        </ol>

</body>
</html>