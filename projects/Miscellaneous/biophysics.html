<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RDM65P2C47"></script>
        <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SYJL7ZYKXB');
    </script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-RDM65P2C47');
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Biophysics E-book</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Biophysics chemistry textbook">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Biophysical chemistry textbook">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
            
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        </script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script>

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/index.html">Home</a>|
            <a href="https://johndellarosa.github.io/resume.html">Resume</a>
            <!-- <a href="./index.html#education">Education</a>
            <a href="./index.html#experience">Experience</a>
            <a href="./index.html#skills">Skills</a> -->
            |
            <a href="https://johndellarosa.github.io//biography.html">About</a>|
            <a href="https://johndellarosa.github.io//projects.html">Projects</a>|
            <a href="https://johndellarosa.github.io//Miscellaneous.html">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
<h1>Biophysical Chemistry Textbook (Work in Progress)</h1>
<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">

            <div id="Table-of-Contents">
                <ol>
                    <li><a href="#Introduction">Introduction</a></li>
                    <li><a href="#Probability">Probability, Statistics, & Entropy</a></li>
                    <li><a href="#Thermodynamics">Thermodynamics</a></li>
                    <li><a href="#Biochemistry">Biochemistry</a></li>

                
                
                
                </ol>


            </div>


            <div id="Introduction">
            <h2>Introduction</h2>
            <h3>Who this text is for</h3>
            This text is aimed at students at junior level or higher who are interested in biophysical chemistry or want a free supplemental text on the subject.
            <h3>Requirements</h3>
            <h4>Math</h4>
            <ul>
            <li>This textbook will assume knowledge of single variable calculus.</li> 
            <li>Multivariable calculus will be used for derivations, especially in the physics-focused chapters.</li> 
            <li>Properties of logarithms and exponentiatials will be used</li> 
            <li>Differential equations will appear, but will not require understanding for how to solve, merely recognizing what the equation mean. </li>
            <li>Linear Algebra will be used in the some of the later probability-focused chapters, such as Markov Chains</li>
        </ul>
            <h4>Physics</h4>
            <ul>
                <li>
                Not much physics is required, just basic understanding the concept of energy, kinetic energy, potential energy, force, etc.
            </li>
            </ul>

            <h4>Chemistry</h4>
            <ul>
                <li>General chemmistry knowledge will be assumed</li>
                <li>Basics of organic chemistry will be assumed.
                    <ul>
                        <li>Ability to understand shorthand for structures</li>
                        <li>Nucleophile, electrophile concepts</li>
                    </ul>
                </li>
            </ul>


            <h4>Biology</h4>
            <ul>
                <li>Familiarity with central dogma of biology</li>
                <li>Basic knowledge of nucleic acids</li>
                <li>Basic knowledge of proteins</li>
            </ul>
            </div>
            <div id="Probability">
            <h2>Probability, Statistics, and Entropy</h2>

            <h3>Entropy</h3>
            <h4>Multiplicity</h4>
            A generalized multiplicity formula for arranging Z types of things in M spaces is
            $$W=\frac{M!}{\prod N_{i}!}=\frac{M!}{N_{1}!*N_{2}!*...*N_{Z}!}$$
            If there are only two possible types and M total and N of type 1, then \(N_2=M-N\) and it reduces to
            $$W=\frac{M!}{N!(M-N)!}$$
            <h4>Extensive and Intensive Properties</h4>
            A property of a system is said to be <b>extensive</b> if it is proportional to the size of the system. One example is mass; if the system is 1L of water, going to 2L of water will double the mass. These properties are often not ratios and can often be intuited via thought experiment. 
<br><br>
            A property of a system is said to be <b>intensive</b> if it remains constant when a system is scaled. Density is an example of an intensive property. In the previous example, the density of the system does not change when you go from 1L of water to 2L of water. Mass and volume of the system both double, which cancel out when you take the ratio to get density. 
<br><br>
            Some properties are neither extensive not intensive if they scale but in a non-linear proportional way. 
            The most notable example is multiplicity. This can be shown if every number in the above calculation is doubled due to the factorial operator. 
            Entropy at large scales behaves like an extensive property, but at very small scales, this does not always hold. 
            </div>
            <div id="Thermodynamics">
            <h2>Introductory Thermodynamics</h2>
            <h3>Laws of Thermodynamics</h3>
            <h4>First Law of Thermodynamics</h4>

            The first law can be formulated in different ways. A common source of confusion for students is the seemingly inconsistent formula. 
<br><br>
            Variant 1:
            $$\Delta U=q+w$$
            Variant 2:
            $$\Delta U=q-w$$
            So why will you see both? This is due to differences between authors in how they define the quantities in the equation. 

            Let's start by defining q, which we will call "heat," since this is less controversial. q will be the heat added to the system. Heat flowing (positive q) into the system will increase the amount of internal energy intuitively, so a positive q will contribute to an increase in energy. 
            Heat flowing out of the system (negative q) will decrease the amount of internal energy.
<br><br>
            Now, for defining w, which is the "work" term. Here are the two equations again with their definition
            <ol>
                <li>\(\Delta U=q+w\), positive w here is defined as work done <b>to</b> the system and negative w is defined as work done <b>by</b> the system</li>
                <li>\(\Delta U=q-w\), positive w here is defined as work done <b>by</b> the system and negative w is defined as work done <b>to</b> the system</li>
            </ol>

            Regardless of your definition of w, if work is being performed on the system, energy is being transferred to it, but if the system is expending energy to do work, then it will lose energy. 
            From here on out, the textbook will use variant 1.
            <br><br>
            Now, the terms work and heat have not been preicsely defined yet. 
            Working definitions for the terms at this point will be:
            <ul>
                <li>Work is controlled, harnessed transfer of energy</li>
                <li>Heat is uncontrolled transfer of energy</li>
            </ul>

            Work can take various forms, but the ones we will be considering will be 
            <ol>
                <li>Expansion work or "pV work," such as a piston moving due to a pressure difference.
                </li>
                <li>Chemical work, which harnesses energy from chemical reactions</li>
            </ol>

            <h4>Second Law of Thermodynamics</h4>
            The second law of thermodynamics states that total entropy is non-decreasing over time. 
            $$\Delta S_{universe}\geq 0$$
            This is sometimes put into formula by breaking down the universe into the "system" or thing of interest and the "surroundings" or everything else. 
            $$\Delta S_{sys}+\Delta S_{surr} \geq 0$$
            A common mistake that students make is interpreting it as saying that entropy cannot decrease locally. 
            This is <b>not</b> true. Entropy can very well decrease in a system, but it will be offset by a greater increase in the surroundings. 
            In fact, much of biology involves this very trade-off.


            <h3>Enthalpy and the Thermodynamic Definition of Entropy</h3>
            <h4> Deriving the Thermodynamic Definition of Entropy</h4>
            By the differential form of the first law of Thermodynamics, we have
            $$dU=(\frac{\partial U}{\partial S})_{V,N}dS+(\frac{\partial U}{\partial V})_{S,N}dV+\sum(\frac{\partial U}{\partial N_i})_{S,V}dN_i$$
            $$=TdS-p_{ext}dV+\sum\mu_idN_i$$
            *Negative sign for pressure since our dV is pointing in opposite direction of \(p_{ext}\).
            Let us define a quantity called Enthalpy, denoted by H, as follows:
            $$H\equiv U+PV$$
            Through the <a href="https://en.wikipedia.org/wiki/Product_rule">product rule</a> for differentiation:
            $$dH=dU+d(PV)=dU+(pdV+Vdp+dVdp)$$
            Substituting in the total differential form of the first law from above for dU:
            $$=(TdS-pdV)+pdV+Vdp+dVdp$$
            The dVdp term goes to 0 in the limit. The pdV term from work cancels out with one of the terms from the product rule for d(PV).
            $$=TdS+Vdp$$
            If we are working in a constant temperature environment, we can let dp=0, yielding the formula
            $$dH=TdS=dq$$

            $$dS=\frac{dq_{rev}}{T}$$
            This is the so-called "Thermodynamic definition of Entropy" which holds for a reversible process at constant pressure<br>


<h4>Irreversible Processes</h4>

            However, what about for irreversible processes? The first law still holds such that 
            $$U=q+w$$
            but
            \(q_{rev}\neq q_{irr}\) 
            and 
            \(w_{rev}\neq w_{irr}\).
            A more general statement which holds for irreversible processes is
            $$dS\geq \frac{\delta Q}{T}$$
            with equality when the process is reversible. This is known as the Clausius Inequality, which is another way of formulating the 2nd law.<br>
            
            <h4>An Aside on Legendre Transforms</h4>
            In the first law, we had
            $$dU=TdS-pdV$$
            and when we defined Enthalpy as \(H\equiv U+PV\), we ended up getting
            $$dH=TdS+Vdp$$
            By adding PV, we were able to switch the positions of the Volume and Pressure variables (along with a sign change). 
            By doing this, we can now have a quantity that is more convenient to work with when we assume different conditions (e.g. constant pressure instead of constant volume). 
            This is from which we got heat being the change in energy at a fixed volume and the change in enthalpy at fixed pressure equivalencies (set dV=0 or dP=0 in the above equations). 
        <br><br>
            This is a theme that will recur for the Helmholtz and Gibbs free energies where instead we will switch temperature and entropy, as dT=0 is more feasible than dS=0. 
            These paired variables are known as "conjugate variables," and the process of switching the differential between the extensive and intensive variable is known as a Legendre transformation.

            <h4>Helmholtz Free Energy and Gibbs Free Energy</h4>

            Let us define a quantity Helmholtz Free Energy, denoted by A (The letter A was chosen for the German word for work as H was already being used for Enthalpy; some books use F for free energy):
            $$A\equiv U-TS$$
            Doing a similar trick as we did for Enthalpy - taking the derivative of both sides and using the linearity of the derivative operator:
            $$dA=dU-d(TS)$$
            Substituting in for dU using the first law; product rule for d(TS)
            $$dA=(TdS-pdV)-(TdS+SdT+dSdT)$$
            Canceling out TdS terms and dSdT goes to 0
            $$dA=-pdV-SdT$$

            If we are working in a constant temperature system (dT=0), then we find that the change in Helmholtz free energy is the amount of work being done. 
            <br><br>
            It is also possible to do what we did for Enthalpy and Helmholtz for a single quantity. 
            Let us define a quantity Gibbs Free Energy, denoted by G:

            $$G\equiv U+PV-TS=A+PV=H-TS$$
            In general chemistry, Gibbs free energy is often introduced as
            $$G=H-TS$$
            so the derivation will go from that point.

            $$dG=dH-d(TS)$$
            Substituting in our above result for dH; product rule for d(TS)
            $$dG=(TdS+Vdp)-(TdS+SdT+dSdT)$$
            Cancelling out TdS terms; dSdT goes to 0
            $$dG=Vdp-SdT$$

        </div>
        <div id="Biochemistry">
            <h2>Biochemistry</h2>
            <h3>Acid-Base Chemistry</h3>
            <h4>Definitions of Acid and Base</h4>
            There are 3 main basic definitions of acid-base chemistry:
            <ul>
                <li>Arrhenius: increases concentration of H+ (acid) or OH- (base)</li>
                <li>Bronsted-Lowry: proton donor (acid) and proton acceptor (base)</li>
                <li>Lewis: electron pair acceptor (acid) and electron pair donor (base)</li>
            </ul>
            The Arrhenius definition has been supplanted by the Bronsted-Lowry one, as it's more general. 
            The Lewis definition is also very important and its acids and bases are also referred to as electrophiles and nucleophiles respectively. 
            For clarity, future uses of the words "acid" and "base" will refer to the Bronsted-Lowry definition; Lewis acids and bases will be referred to by their aforementioned organic chemistry terms.

            <h4>Equilibrium Constants</h4>

            To be filled later

            <h4>Henderson-Hasselbach Equation and pKa</h4>

            Let us start with a generic acid dissociation equation from before with an acid dissociation constant \(K_A\):

            $$HA+H_2O\rightleftharpoons A^-+H_3O^+$$

            The associated equilibrium equation is 

            $$K_A=\frac{[A^-][H_3O^+]}{HA}$$
            There are various ways you can algebraically manipulate the equation and take logs in order to get to the final equation, but I will get things on the right side prior to taking the log
            $$\frac{1}{[H_3O^+]}=\frac{1}{K_A}\times\frac{[A^-]}{[HA]}$$
            Taking the log of both sides.
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A}\times\frac{[A^-]}{[HA]})$$
            Using the multiplication and division rule for the argument:
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A})+\log(\frac{[A^-]}{[HA]})$$
            Using a specific case of the log power rule, there is the relationship \(\log(1/a)=-\log(a)\), we get
            $$-\log([H^+])=-\log(K_A)+\log(\frac{[A^-]}{[HA]})$$
            By definition, \(pKa\equiv -\log(K_A)\) and \(pH\equiv -\log([H^+])\). Thus, we arrive at the Henderson-Hasselbach equation:
            $$pH=pKa+\log(\frac{[A^-]}{[HA]})$$

            This equation allows us to relate the pH of an environment, the pKa of an acid, and the ratio of deprotonated to protonated occurences of that acid

            

            <h4>Amino Acids and Acid-Base Chemistry</h4>
            As amino acids have both amine and carboxylic acid groups, they can all act as both acids and bases to some extent. 
            In fact, at a neutral pH, amino acids will be predominantly Zwitterions (molecules that have both positive and negative charges); the amine group next to the alpha carbon will be protonated and the carboxylic acid group next to the alpha carbon will be deprotonated. 
            <br>
            However, a number of the proteogenic amino acids can also participate in acid-base chemistry through their sidechains. 

            <p style="width: 90%; text-align: center; margin: auto;"><a href="https://commons.wikimedia.org/wiki/File:Proteinogenic_Amino_Acid_Table.png#/media/File:Proteinogenic_Amino_Acid_Table.png"><img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Proteinogenic_Amino_Acid_Table.png" alt="Proteinogenic Amino Acid Table.png" style="max-width: 100%;"></a><br>By <a href="https://commons.wikimedia.org/w/index.php?title=User:Thomas.ryckmans68&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="User:Thomas.ryckmans68 (page does not exist)">Thomas.ryckmans68 <span class="int-own-work" lang="en"&gt;Own work&lt;/span&gt;, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=117084381">Link</a></p>
            <br>
            <br>

            One might ask - why do the amide groups in Asparagine and Glutamine not participate in acid-base chemistry? 
            The answer has to do with conjugated pi systems. The lone pair on the nitrogen is not "available" to grab a proton because it prefers to be in a conjugated system with the carbonyl. 
            For the same reason, in proteins, you will not see the amide backbone being charged, only the N and C termini.

        </div>

        <div id="Kinetics">
            <h2>Kinetics</h2>
            <h3>Refresher on Kinetics</h3>
                <h4>Rate Laws</h4>

                <h4>Relationship with Equilibrium Constant</h4>

                <h4>Integrated Rate Laws</h4>



            <h3>Michaelis-Menten Kinetics</h3>
            <h4>Binding and Unbinding</h4>
            <h4>Derivation</h4>
            <h4>More Complicated Dynamics</h4>

            <h3>Diffusion</h3>
        </div>

        <div id="Stochastic-Processes">
            <h3>Introduction to Discrete Stochastic Processes</h3>
            <h4>Distributions</h4>
            <h4>Markov Chains</h4>

            Imagine a system where a protein is in one of two states: A or E. Each minute you observe its state, and it has the ability to go from state A to E or E to A, but it does not necessarily switch. We will denote these observational periods at \(t=0, 1, \ldots\)
            <br><br>
            An important assumption that we will make is that only the current state determines the probability of switching conformations by the second; the state that it was in prior to this observation have <b>no</b> impact on the chances. 
            E.g. if we are at t=n, the state that the protein was in at t=n-1 has no impact on the probability for t=n+1. 
            This is called the <b>Markov property</b>.
            
            <br><br>
            
            For simplicity's state, we will assume that these observations are discrete events and not worry about what happens outside of our regular 1-minute interval observations. 
            

            <figure><a href="https://commons.wikimedia.org/wiki/File:Markovkate_01.svg#/media/File:Markovkate_01.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Markovkate_01.svg" alt="Markovkate 01.svg" style="max-height: 300px;" ></a><br>By <a href="//commons.wikimedia.org/wiki/User:Joxemai4" title="User:Joxemai4">Joxemai4</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=10284158">Link</a></figure>

            We are given the dynamics of the protein conformational changes: 
                        <ul>
                <li>If the protein is in state A, it has a 40% chance of switching to the E state by the next observation and a 60% chance of remaining in the A state. 
                </li>
                <li>If the protein is in state E, it has a 70% chance of switching to the A conformation by the next observation and a 30% chance of remaining in the E state</li>
                        </ul>

            Given a start state, we can determine the probability, not just of the next minute, but subsequent observations as well. 
            
            This can be represented naturally using Linear Algebra. 

            $$\begin{bmatrix}
            0.6 & 0.4\\ 
            0.7 & 0.3
            \end{bmatrix}$$

            Where the row is the starting state and the column is the final state. 

            If we are in the A state, we can put a 1 in the first column and a 0 elsewhere:

            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}$$
            It is not actually necessary to know the starting state to make statements on the probability of the state over time. We can have our start be a probability distribution instead. 
            
            To get the probability of being in each state after 1 interval, we can multiply the two:
            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}=\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}$$

            We observe a 60% chance of remaining in state A and a 40% chance of transitioning to state E as expected at t=1. 
            
            We can use this result to see what the probability of being in each state are at t=2

            $$\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}=\begin{bmatrix}
            0.6*0.6+0.7*0.4 & 0.6*0.4+0.4*0.3
            \end{bmatrix}$$
            $$=\begin{bmatrix}
            0.64 & 0.36
            \end{bmatrix}$$
            We can view this as plugging in a probability distribution into a Markov chain rather than knowing the initial state.
            A general formula for the probability distribution at time t=n would thus be:

            $$\begin{bmatrix}
            p_{A,t=0} & p_{E,t=0}
            \end{bmatrix}\begin{bmatrix}
                        p_{A\rightarrow A} & p_{A\rightarrow E}\\ 
                        p_{E\rightarrow A} & p_{E\rightarrow E}
                        \end{bmatrix}^n$$
            But what are the long-run probabilities? What if we just came across this in natively (assuming the same dynamics) - what would we expect our chances of finding it in a given state are? We can figure out what the distribution would be by taking the limit as \(n\rightarrow\infty\).

            <br><br>


            Additionally, we are not restricted to just 2 states, our probability vector could have m columns for m possible states and our transition matrix would be mxm. 
            Markov chains can even be abstracted to infinite-state versions, although some properties change, which are beyond the scope of this textbook.
            <br><br>





            <h4>Advanced Applications in Biology</h4>
            
        </div>




        </div>

</body>
</html>