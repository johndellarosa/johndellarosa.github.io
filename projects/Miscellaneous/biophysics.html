<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-RDM65P2C47"></script>
        <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-SYJL7ZYKXB');
    </script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-RDM65P2C47');
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Biophysics E-book</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Biophysics chemistry textbook">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Biophysical chemistry textbook">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
            
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
        </script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script>

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/index.html">Home</a>|
            <a href="https://johndellarosa.github.io/resume.html">Resume</a>
            <!-- <a href="./index.html#education">Education</a>
            <a href="./index.html#experience">Experience</a>
            <a href="./index.html#skills">Skills</a> -->
            |
            <a href="https://johndellarosa.github.io//biography.html">About</a>|
            <a href="https://johndellarosa.github.io//projects.html">Projects</a>|
            <a href="https://johndellarosa.github.io//Miscellaneous.html">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
<h1>Biophysical Chemistry Textbook (Work in Progress)</h1>
<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">

            <div id="Table-of-Contents">
                <ol>
                    <li><a href="#Introduction">Introduction</a></li>
                    <li><a href="#Probability">Probability, Statistics, & Entropy</a></li>
                    <li><a href="#Thermodynamics">Thermodynamics</a></li>
                    <li><a href="#Advanced-Thermodynamics">Advanced Thermodynamics</a></li>
                    <li><a href="#Biochemistry">Biochemistry</a></li>
                    <li><a href="#Kinetics">Kinetics</a></li>
                    <li><a href="#Stochastic-Processes">Stochastic Processes</a></li>
                
                
                
                </ol>


            </div>


            <div id="Introduction">
            <h2>Introduction</h2>
            <h3>Who this text is for</h3>
            This text is aimed at students at junior level or higher who are interested in biophysical chemistry or want a free supplemental text on the subject.
            <h3>Requirements</h3>
            <h4>Math</h4>
            <ul>
            <li>This textbook will assume knowledge of single variable calculus.</li> 
            <li>Multivariable calculus will be used for derivations, especially in the physics-focused chapters.</li> 
            <li>Properties of logarithms and exponentiatials will be used</li> 
            <li>Differential equations will appear, but will not require understanding for how to solve, merely recognizing what the equation mean. </li>
            <li>Linear Algebra will be used in the some of the later probability-focused chapters, such as Markov Chains</li>
            <li>Basics of probability and statistics will be assumed</li>
        </ul>
            <h4>Physics</h4>
            <ul>
                <li>
                Not much physics is required, just basic understanding the concept of energy, kinetic energy, potential energy, force, etc.
            </li>
            </ul>

            <h4>Chemistry</h4>
            <ul>
                <li>General chemmistry knowledge will be assumed</li>
                <li>Basics of organic chemistry will be assumed.
                    <ul>
                        <li>Ability to understand shorthand for structures</li>
                        <li>Nucleophile, electrophile concepts</li>
                    </ul>
                </li>
            </ul>


            <h4>Biology</h4>
            <ul>
                <li>Familiarity with central dogma of biology</li>
                <li>Basic knowledge of nucleic acids</li>
                <li>Basic knowledge of proteins</li>
            </ul>
            </div>
            <div id="Probability">
            <h2>Probability, Statistics, and Entropy</h2>
            <h3>Probability</h3>
            <h4>Defining terms</h4>
            Before we show any equations, we must define some terms. Let A and B be events.

            <ul>
                <li>P(A) is the probability of event A occurring</li>
                <li>\(P(A\cup B)\) is the probability of A or B or both occurring. The \(\lor\) can also be seen instead of \(\cup\)</li>
                <li>\(P(A\cap B)\) is the probability of A <b>and</b> B both occurring. The \(\land\) can also be seen instead of \(\cap\)</li>
                <li>\(P(\neg A)\) is the probability of A not occurring. This can also be written as a bar above the A</li>
            </ul>

            <h4>De Morgan's Laws</h4>
            De Morgan's laws describe how negation interactions with grouping of events. The statements can be rather formal, but verbally seem obvious. 

            "If neither A nor B happened, then that means A didn't happen and B didn't happen"
            $$\neg(A \lor B)=(\neg A)\land (\neg B)$$

            "If A and B didn't both happen, then that means either A didn't happen, B didn't happen, or neither happened"
            $$\neg(A \land B)=(\neg A)\lor (\neg B)$$


            <h4>Conditional Probability</h4>
            Let A and B be two events. We denote the conditional probability of A occurring given B having occurred as \(P(A|B)\)

            This is defined mathematically as:

            $$P(A|B)\equiv \frac{P(A \cap B)}{P(B)}$$
            where \(P(A\cap B)\) is the probability of both A and B occurring.
            


            <h4>Bayes' Theorem</h4>
            Bayes' Theorem relates two events and the conditional probabilities of one upon another. 
            Let A and B two events with non-zero probabilities
            $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$



            <h3>Entropy</h3>
            <h4>Multiplicity</h4>
            A generalized multiplicity formula for arranging Z types of things in M spaces is
            $$W=\frac{M!}{\prod N_{i}!}=\frac{M!}{N_{1}!*N_{2}!*...*N_{Z}!}$$
            If there are only two possible types and M total and N of type 1, then \(N_2=M-N\) and it reduces to
            $$W=\frac{M!}{N!(M-N)!}$$
            <h4>Extensive and Intensive Properties</h4>
            A property of a system is said to be <b>extensive</b> if it is proportional to the size of the system. One example is mass; if the system is 1L of water, going to 2L of water will double the mass. These properties are often not ratios and can often be intuited via thought experiment. 
<br><br>
            A property of a system is said to be <b>intensive</b> if it remains constant when a system is scaled. Density is an example of an intensive property. In the previous example, the density of the system does not change when you go from 1L of water to 2L of water. Mass and volume of the system both double, which cancel out when you take the ratio to get density. 
<br><br>
            Some properties are neither extensive not intensive if they scale but in a non-linear proportional way. 
            The most notable example is multiplicity. This can be shown if every number in the above calculation is doubled due to the factorial operator. 
            Entropy at large scales behaves like an extensive property, but at very small scales, this does not always hold. 
            </div>
            <div id="Thermodynamics">
            <h2>Introductory Thermodynamics</h2>
            <h3>Laws of Thermodynamics</h3>
            <h4>First Law of Thermodynamics</h4>

            The first law can be formulated in different ways. A common source of confusion for students is the seemingly inconsistent formula. 
<br><br>
            Variant 1:
            $$\Delta U=q+w$$
            Variant 2:
            $$\Delta U=q-w$$
            So why will you see both? This is due to differences between authors in how they define the quantities in the equation. 

            Let's start by defining q, which we will call "heat," since this is less controversial. q will be the heat added to the system. Heat flowing (positive q) into the system will increase the amount of internal energy intuitively, so a positive q will contribute to an increase in energy. 
            Heat flowing out of the system (negative q) will decrease the amount of internal energy.
<br><br>
            Now, for defining w, which is the "work" term. Here are the two equations again with their definition
            <ol>
                <li>\(\Delta U=q+w\), positive w here is defined as work done <b>to</b> the system and negative w is defined as work done <b>by</b> the system</li>
                <li>\(\Delta U=q-w\), positive w here is defined as work done <b>by</b> the system and negative w is defined as work done <b>to</b> the system</li>
            </ol>

            Regardless of your definition of w, if work is being performed on the system, energy is being transferred to it, but if the system is expending energy to do work, then it will lose energy. 
            From here on out, the textbook will use variant 1.
            <br><br>
            Now, the terms work and heat have not been preicsely defined yet. 
            Working definitions for the terms at this point will be:
            <ul>
                <li>Work is controlled, harnessed transfer of energy</li>
                <li>Heat is uncontrolled transfer of energy</li>
            </ul>

            Work can take various forms, but the ones we will be considering will be 
            <ol>
                <li>Expansion work or "pV work," such as a piston moving due to a pressure difference.
                </li>
                <li>Chemical work, which harnesses energy from chemical reactions</li>
            </ol>

            <h4>Second Law of Thermodynamics</h4>
            The second law of thermodynamics states that total entropy is non-decreasing over time. 
            $$\Delta S_{universe}\geq 0$$
            This is sometimes put into formula by breaking down the universe into the "system" or thing of interest and the "surroundings" or everything else. 
            $$\Delta S_{sys}+\Delta S_{surr} \geq 0$$
            A common mistake that students make is interpreting it as saying that entropy cannot decrease locally. 
            This is <b>not</b> true. Entropy can very well decrease in a system, but it will be offset by a greater increase in the surroundings. 
            In fact, much of biology involves this very trade-off.

            <h3>Energy</h3>
            <h4>Forms of Energy</h4>
            Energy can take different forms. The two main categories are Kinetic and Potential energy. 
            As its name suggests, kinetic energy is associated with movement. Here are 3 forms which we will consider throughout the book:
            <ol>
                <li>Translational: a molecule's movement through space</li>
                <li>Rotational: a molecule rotating about some axis</li>
                <li>Vibrational: bonded atoms vibrating</li>
            </ol>
            Potential energy is associated with position rather than movement. This can also take several forms, including but not limited to:
            <ol>
                <li>Chemical bonds</li>
                <li>Gravitational</li>
                <li>Concentration gradients</li>
                <li>Electrostatic interactions</li>
                <li>Compressed spring</li>
            </ol>
            <h4>Physics Equations</h4>
            As one formulation of the First Law states, there is conservation of energy with no input or removal from the system. 
            However, energy can be converted from one form to another. Imagine a ball rolling off of a table; its gravitational potential energy gets converted into kinetic energy, sending it downward. 
            U sometimes refers to potential energy, but also internal energy in general. In this section, U will refer to <b>Potential energy</b> and E will refer to <b>Total energy</b>.
            
            <br><br>
            In this section, positive work will refer to work <b>done</b> by the system which is opposite what we had before, but it is common convention. 

            $$W=-\Delta U$$

            How is are work required and potential energy related to forces?

            $$W=\int F\cdot dx$$

            The work done is the integral of the force, F, applied over the distance. This force may be a function of the position, x. 

            <h4>Hooke's Law</h4>
            Hooke's law describes the restoring force in a spring. In plain English, Hooke's law states that the restoring force a spring has is proportional to the displacement from the rest length.
            $$F=-k(x-x_0)$$
            where x is the length of the spring, \(x_0\) is the rest length of the spring, k is the spring constant, and F is the force. The sign is negative as the force counteracts the stretching and applies a force in the opposite direction. 
            We can also call \(x-x_0\) to be simply x by having our coordinate system have x_0=0; longer is positive x; compressed is negative x.

            Using our formula for work, we can calculate the work required to stretch a spring.
            $$W=\int_0^z (-k x) \cdot dx$$
            Because the force is in the opposite direction as the dx vector, we get an additional negative sign which cancels things out. 
            $$W=\int_0^z(kx)dx$$
            $$=\frac{1}{2}kz^2$$
            The work required is \(\frac{1}{2}kz^2\) which will be converted into potential energy.

            <h4>Electrostatics</h4>
            Another common force is electrostatic. The electrostatic force between two point charges is described by Coulomb's Law. 
            $$F=\frac{1}{4\pi D\varepsilon_0}\frac{q_1q_2}{r^2}$$
            where F is the force, D is the dielectric constant for the medium, \(\varepsilon_0\) is the vacuum permittivity (a constant), \(q_i\) is the charge of particle i, and r is the distance between the two charged particles. 
            In this form of Coulomb's Law, a negative value for F means an attractive force and a positive F means a repulsive force. 




            </div>
            <div id="Advanced-Thermodynamics">
                <h2>Advanced Thermodynamics</h2>
                <h3>Introduction</h3>
                This section will delve deeper into thermodynamics. Multivariable calculus will be used to expand upon topics covered earlier as well as introduce new topics. 
                The <b>Fundamental Thermodynamic Relation</b> will be covered as an extension of the first law.
                <h3>Laws of Thermodynamics Revisited</h3>
            <h4>Total derivatives</h4>

            While the derivation is outside the scope of this textbook, the first law can be expressed as the following total derivative:
            $$dU=(\frac{\partial U}{\partial S})_{V,N}dS+(\frac{\partial U}{\partial V})_{S,N}dV+\sum(\frac{\partial U}{\partial N_i})_{S,V}dN_i$$
            $$=TdS-p_{ext}dV+\sum\mu_idN_i$$
            where T is temperature, \(p_{ext}\) is external pressure, V is volume, \(\mu_i\) is the chemical potential for a chemical species i, and \(N_i\) is the number of species i

            <h4>Temperature and Entropy</h4>
            From the Fundamental Thermodynamic Relation, we have:
            $$(\frac{\partial S}{\partial U})_{N,V}=\frac{1}{T}$$
            If \(T_{A}>T_{B}\), then \(\frac{1}{T_{A}}<\frac{1}{T_{B}}\)
            $$(\frac{\partial S_{A}}{\partial U_{A}})_{N,V}<(\frac{\partial S_{B}}{\partial U_{B}})_{N,V}$$
            by substituting in the thermodynamic definition for temperature, we see that the lower temperature system, B, would have a greater change in entropy for the same change in energy. This means that there would be a net entropy increase if energy were to flow from A to B. 
            So when would this heat flow stop? When entropy is maximized/equilibrium. The termination condition would be when there is no net increase in entropy due to heat flow or in mathematical terms:
            $$(\frac{\partial S_{A}}{\partial U_{A}})_{N,V}=(\frac{\partial S_{B}}{\partial U_{B}})_{N,V}$$
            which is equivalent to saying
            $$\frac{1}{T_{A}}=\frac{1}{T_{B}}$$
            or alternatively
            $$T_{A}=T_{B}$$

            In plain English to summarize: For the same amount of energy added to a system, the one with the lower temperature would increase in entropy more. 
            Conversely, for the same amount of energy taken out of a system, the one with a higher entropy would decrease in energy less. Therefore, if you take energy from the higher temperature system and put it into the lower temperature system, there will be a net increase in entropy. 
            This will change the temperatures of the systems, bringing them closer together. This exchange in energy will stop when the temperatures are the same, or when there is no longer a possible increase in energy from taking energy from one system and putting it in another.



            <h3>Enthalpy and the Thermodynamic Definition of Entropy</h3>
            <h4> Deriving the Thermodynamic Definition of Entropy</h4>
            By the differential form of the first law of Thermodynamics, we have
            $$dU=(\frac{\partial U}{\partial S})_{V,N}dS+(\frac{\partial U}{\partial V})_{S,N}dV+\sum(\frac{\partial U}{\partial N_i})_{S,V}dN_i$$
            $$=TdS-p_{ext}dV+\sum\mu_idN_i$$
            *Negative sign for pressure since our dV is pointing in opposite direction of \(p_{ext}\).
            Let us define a quantity called Enthalpy, denoted by H, as follows:
            $$H\equiv U+PV$$
            Through the <a href="https://en.wikipedia.org/wiki/Product_rule">product rule</a> for differentiation:
            $$dH=dU+d(PV)=dU+(pdV+Vdp+dVdp)$$
            Substituting in the total differential form of the first law from above for dU:
            $$=(TdS-pdV)+pdV+Vdp+dVdp$$
            The dVdp term goes to 0 in the limit. The pdV term from work cancels out with one of the terms from the product rule for d(PV).
            $$=TdS+Vdp$$
            If we are working in a constant temperature environment, we can let dp=0, yielding the formula
            $$dH=TdS=dq$$

            $$dS=\frac{dq_{rev}}{T}$$
            This is the so-called "Thermodynamic definition of Entropy" which holds for a reversible process at constant pressure<br>


<h4>Irreversible Processes</h4>

            However, what about for irreversible processes? The first law still holds such that 
            $$U=q+w$$
            but
            \(q_{rev}\neq q_{irr}\) 
            and 
            \(w_{rev}\neq w_{irr}\).
            A more general statement which holds for irreversible processes is
            $$dS\geq \frac{\delta Q}{T}$$
            with equality when the process is reversible. This is known as the Clausius Inequality, which is another way of formulating the 2nd law.<br>
            
            <h4>An Aside on Legendre Transforms</h4>
            In the first law, we had
            $$dU=TdS-pdV$$
            and when we defined Enthalpy as \(H\equiv U+PV\), we ended up getting
            $$dH=TdS+Vdp$$
            By adding PV, we were able to switch the positions of the Volume and Pressure variables (along with a sign change). 
            By doing this, we can now have a quantity that is more convenient to work with when we assume different conditions (e.g. constant pressure instead of constant volume). 
            This is from which we got heat being the change in energy at a fixed volume and the change in enthalpy at fixed pressure equivalencies (set dV=0 or dP=0 in the above equations). 
        <br><br>
            This is a theme that will recur for the Helmholtz and Gibbs free energies where instead we will switch temperature and entropy, as dT=0 is more feasible than dS=0. 
            These paired variables are known as "conjugate variables," and the process of switching the differential between the extensive and intensive variable is known as a Legendre transformation.

            <h4>Helmholtz Free Energy and Gibbs Free Energy</h4>

            Let us define a quantity Helmholtz Free Energy, denoted by A (The letter A was chosen for the German word for work as H was already being used for Enthalpy; some books use F for free energy):
            $$A\equiv U-TS$$
            Doing a similar trick as we did for Enthalpy - taking the derivative of both sides and using the linearity of the derivative operator:
            $$dA=dU-d(TS)$$
            Substituting in for dU using the first law; product rule for d(TS)
            $$dA=(TdS-pdV)-(TdS+SdT+dSdT)$$
            Canceling out TdS terms and dSdT goes to 0
            $$dA=-pdV-SdT$$

            If we are working in a constant temperature system (dT=0), then we find that the change in Helmholtz free energy is the amount of work being done. 
            <br><br>
            It is also possible to do what we did for Enthalpy and Helmholtz for a single quantity. 
            Let us define a quantity Gibbs Free Energy, denoted by G:

            $$G\equiv U+PV-TS=A+PV=H-TS$$
            In general chemistry, Gibbs free energy is often introduced as
            $$G=H-TS$$
            so the derivation will go from that point.

            $$dG=dH-d(TS)$$
            Substituting in our above result for dH; product rule for d(TS)
            $$dG=(TdS+Vdp)-(TdS+SdT+dSdT)$$
            Cancelling out TdS terms; dSdT goes to 0
            $$dG=Vdp-SdT$$

        </div>
        <div id="Statistical-Mechanics">
            <h2>Statistical Mechanics</h2>
            <h3>Boltzmann Distribution</h3>



        </div>
        <div id="Biochemistry">
            <h2>Biochemistry</h2>
            <h3>Acid-Base Chemistry</h3>
            <h4>Definitions of Acid and Base</h4>
            There are 3 main basic definitions of acid-base chemistry:
            <ul>
                <li>Arrhenius: increases concentration of H+ (acid) or OH- (base)</li>
                <li>Bronsted-Lowry: proton donor (acid) and proton acceptor (base)</li>
                <li>Lewis: electron pair acceptor (acid) and electron pair donor (base)</li>
            </ul>
            The Arrhenius definition has been supplanted by the Bronsted-Lowry one, as it's more general. 
            The Lewis definition is also very important and its acids and bases are also referred to as electrophiles and nucleophiles respectively. 
            For clarity, future uses of the words "acid" and "base" will refer to the Bronsted-Lowry definition; Lewis acids and bases will be referred to by their aforementioned organic chemistry terms.

            <h4>Equilibrium Constants</h4>

            To be filled later

            <h4>Henderson-Hasselbach Equation and pKa</h4>

            Let us start with a generic acid dissociation equation from before with an acid dissociation constant \(K_A\):

            $$HA+H_2O\rightleftharpoons A^-+H_3O^+$$

            The associated equilibrium equation is 

            $$K_A=\frac{[A^-][H_3O^+]}{HA}$$
            There are various ways you can algebraically manipulate the equation and take logs in order to get to the final equation, but I will get things on the right side prior to taking the log
            $$\frac{1}{[H_3O^+]}=\frac{1}{K_A}\times\frac{[A^-]}{[HA]}$$
            Taking the log of both sides.
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A}\times\frac{[A^-]}{[HA]})$$
            Using the multiplication and division rule for the argument:
            $$\log(\frac{1}{[H^+]})=\log(\frac{1}{K_A})+\log(\frac{[A^-]}{[HA]})$$
            Using a specific case of the log power rule, there is the relationship \(\log(1/a)=-\log(a)\), we get
            $$-\log([H^+])=-\log(K_A)+\log(\frac{[A^-]}{[HA]})$$
            By definition, \(pKa\equiv -\log(K_A)\) and \(pH\equiv -\log([H^+])\). Thus, we arrive at the Henderson-Hasselbach equation:
            $$pH=pKa+\log(\frac{[A^-]}{[HA]})$$

            This equation allows us to relate the pH of an environment, the pKa of an acid, and the ratio of deprotonated to protonated occurences of that acid

            

            <h4>Amino Acids and Acid-Base Chemistry</h4>
            As amino acids have both amine and carboxylic acid groups, they can all act as both acids and bases to some extent. 
            In fact, at a neutral pH, amino acids will be predominantly Zwitterions (molecules that have both positive and negative charges); the amine group next to the alpha carbon will be protonated and the carboxylic acid group next to the alpha carbon will be deprotonated. 
            <br>
            However, a number of the proteogenic amino acids can also participate in acid-base chemistry through their sidechains. 

            <p style="width: 90%; text-align: center; margin: auto;"><a href="https://commons.wikimedia.org/wiki/File:Proteinogenic_Amino_Acid_Table.png#/media/File:Proteinogenic_Amino_Acid_Table.png"><img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Proteinogenic_Amino_Acid_Table.png" alt="Proteinogenic Amino Acid Table.png" style="max-width: 100%;"></a><br>By <a href="https://commons.wikimedia.org/w/index.php?title=User:Thomas.ryckmans68&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="User:Thomas.ryckmans68 (page does not exist)">Thomas.ryckmans68 <span class="int-own-work" lang="en"&gt;Own work&lt;/span&gt;, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=117084381">Link</a></p>
            <br>
            <br>

            One might ask - why do the amide groups in Asparagine and Glutamine not participate in acid-base chemistry? 
            The answer has to do with conjugated pi systems. The lone pair on the nitrogen is not "available" to grab a proton because it prefers to be in a conjugated system with the carbonyl. 
            For the same reason, in proteins, you will not see the amide backbone being charged, only the N and C termini.

        </div>

        <div id="Kinetics">
            <h2>Kinetics</h2>
            <h3>Refresher on Kinetics</h3>
                <h4>Elementary Reactions</h4>

                Let us consider elementary reactions - that is they have no intermediate steps:
                Let A, B be the reactants and P be the product (not Phosphorus). 



                For <b>single step</b> unidirectional reactions, we can write an equation that describes the rate of change in chemical species:

                $$A\rightarrow P$$

                Instinctively, if this is the only reaction taking place, the concentration of A should be going down by the same rate by which the concentration of P goes up.


                $$\frac{d[A]}{dt}=-\frac{d[P]}{dt}$$

                It would also be intuitive to think that the higher the concentration of A, the faster the reaction would go. 
                Probabilitistically, a justification would be that each A molecule has a chance per unit time of converting to P. 
                Then if each conversion is independent, you would expect the rate of conversion to be directly proportional to the number of A. 
                Thus, we also get the relationship.

                $$\frac{d[P]}{dt}=k[A]$$

                where k is a constant of proportionality which is specific to the reaction. We will call this the <b>rate constant</b>.

                Let us now consider a bimolecular, <b>single-step</b> unidirectional reaction:

                $$A+B\rightarrow P$$

                By similar logic, we can write the following relationships:

                $$\frac{d[A]}{dt}=\frac{d[B]}{dt}=-\frac{d[P]}{dt}$$

                $$\frac{d[P]}{dt}=k[A][B]$$

                <h4>Multiple Step Reactions</h4>


                <h4>Relationship with Equilibrium Constant</h4>

                <h4>Integrated Rate Laws</h4>

                This section will involve solving a few simple differential equations. 
                As thsi section is more concerned with the results and logic rather than the math, the derivations will not be especially rigorous and will be playing fast and loose with some math.


                Let us consider the simplest case, a reaction that has a rate-limiting step that is independent of concentration.

                $$\frac{d[P]}{dt}=k$$
                
                If the mathematicians aren't watching, you can "multiply" by dt to get

                $$d[P]=kdt$$
                While not good notation, we can itegrate from time t=0 to a a generic time t=t.

                $$\int_0^td[P]=\int_0^tkdt$$
                $$[P]_t-[P]_0=kt$$

                We would expect this as it is a constant rate process, so it should just be the rate multiplied by the amount of time.

                <br><br>
                Now for a more realistic example.

                Consider a reaction with the rate law:

                $$\frac{d[A]}{dt}=-k[A]$$

                Again we will want each variable to be on its own side. Using some algebra and abuse of notation, we get

                $$\frac{d[A]}{[A]}=-kdt$$

                We can treat it as if we were integrating each side seperately.

                $$\int \frac{d[A]}{[A]}=\int-kdt$$

                While it may appear intimidating, we are essentially just integrating \(\frac{1}{x}\) and a constant.

                $$ln([A]_t)-ln([A]_0)=-kt$$

                In these sorts of problems, once we integrate, we then try to isolate the concentration at time t.

                $$ln([A]_t)=ln([A]_0)-kt$$
                Exponentiating things, we get
                $$[A]_t=[A]_0e^{-kt}$$
                This is the <b>first order integrated rate law</b>
                Now, we have an equation that describes the concentration over time, not just the rate.

                For completeness, consider a rate law corresponding to a reaction with the rate law:

                $$\frac{d[A]}{dt}=-k[A]^2$$

                $$\frac{d[A]}{[A]^2}=-kdt$$

                $$\int\frac{d[A]}{[A]^2}=\int-kdt$$

                $$\frac{-1}{[A]_t}-\frac{-1}{[A]_0}=-kt$$
                $$\frac{1}{[A]_t}=\frac{1}{[A]_0}+kt$$
                $$[A]_t=\frac{1}{\frac{1}{[A]_0}+kt}$$

                So far, the integrated rate laws have been univariate. Differential equations with multiple variables are possible and are called "Partial Differential Equations" or PDEs. 

            <h3>Michaelis-Menten Kinetics</h3>
            <h4>Binding and Unbinding</h4>

            Let us picture a an enzymatic reaction. We have a substrate, S, which binds to an Enzyme, E, resulting in a bound complex, \(E\cdot S\). After some amount of time, the substrate is transformed into product, P, and the enzyme unbinds. 
            We will have the formation of the Enzyme-substrate complex be reversible, but consider the product formation to be irreversible.

            $$S+E\rightleftharpoons_{k_{-1}}^{k_1} E\cdot S \rightarrow^{k_{cat}} E+P$$

            

            <h4>Derivation</h4>

            There is a fixed amount of enzyme - either the enzyme is bound or unbound. Let us call the total amount of Enzyme, \(E_0\)

            $$E_0=[E]+[E\cdot S]$$
            We can write general, complex rate laws by looking at the flux into and out of each state
            $$\frac{d[X]}{dt}=\sum R_i-\sum R_{j}$$
            where R_i is the rate for a reaction step that produces X and R_j is the rate for a reaction step that depletes X. 

            $$\frac{d[S]}{dt}=k_{-1}[E\cdot S]-k_1[E][S]$$
            $$\frac{d[E]}{dt}=k_{-1}[E\cdot S]+k_{cat}[E\cdot S]-k_{1}[E][S]$$
            $$=(k_{-1}+k_{cat})[E\cdot S]-k_1[E][S]$$
            $$\frac{d[P]}{dt}=k_{cat}[E\cdot S]$$
            $$\frac{d[E\cdot S]}{dt}=k_1[E][S]-k_{-1}[E\cdot S]-k_{cat}[E\cdot S]$$

            A "steady-state approximation" is then made; that is, we treat the intermediate complex's concentration as a constant. 
            $$\text{Let }\frac{d[E\cdot S]}{dt}=0$$
            Using our above equations, we can then get
            $$k_{1}[E][S]=(k_{-1}+k_{cat})[E\cdot S]$$
            Since we are ultimately interested in the rate of product formation, we should look to that equation for a possible next step. 
            $$\frac{d[P]}{dt}=k_{cat}[E\cdot S]$$
            Since we see that there is \([E\cdot S]\), we should manipulate the previous equation to find \([E\cdot S]\) in terms of [E] and [S], especially since we do not directly know the concentration of the enzyme-substrate complex. We know how much substrate and enzyme there are initially. We also do not necessarily know how much unbound enzyme there is either as that would also require knowledge of how much bound enzyme there was. So let's try to reduce it from 2 unknowns to 1 unknown. 
            By using our total enzyme equation above, we can get
            $$k_{1}(E_0-[E\cdot S])[S]=(k_{-1}+k_{cat})[E\cdot S]$$
            Let's now algebraically manipulate the equation to isolate \(E\cdot S\)..
            $$k_{1}E_0[S]-k_{1}[E\cdot S][S]=(k_{-1}+k_{cat})[E\cdot S]$$
            $$k_{1}E_0[S]=(k_{1}[S]+(k_{-1}+k_{cat}))[E\cdot S]$$
            $$[E\cdot S]=\frac{k_1E_0[S]}{k_1[S]+(k_{-1}+k_{cat})}$$
            For simplicity, we will divide the numerator and denominator of the right side by \(k_1\)
            $$[E\cdot S]=\frac{E_0[S]}{\frac{k_{-1}+k_2}{k_1}+[S]}$$
            We can then plug \(E\cdot S\) in to the rate of product formation equation
            $$\frac{dP}{dt}=k_{cat}[E\cdot S]$$
            $$=\frac{k_{cat}E_0[S]}{\frac{k_{-1}+k_2}{k_1}+[S]}$$

            While this result may look unruly, it does tell us some nice properties. 
            Let us define a constant \(k_M\)
            $$k_M=\frac{k_{-1}+k_{cat}}{k_1}$$
            $$\frac{dP}{dt}=\frac{k_{cat}E_0[S]}{k_M+[S]}$$
            When the susbtrate concentration is equal to \(k_M\), the rate of production is at 50% of max rate.

            $$V_{max}\equiv k_{cat}E_0$$

            $$v=\frac{V_{max}[S]}{k_M+[S]}$$









            <h4>More Complicated Dynamics</h4>

            <h3>Diffusion</h3>
        </div>

        <div id="Stochastic-Processes">
            <h2>Stochastic Processes</h2>
            <h3>Discrete Time Stochastic Processes</h3>

            <h4>Discrete-Time Markov Chains</h4>

            Imagine a system where a protein is in one of two states: A or E. Each minute you observe its state, and it has the ability to go from state A to E or E to A, but it does not necessarily switch. We will denote these observational periods at \(t=0, 1, \ldots\)
            <br><br>
            An important assumption that we will make is that only the current state determines the probability of switching conformations by the second; the state that it was in prior to this observation have <b>no</b> impact on the chances. 
            E.g. if we are at t=n, the state that the protein was in at t=n-1 has no impact on the probability for t=n+1. 
            This is called the <b>Markov property</b>.
            
            <br><br>
            
            For simplicity's state, we will assume that these observations are discrete events and not worry about what happens outside of our regular 1-minute interval observations. 
            

            <figure><a href="https://commons.wikimedia.org/wiki/File:Markovkate_01.svg#/media/File:Markovkate_01.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/2/2b/Markovkate_01.svg" alt="Markovkate 01.svg" style="max-height: 300px;" ></a><br>By <a href="//commons.wikimedia.org/wiki/User:Joxemai4" title="User:Joxemai4">Joxemai4</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=10284158">Link</a></figure>

            We are given the dynamics of the protein conformational changes: 
                        <ul>
                <li>If the protein is in state A, it has a 40% chance of switching to the E state by the next observation and a 60% chance of remaining in the A state. 
                </li>
                <li>If the protein is in state E, it has a 70% chance of switching to the A conformation by the next observation and a 30% chance of remaining in the E state</li>
                        </ul>

            Given a start state, we can determine the probability, not just of the next minute, but subsequent observations as well. 
            
            This can be represented naturally using Linear Algebra. 

            $$\begin{bmatrix}
            0.6 & 0.4\\ 
            0.7 & 0.3
            \end{bmatrix}$$

            Where the row is the starting state and the column is the final state. 
<br><br>
            
            For our starting state vector, if we are in the A state, we can put a 1 in the first column and a 0 elsewhere:

            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}$$
            It is not actually necessary to know the starting state to make statements on the probability of the state over time. We can have our start be a probability distribution instead. 
            
            To get the probability of being in each state after 1 interval, we can multiply the two:
            $$\begin{bmatrix}
            1 & 0
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}=\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}$$

            We observe a 60% chance of remaining in state A and a 40% chance of transitioning to state E as expected at t=1. 
            
            We can use this result to see what the probability of being in each state are at t=2

            $$\begin{bmatrix}
            0.6 & 0.4
            \end{bmatrix}\begin{bmatrix}
                        0.6 & 0.4\\ 
                        0.7 & 0.3
                        \end{bmatrix}$$
                        $$=\begin{bmatrix}
            0.6*0.6+0.7*0.4 & 0.6*0.4+0.4*0.3
            \end{bmatrix}$$
            $$=\begin{bmatrix}
            0.64 & 0.36
            \end{bmatrix}$$
            We can view this as plugging in a probability distribution into a Markov chain rather than knowing the initial state.
            A general formula for the probability distribution at time t=n would thus be:

            $$\begin{bmatrix}
            p_{A,t=0} & p_{E,t=0}
            \end{bmatrix}\begin{bmatrix}
                        p_{A\rightarrow A} & p_{A\rightarrow E}\\ 
                        p_{E\rightarrow A} & p_{E\rightarrow E}
                        \end{bmatrix}^n$$
            But what are the long-run probabilities? What if we just came across this natively (assuming the same dynamics) - what would we expect our chances of finding it in a given state are? We can figure out what the distribution would be by taking the limit as \(n\rightarrow\infty\).

            <br><br>


            Additionally, we are not restricted to just 2 states, our probability vector could have m columns for m possible states and our transition matrix would be mxm. 
            Markov chains can even be abstracted to infinite-state versions, although some properties change, which are beyond the scope of this textbook.

            Furthermore, the concept can be extended to continuous time processes. 
            <br><br>



            <h3>Continuous-Time Stochastic Processes</h3>
            <h4>Poisson Process</h4>
            When we were working with discrete-time Markov chains, we had a matrix or depiction of
            $$P(X_{i,t=n+1}|X_{j,t=n})$$
            where \(X_i\) is the new possible state and \(X_j\) is the current state. 
            These probabilities are implicitly given based on the time interval. We do not consider what happens in between observation periods; it is possible that we observe the same state in consecutive observations, but the protein went from A to E then back to A in between.
            To get to continuous-time stochastic processes, we can take the limit as the time between observation periods goes to 0.
<br><br>
            For a full derivation, see <a href="https://www.math.uchicago.edu/~lawler/finbook.pdf">Lawler's Stochastic Calculus book</a> chapter 6.
            
<br><br>
            Let us first consider a process that can only increase. In a biology context, we can say that this is the total amount of mRNA produced over a period (not the mRNA level since mRNA can degrade and would go down). 
            Let assume that there is an mRNA production rate \(\lambda\) and define \(p(s)\) to be the probability that at least 1 mRNA moecule is produced during the interval [t,t+s]. 
            Then the probability of 1 mRNA molecule being produced in a short interval is approximately the rate times the duration of time. This can be formalized as 
            $$p(\Delta t)=\lambda \Delta t+o(\Delta t)$$

            Let \(X_s\) be the total number of molecules of mRNA produced by time t=s, T be the first time that a molecule of mRNA is produced, or formally:

            $$T=\text{inf}\{s:X_s=1\}$$

            From this, it can be shown that the probability that no molecules have been produced during an interval of length t is
            
            $$P\{T>t\}=e^{-\lambda*t}$$
            Thus, the time for between mRNA production is exponentially distributed like 
            $$f(t)=\lambda e^{-\lambda t}$$
            
            This distribution is nice as it has the "memory-less" property that our discrete-time Markov chains had.
            Now, this has only given us a distribution of how long it takes between transcription events, not a total amount. 
<br><br>
            Without deriving it, it can be shown that the distribution for the total number of "events" with exponentially distributed events has the following distribution:

            $$P(X_{t+s}-X_s=k)=e^{-\lambda t}\frac{(\lambda t)^k}{k!}$$

            This is known as a Poisson distribution, which is parameterized by \(\lambda\), the rate constant. 
            The distribution has the following properties:
            <ol>
                <li>Mean of \(\lambda t\) </li>
                <li>Variance of \(\lambda t\)</li>
            </ol>
            The Poisson process can be further generalized to allow for variable-sized jumps rather than just jumps of 1. 
            This is known as a "Compound Poisson Process." 


            <h4>Advanced Applications in Biology</h4>
            The classical example of stochastic processes in biology is mRNA expression levels - as it is a quantity that can both degrade and be produced at irregular intervals constantly. 

            
        </div>




        </div>

        <h2>Works Cited</h2>
        <ol>
            <li><a href="https://www.math.uchicago.edu/~lawler/finbook.pdf">https://www.math.uchicago.edu/~lawler/finbook.pdf</a></li>
        </ol>

</body>
</html>