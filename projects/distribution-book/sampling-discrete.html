<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Discrete Distribution Sampling</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Copula, Probability Integral Transform">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to discrete distribution sampling techniques.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/sampling-discrete"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Distribution-Sampling">
                <h1>Distribution Sampling</h1>
                <h2>Introduction to Distribution Sampling</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./probability-2.html">Probability II</a></li>
                    <li><a href="./sampling-continuous.html">Sampling (Continuous)</a></li>
                </ol>

                <h3>Introduction</h3>
                In this chapter, we will explore the methods used to sample from discrete probability distributions. 

                <br><br>
                As with the <a href="./sampling-continuous.html">continuous case</a>, this chapter will be more applied than theoretical. Because there a fewer common discrete distributions, we will focus more on specifcs than in the continuous chapter. 
                The main ones also tend to have common use cases in algorithms. 



                <h2>Common Distributions</h2>

                We will briefly review the common distributions.
                <h3>Bernoulli</h3>
                The most basic, binary case which I will assume you are familiar with. Its PMF is

                $$P(X=x)=\begin{cases}
                p & \text{if }x=1\\
                1-p & \text{if }x=0
                \end{cases}$$

                Due to it corresponding to a binary outcome with a given probability, this is one of the fundamental building blocks. Often this will be available from a library in your language of choice, but it is good to know how to implement it. 

                <h3>Binomial Distribution</h3>
                The number of successes in n independent Bernoulli trials has a PMF given by

                $$P(X=k)={n \choose k} p^k(1-p)^{n-k};\quad k=0,1,\dots,n$$

                <h3>Multinomial Distribution</h3>
                The multinomial distribution extends the binomial distribution into more than 2 possible outcomes.
                <br><br>
                Let n be the total number of trials; \(p_1,p_2,\dots,p_k\) be the probabilities for the k categories such that \(\sum_{i=1}^kp_i=1\); and \(x_1,x_2,\dots,x_k\) be the number of times each category is observed, where \(\sum_{i=1}^kx_i=n\).

                <br><br>
                The probability of a particular combination is given by the multinomial PMF:
                $$P(X_1=x_1,X_2=x_2,\dots,X_k=x_k)=\frac{n!}{x_1!x_2!\dots x_k!}p_1^{x_1}p_{2}^{x_2}\dots p_k^{x_k}$$

                If you only care about one outcome of many, you essentially reduce it to a binomial case where the outcomes are desired category and any other category. Thus, the marginal distribution is given by:

                $$P(X_i=x_i)={n\choose x_i}p_i^{x_i}(1-p_i)^{n-x_i}$$

                This can be a useful relationship for algorithms. 




                <h3>Geometric Distribution</h3>
                Models the number of trials until the first success in a series of Bernoulli trials. Can be extended to the Negative Binomial Distribution. 

                $$P(X=k)=(1-p)^{k-1}p;\quad k=1,2,3,\dots$$

                <h3>Negative Binomial Distribution</h3>

                Similar to how the binomial is the sum of independent Bernoulli trials, the negative binomial is a sum of Geometric distributions. 
                This particular distribution has an annoyingly high number of ways to parameterize it, but for the purpose of the book, we will use the convention that it corresponds to the number of failures in a series of Bernoulli trials until you get r successes, where r is a given integer. 

                $$P(X=k)={k+r-1 \choose k}(1-p)^kp^r$$



                <h3>Poisson Distribution</h3>
                Represents the number of events in a fixed interval of time or space when events occur independently at a constant rate \(\lambda\)
                $$P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!};\quad k=0,1,2,\dots$$


                <h2>General Sampling Methods</h2>

                The basics techniques from continuous sampling can also be applied to discrete distributions. 
                
                <h3>Inverse Transform Sampling</h3>
                Inverse Transform Sampling is a widely used technique for generating random samples from a given probability distribution, applicable to both discrete and continuous distributions. Despite the general similarities between the two cases, there are notable differences in implementation due to the nature of the cumulative distribution function (CDF) and the values the random variable can take. 
                <br><br>
                For discrete distributions, the process involves generating a random value from a uniform distribution and finding the smallest discrete value whose cumulative probability is greater than or equal to this uniform random variable. This is fundamentally different from the continuous case, where the inverse CDF is computed analytically or using some root-finding method. 

                <h4>Algorithm</h4>

                <h5>Compute the CDF</h5>

                Define the cumulative function F(x) for a discrete random variable. 

                $$F(x)=P(X\leq x)$$

                <h5>Generate a Uniform Random Variable</h5>
                Draw a random number U from a uniform distribution on the interval [0,1] where \(U\sim\mathcal{U}(0,1)\). 

                <h5>Find Smallest Value of X Satisfying Criterium</h5>

                The final step is finding the smallest x in the support such that \(F(x)\geq U\). The sampled value is the smallest such x.

                <h5>Example</h5>

                For a geometric distribution with parameter p,

                $$P(X=k)=(1-p)^{k-1}p\quad k=1,2,3,\dots$$

                The corresponding CDF is

                $$F(k)=1-(1-p)^k$$

                To sample, generate \(U\sim\mathcal{U}(0,1)\), and find the smallest k such that:

                $$1-(1-p)^k\geq U$$

                Rearranging, 

                $$k=\left\lceil\frac{\log(1-U)}{\log(1-p)}\right\rceil$$
                This is one case where we can get an easy answer through algebra, but this is not typical.

                <h3>Rejection Sampling</h3>
                Rejection Sampling is a method used when the distribution is difficult to sample directly, but a simpler distribution bounds its PMF. This method relies on generating samples from an easy-to-sample proposal distribution and then rejecting some of them based on a condition. This is very similar to the continuous case. 

                <h4>Algorithm</h4>
                <h5>Choose a proposal distribution</h5>
                The proposal distribution q(x) is the one that you will actually sample from. As in the continuous case, it must satisfy 
                $$p(x)\leq cq(x)\quad\forall x$$
                where p is our desired pmf. 

                <h5>Generate X from the Proposal Distribution</h5>
                Sample from the proposal distribution and get a sample value x from it

                <h5>Generate a U(0,1)</h5>
                Generate a uniform random variable \(U\sim\mathcal{U}(0,1)\).

                <h5>Check Acceptance Criterium</h5>
                We accept the sample x if 

                $$U\leq \frac{P(X=x)}{cq(x)}$$

                If this condition does not hold, repeat the process starting from generating x from the proposal distribution. 

                <h3>Alias Method</h3>
                The Alias Method is one that is unique to discrete distributions, so we haven't covered it in the prior chapter. 
                It's useful when there are many, but finite, possible values that the distribution could output. It is able to perform sampling at O(1) once the preprocessing is done, which makes it useful when many samples are needed to be drawn. 
                For other methods, the time complexity of sampling could be O(n) if you need to traverse the support and compare it to the CDF, such as with inverse transform sampling.

                <h4>Pre-processing</h4>
                The Alias method involves constructing two tables: 
                <ol>
                    <li>Probability Table: Probabilities of the enumerated outcomes that are scaled such that it makes it easier to uniformly sample</li>

                    <li>Alias Table: Stores the secondary outcomes that are used when the primary outcome's probability isn't high enough.</li>
                </ol>

                Each entry in these tables corresponds to one of the possible outcomes, and the sampling algorithm involves selecting a random index and deciding which outcome to return based on a simple comparison.

                <h5>Normalize the Probabilities</h5>
                Scale the original weights such that they sum up to 1 then multiply each probability by n, so that the average probability becomes 1:

                $$p_{i}'=p_i\times n$$

                Now, we have a set of modified probabilities \(p_{1}',p_{2}',\dots,p_{n}'\), where some values will be greater than 1 ("heavy"), and others will be less than 1 ("light").


                <br><br>
                We create two lists:

                <ul>
                    <li>A list of indices for overfilled outcomes</li>
                    <li>A list of indices for underfilled outcomes</li>
                </ul>

                <h5>Construct the Probability and Alias Tables</h5>

                <ol>
                    <li>Probability table: stores values that correspond to the adjusted probabilities for each outcome</li>
                    <li>Alias table: stores indices for the "alias" outcomes, which represent the secondary outcome that can be sampled if the primary outcome is not chosen.</li>
                </ol>


                Now the probabilities have to be balanced out:

                <ul>
                    
                    <li>Select an underfilled outcome: take an index i from the list of underfilled outcomes. The probability of this outcome is less than 1, so it cannot be directly sampled from.</li>
                    <li>Select an Overfilled Outcome: Take an index j from the list of overfilled outcomes. The probability for this is greater than 1, so we can "transfer" part of its excess to the underfilled outcome</li>
                    <li>Set the entry in the <b>Probability Table</b> for outcome i to its current scaled probability \(p_{i}'\) (which is less than 1).</li>
                    <li>Set the entry in the Alias Table for outcome i to point to outcome j, meaning the outcome j will serve as the alias for outcome i</li>
                    <li>Adjust the probability for the overfilled outcome j by reducing it by the amount we gave to outcome i.
                        $$p_j'\leftarrow p_j'-(1-p_i')$$
                        where \(\leftarrow\) is the assignment operator. Now, the outcome i has total probability 1 (split between itself and outcome j), and the overfilled outcome j has less excess probability.

                    </li>
                    <li>Update the lists: if the probability for j has fallen below 1, move it to the list of underfilled outcomes. If j is still overfilled, leave it in the list of overfilled outcomes.</li>
                
                </ul>
                
                Repeat the above steps until all probabilities are close to 1. At the end of the process, each entry in the Probability Table will have a value less than or equal to 1, and each entry in the Alias table will store an alias for the corresponding outcome. Both lists should be empty. If not, there might be an outcome left that is close to 1. Just set that probability to 1 in the Probability Table and have it be its own alias. 

                <h4>Sampling</h4>
                <h5>Choose index</h5>
                Pick a random index i uniformly from \(\{1,2,\dots,n\}\).

                <h5>Sample Primary of Secondary</h5>
                We now simulate a Bernoulli trial with p being the value in the Probability Table for index i. If success, then return i; else, return the alias stored in the Alias table at index i.
                


                <h2>Distribution-Specific Algorithms</h2>

                While general methods like Inverse Transform Sampling and Rejection Sampling can be applied to any distribution, there are distribution-specific algorithms that offer more efficient sampling for particular distributions. These algorithms often take advantage of the structure of the distribution's probability mass function (PMF) or cumulative distribution function (CDF). Other properties that can be taken advantage of are convolution relationships, such as Bernoulli and Binomial; Geometric and Negative Binomial. 



                <h3>Bernoulli</h3>
                The Bernoulli distribution can be sampled directly using a uniform random variable. 

                <h4>Generate a U(0,1)</h4>
                Generate a random uniform number \(U\sim\mathcal{U}(0,1)\)

                <h4>Compare Against P</h4>
                If \(U\leq p\), return 1; else, return 0.

                <h3>Binomial</h3>

                <h4>Direct Summation</h4>
                We can take advantage of the fact that the sum of iid Bernoullis is equivalent to a Binomial distribution. 

                <br><br>
                For \(i=1\) to \(n\), sample \(X_i\sim\) Bernoulli(p). Return the sum of all \(X_i\)

                <h5>Python Implemnetation</h5>
                <code style="margin: auto; width: 50%;"><pre>
import random

def sample_bernoulli(p):
    U = random.uniform(0, 1)
    if U <= p:
        return 1
    else:
        return 0
                    
def sample_binomial(n, p):
    successes = 0
    for _ in range(n):
        successes += sample_bernoulli(p)
    return successes
                                           
                                    </pre></code>
                <br><br>
                This has time complexity of O(n). While this is a simple algorithm and not super computationally heavy, if you can avoid generating many uniform random variables, it can sometimes lead to noticeably better performance. 


                <h4>BINV</h4>

                For large n, an alternative is the BINV Algorithm, which uses a similar approach to inverse transform sampling.

                <h5>Python Implementation</h5>
                <code style="margin: auto; width: 50%;"><pre>
import math
import random

def sample_binomial_binv(n, p):
    q = (1 - p)**n
    F = q
    X = 0
    U = random.uniform(0, 1)
    
    while U > F:
        X += 1
        q = q * (n - X + 1) * p / (X * (1 - p))
        F += q
    
    return X
                    


                </pre></code>


                <h3>Multinomial</h3>
                
                <h4>Naive Algorithm</h4>
                Similar to the binomial distribution, we can just sample n times from the associated categorical distribution. 

                $$P(k)=p_k$$

                The categorical distribution sampling algorithm is very similar to that of the Bernoulli. The only difference is that we no longer have a single threshold. 

                <h5>Generate a U(0,1)</h5>
                Same as with the Bernoulli, we generate a realization \(U\sim \mathcal{U}(0,1)\). 

                <h5>Use cumulative probability</h5>

                While the Bernoulli had a threshold based on p, we have more than 3 areas that we break the line segment from 0 to 1 into. We do this based on something similar to a CDF:

                $$P(C_i)=\sum_{j=1}^ip_j$$

                We then output the sample based on which region it lies in. An example will make this more clear.


                <h5>Example</h5>

                Let's say we have k=3, and our p-vector is \([0.2,0.3,0.5]\). Our thresholds are 

                <ul>
                    <li>k=1 if \(0\lt U \leq 0.2\)</li>
                    <li>k=2 if \(0.2 \lt U \leq 0.5\)</li>
                    <li>k=3 if \(U \gt 0.5\)</li>
                </ul>

                </div>



             




                <div id="Exercises">
                    <h2>Discrete Sampling Practice Problems</h2>

                    <ol>
                        <li>Use rejection sampling to generate samples from a Beta distribution with \(\alpha=2,\beta=3\). Use a Uniform as the proposal distribution and scale appropriately. 
                            For reference, the non-normalized PDF for a Beta Distribution is \(f(x|\alpha,\beta)=x^{\alpha-1}(1-x)^{\beta-1}\).
                        </li>
                        <li>Use rejection sampling to generate samples from a Raised Cosine distribution with \(\mu=0,s=1\). Use a Uniform as the proposal distribution and scale appropriately. 
                            For reference, the PDF for a Raised Cosine Distribution is \(f(x|\mu,s)=\frac{1}{2s}\left[1+\cos\left(\frac{x-\mu}{s}\pi\right)\right]\) with \(x\in[\mu-s,\mu+s]\).
                        </li>
                        <li>Use rejection sampling to generate samples from a Wigner Semicircle distribution with \(R=2\). Use a Uniform as the proposal distribution and scale appropriately. 
                            For reference, the PDF for a Wigner Semicircle Distribution is \(f(x|R)=\frac{2}{\pi R^2}\sqrt{R^2-x^2}\) with \(x\in[-R,R]\).
                        </li>
                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>