<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Discrete Distribution Sampling</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Copula, Probability Integral Transform">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to discrete distribution sampling techniques.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/sampling-discrete"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Distribution-Sampling">
                <h1>Distribution Sampling</h1>
                <h2>Introduction to Distribution Sampling</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./probability-2.html">Probability II</a></li>
                    <li><a href="./sampling-continuous.html">Sampling (Continuous)</a></li>
                </ol>

                <h3>Introduction</h3>
                In this chapter, we will explore the methods used to sample from discrete probability distributions. 

                <br><br>
                As with the <a href="./sampling-continuous.html">continuous case</a>, this chapter will be more applied than theoretical. Because there a fewer common discrete distributions, we will focus more on specifcs than in the continuous chapter. 
                The main ones also tend to have common use cases in algorithms. 



                <h2>Common Distributions</h2>

                We will briefly review the common distributions.
                <h3>Bernoulli</h3>
                The most basic, binary case which I will assume you are familiar with. Its PMF is

                $$P(X=x)=\begin{cases}
                p & \text{if }x=1\\
                1-p & \text{if }x=0
                \end{cases}$$

                Due to it corresponding to a binary outcome with a given probability, this is one of the fundamental building blocks. Often this will be available from a library in your language of choice, but it is good to know how to implement it. 

                <h3>Binomial Distribution</h3>
                The number of successes in n independent Bernoulli trials has a PMF given by

                $$P(X=k)={n \choose k} p^k(1-p)^{n-k};\quad k=0,1,\dots,n$$

                <h3>Geometric Distribution</h3>
                Models the number of trials until the first success in a series of Bernoulli trials. Can be extended to the Negative Binomial Distribution. 

                $$P(X=k)=(1-p)^{k-1}p;\quad k=1,2,3,\dots$$

                <h3>Negative Binomial Distribution</h3>

                Similar to how the binomial is the sum of independent Bernoulli trials, the negative binomial is a sum of Geometric distributions. 
                This particular distribution has an annoyingly high number of ways to parameterize it, but for the purpose of the book, we will use the convention that it corresponds to the number of failures in a series of Bernoulli trials until you get r successes, where r is a given integer. 

                $$P(X=k)={k+r-1 \choose k}(1-p)^kp^r$$



                <h3>Poisson Distribution</h3>
                Represents the number of events in a fixed interval of time or space when events occur independently at a constant rate \(\lambda\)
                $$P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!};\quad k=0,1,2,\dots$$


                <h2>General Sampling Methods</h2>

                The basics techniques from continuous sampling can also be applied to discrete distributions. 
                
                <h3>Inverse Transform Sampling</h3>
                Inverse Transform Sampling is a widely used technique for generating random samples from a given probability distribution, applicable to both discrete and continuous distributions. Despite the general similarities between the two cases, there are notable differences in implementation due to the nature of the cumulative distribution function (CDF) and the values the random variable can take. 
                <br><br>
                For discrete distributions, the process involves generating a random value from a uniform distribution and finding the smallest discrete value whose cumulative probability is greater than or equal to this uniform random variable. This is fundamentally different from the continuous case, where the inverse CDF is computed analytically or using some root-finding method. 

                <h4>Algorithm</h4>

                <h5>Compute the CDF</h5>

                Define the cumulative function F(x) for a discrete random variable. 

                $$F(x)=P(X\leq x)$$

                <h5>Generate a Uniform Random Variable</h5>
                Draw a random number U from a uniform distribution on the interval [0,1] where \(U\sim\mathcal{U}(0,1)\). 

                <h5>Find Smallest Value of X Satisfying Criterium</h5>

                The final step is finding the smallest x in the support such that \(F(x)\geq U\). The sampled value is the smallest such x.

                <h5>Example</h5>

                For a geometric distribution with parameter p,

                $$P(X=k)=(1-p)^{k-1}p\quad k=1,2,3,\dots$$

                The corresponding CDF is

                $$F(k)=1-(1-p)^k$$

                To sample, generate \(U\sim\mathcal{U}(0,1)\), and find the smallest k such that:

                $$1-(1-p)^k\geq U$$

                Rearranging, 

                $$k=\left\lceil\frac{\log(1-U)}{\log(1-p)}\right\rceil$$
                This is one case where we can get an easy answer through algebra, but this is not typical.

                <h3>Rejection Sampling</h3>
                Rejection Sampling is a method used when the distribution is difficult to sample directly, but a simpler distribution bounds its PMF. This method relies on generating samples from an easy-to-sample proposal distribution and then rejecting some of them based on a condition. This is very similar to the continuous case. 

                <h4>Algorithm</h4>
                <h5>Choose a proposal distribution</h5>
                The proposal distribution q(x) is the one that you will actually sample from. As in the continuous case, it must satisfy 
                $$p(x)\leq cq(x)\quad\forall x$$
                where p is our desired pmf. 

                <h5>Generate X from the Proposal Distribution</h5>
                Sample from the proposal distribution and get a sample value x from it

                <h5>Generate a U(0,1)</h5>
                Generate a uniform random variable \(U\sim\mathcal{U}(0,1)\).

                <h5>Check Acceptance Criterium</h5>
                We accept the sample x if 

                $$U\leq \frac{P(X=x)}{cq(x)}$$

                If this condition does not hold, repeat the process starting from generating x from the proposal distribution. 

                <h2>Distribution-Specific Algorithms</h2>

                While general methods like Inverse Transform Sampling and Rejection Sampling can be applied to any distribution, there are distribution-specific algorithms that offer more efficient sampling for particular distributions. These algorithms often take advantage of the structure of the distribution's probability mass function (PMF) or cumulative distribution function (CDF). Other properties that can be taken advantage of are convolution relationships, such as Bernoulli and Binomial; Geometric and Negative Binomial. 



                <h3>Bernoulli</h3>
                The Bernoulli distribution can be sampled directly using a uniform random variable. 

                <h4>Generate a U(0,1)</h4>
                Generate a random uniform number \(U\sim\mathcal{U}(0,1)\)

                <h4>Compare Against P</h4>
                If \(U\leq p\), return 1; else, return 0.

                <h3>Binomial</h3>

                <h4>Direct Summation</h4>
                We can take advantage of the fact that the sum of iid Bernoullis is equivalent to a Binomial distribution. 

                <br><br>
                For \(i=1\) to \(n\), sample \(X_i\sim\) Bernoulli(p). Return the sum of all \(X_i\)

                <h5>Python Implemnetation</h5>
                <code style="margin: auto; width: 50%;"><pre>
import random

def sample_bernoulli(p):
    U = random.uniform(0, 1)
    if U <= p:
        return 1
    else:
        return 0
                    
def sample_binomial(n, p):
    successes = 0
    for _ in range(n):
        successes += sample_bernoulli(p)
    return successes
                                           
                                    </pre></code>
                <br><br>
                This has time complexity of O(n). While this is a simple algorithm and not super computationally heavy, if you can avoid generating many uniform random variables, it can sometimes lead to noticeably better performance. 


                <h4>BINV</h4>

                For large n, an alternative is the BINV Algorithm, which uses a similar approach to inverse transform sampling.

                <h5>Python Implementation</h5>
                <code style="margin: auto; width: 50%;"><pre>
import math
import random

def sample_binomial_binv(n, p):
    q = (1 - p)**n
    F = q
    X = 0
    U = random.uniform(0, 1)
    
    while U > F:
        X += 1
        q = q * (n - X + 1) * p / (X * (1 - p))
        F += q
    
    return X
                    


                </pre></code>



                </div>



             




                <div id="Exercises">
                    <h2>Discrete Sampling Practice Problems</h2>

                    <ol>

                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>