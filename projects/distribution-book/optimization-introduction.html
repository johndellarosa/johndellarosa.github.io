<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Introduction to Optimization</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Copula, Probability Integral Transform">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to optimization algorithms and fitting parameters.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/optimization-introduction"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Optimization">
                <h1>Introduction to Optimization</h1>
                <h2>Basic Terminology</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./probability-2.html">Probability II</a></li>
                    <!-- <li><a href="./sampling-continuous.html">Sampling</a></li> -->
                    <!-- <li><a href="./multivariate-intro">Introduction to Multivariate Distributions</a></li> -->
               
                </ol>

                In the last section, we covered parameter estimation strategies. In this section, we will continue coming that, but with a focus on the algorithm and optimization.

                <h3>Fundamentals of Optimization</h3>

                Optimization is the process of finding the value of a parameter \(\theta\) that results in the maximum or minimum value of the given function \(f(\theta)\). 
                <ul>
                    <li>Maximization: Finding \(\theta\) such that \(f(\theta)\) is as large as possible. This value of \(f(\theta)\) is the maximum and \(\theta\) is called the maximizer.</li>
                    <li>Minimization: Finding \(\theta\) such that \(f(\theta)\) is as small as possible. This value of \(f(\theta)\) is the minimum and \(\theta\) is called the minimizer.</li>
                </ul>
                In practice, an maximization problem can be turned into a minimization problem through multiplying the objective function by -1, so any technique applicable to one works on the other through slight modification. 
                <h4>Objective Function</h4>
                An objective function is a mathematical expression that we aim to maximize or minimize with respect to the parameter \(\theta\).

                <h5>Likelihood Function</h5>
                This measures the probability of observing the given data \(X\) under the parameter \(\theta\). 

                $$L(\theta)=P(X|\theta)$$

                <h6>Example: Bernoulli</h6>

                Consider a Bernoulli distribution where X can be 1 with probability p or 0 with probability \(1-p\). 

                $$L(p)=\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$$

                <h5>Log-Likelihood Function</h5>

                Often it is more convenient to work in the log-likelihood since the logarithm turns products into sums.

                $$\ell(\theta)=\log L(\theta)$$

                <h6>Bernoulli Log-Likelihood</h6>

                $$\ell(p)=\sum_{i=1}^{n}[x_i\log(p)+(1-x_i)\log(1-p)]$$

                <h5>Loss Function</h5>
                A general term which measures the discrepancy between observed values and those predicted by a model with parameter \(\theta\). One example is the sum of squared residuals (SSR).

                <h2>Analytical Optimization</h2>
                Analytical optimization involves using calculus.
                <h3>First Derivative Test</h3>
                The first derivative test looks at points where the first derivative of the function is zero or undefined.

                <ol>
                    <li>Compute the first derivative \(f'(\theta)\)</li>
                    <li>Solve \(f'(\theta)=0\) to find critical points</li>
                </ol>
                Why does this make sense? If we are trying to minimize (maximize) a function and the derivative is non-zero, then we can further decrease (increase) it by increasing (decreasing) \(\theta\).


                <h3>Second Derivative Test</h3>

                <ul>
                    <li>If \(f''(\theta)\gt 0\), the function is concave upward, indicating a local minimum</li>
                    <li>If \(f''(\theta)\lt 0\), the function is concave downward, indicating a local maximum.</li>
                </ul>

                <ol>
                    <li>Compute the second derivative \(f''(\theta)\)</li>
                    <li>Evaluate \(f''(\theta)\) at each critical point</li>
                </ol>

                <h3>Example: MLE for Mean of a Normal</h3>

                Example: Estimate the mean \(\mu\) of a normal distribution with a known variance \(\sigma^2\), given data \(X=\{x_1,x_2,\dots,x_n\}\).

                $$L(\mu)=\prod_{i=1}^{n}\frac{1}{2\pi\sigma^2}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2})$$

                Taking the log:

                $$\ell(\mu)=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2$$

                Computing the first derivative:

                $$\frac{d\ell}{d\mu}=-\frac{1}{\sigma^2}\sum_{i=1}^{n}(x_i-\mu)(-1)=\frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu)$$
                Set derivative to zero:

                $$\frac{d\ell}{d\mu}=0\Longrightarrow \sum_{i=1}^{n}(x_i-\mu)=0$$

                $$\mu=\frac{1}{n}\sum_{i=1}^{n}x_i$$

                Second derivative test:

                $$\frac{d^2\ell}{d\mu^2}=-\frac{n}{\sigma^2}\lt 0$$

                Since the second derivative is negative, the critical point corresponds to a maximum.

                $$\hat{\mu}=\bar{x}$$

                <h2>Numerical Optimization Methods</h2>
                For more complex problems, it is not feasible or possible to get a closed form answer. Numerical Methods use an iterative approach by having an estimate point, taking derivatives there, and then moving the estimate point accordingly. 

                <h3>Newton-Raphson Method</h3>

                The Newton-Raphson uses the first and second derivative to adjust the estimate.

                <ol>
                    <li>Initial Guess: Choose an initial estimate \(\theta_0\)</li>
                    <li>Iteration: \(\theta_{k+1}=\theta_k-\frac{f'(\theta_k)}{f''(\theta_k)}\)</li>
                    <li>Convergence Check: If \(|\theta_{k+1}-\theta_k|\lt \varepsilon\), stop; else, repeat</li>
                </ol>

                For the Newton-Raphson Method, the function must be twice differentiable, with the second derivative not being zero at the optimum. 

                <h4>Example: Exponential Distribution</h4>
                The Exponential distribution has a simple closed form MLE solution, but for pedagogical reasons, we will apply the Newton-Raphson Method to estimate \(\lambda\).

                <br><br>
                Given data \(X=\{x_1,x_2,\dots,x_n\}\) from an exponential distribution, estimate \(\lambda\):

                $$f(x;\lambda)=\lambda e^{-\lambda x}$$

                $$L(\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i}=\lambda^n e^{-\lambda \sum x_i}$$

                $$\ell(\lambda)=n \log(\lambda)-\lambda\sum_{i=1}^n x_i$$
                Now for Newton-Raphson, we have to calculate the first and second derivative:
                $$\ell'(\lambda)=\frac{n}{\lambda}-\sum x_i$$

                $$\ell"(\lambda)=-\frac{n}{\lambda^2}$$

                Thus, we get the iteration formula:

                $$\lambda_{k+1}=\lambda_k-\frac{\ell'(\lambda_k)}{\ell"(\lambda_k)}=\lambda_k-\left(\frac{\frac{n}{\lambda_k}-\sum x_i}{-\frac{n}{\lambda_{k}^2}}\right)$$

                This simplifies to

                $$\lambda_{k+1}=\lambda_k+\left(\frac{n-\lambda_k\sum x_i}{n}\right)$$
                
                
                <h3>Secant Method</h3>
            The Secant Method is similar to Newton-Raphson but doesn't require computation of the second derivative. 

            <h4>Algorithm</h4>
            <ol>
                <li>Initial Estimates: Choose two initial guesses \(\theta_0\) and \(\theta_1\)</li>
                <li>Iteration: \(\theta_{k+1}=\theta_k-f'(\theta_k)\left(\frac{\theta_k-\theta_{k-1}}{f'(\theta_k)-f'(\theta_{k-1})}\right)\)</li>
                <li>Convergence Check: If \(\theta_{k+1}-\theta_k|\lt \varepsilon\), stop; else, repeat.</li>
            </ol>

            <h3>Bisection Method</h3>
            This technique can be used to find where the derivative is 0 (or more generally a function). 
            

            <h4>Algorithm</h4>

            <ol>
                <li>Initial Interval: Choose \([a,b]\) such that \(f(a)\cdot f(b)\lt 0\)</li>
                <li>Compute Midpoint: \(c=\frac{a+b}{2}\)</li>
                <li>Check Sign:
                    <ul>
                        <li>If \(f(a)\cdot f(c)\lt 0\), set \(b\leftarrow c\)</li>
                        <li>Else, set \(a\leftarrow c\)</li>
                    </ul>

                </li>
                <li>Convergence Check: If \(|b-a|\lt \varepsilon\), stop; else repeat</li>
            </ol>
            
            
            
            </div>


            <div>
                <h2>Gamma Distribution Example</h2>

                The Gamma distribution is a continuous probility distribution whose PDF is given by:

                $$f(x;\alpha,\beta)=\frac{x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}$$
                where
                <ul>
                    <li>\(x\in(0,\infty)\)</li>
                    <li>\(\alpha\) is the shape parameter \((\alpha\gt 0)\)</li>
                    <li>\(beta\) is the scale parameter \((\beta\gt 0)\)</li>
                    <li>\(\Gamma(\cdot)\) is the gamma function</li>
                </ul>

                For our example, we will assume that \(\beta\) is known, so estimating the shape parameter \(\alpha\) becomes a univariate optimization problem.

                <h3>Log-Likelihood</h3>
                Given a sample of n independent observations \(\{x_i\}\) from a Gamma distribution with known \(\beta\) and unknown \(\alpha\), the log-likelihood function is:

                $$\ell(\alpha)=(\alpha-1)\sum_{i=1}^{n}\log(x_i)-\frac{\sum_{i=1}^{n}x_i}{\beta}-n\alpha\log(\beta)-n\log(\Gamma(\alpha))$$


                To find the MLE of \(\alpha\), we need to solve \(\ell'(\alpha)=0\), where \(\ell'(\alpha)\) is the first derivative of the log-likelihood function with respect to \(\alpha\), also known as the score function.

                <br>

                $$S(\alpha)=\frac{d\ell}{d\alpha}=\sum_{i=1}^{n}\log(x_i)-n\log(\beta)-n\psi(\alpha)$$
                where \(\psi(\dot)\) is the digamma function

                For Newton's Method, we need the second derivative of the likelihood function (first derivative of score function) calculated as well:

                $$S'(\alpha)=\frac{d^2\ell}{d\alpha^2}=-n\psi^{(1)}(\alpha)$$
                where \(\psi^{(1)}(\alpha)\) is the trigamma function.

                <br>

                <h3>Newton Example</h3>

                Our iterative equation in Newton's Method will be:

                $$\alpha_{n+1}=\alpha_n-\frac{f(\alpha_n)}{f'(\alpha_n)}$$

                In the context of MLE, we set \(f(\alpha)=S(\alpha)\) and \(f'(\alpha)=S'(\alpha)\) to find where \(S(\alpha)=0\).


                <h4>Import Libraries</h4>

                <code style="margin: auto; width: 50%;"><pre>
import numpy as np
from scipy.special import psi, polygamma  # Digamma and trigamma functions
                    
                </pre></code>

                <h4>Defining the Functions</h4>

                <code style="margin: auto; width: 50%;"><pre>
def neg_log_likelihood(alpha, data, beta):
    n = len(data)
    if alpha <= 0:
        return np.inf  # Shape parameter must be positive
    sum_ln_x = np.sum(np.log(data))
    sum_x = np.sum(data)
    ll = (alpha - 1) * sum_ln_x - n * alpha * np.log(beta) - n * np.log(np.math.gamma(alpha)) - sum_x / beta
    return -ll  # Negative log-likelihood

def score_function(alpha, data, beta):
    if alpha <= 0:
        return np.inf
    n = len(data)
    mean_ln_x = np.mean(np.log(data))
    score = n * (mean_ln_x - np.log(beta) - psi(alpha))
    return score

def score_function_derivative(alpha, data, beta):
    if alpha <= 0:
        return np.inf
    n = len(data)
    derivative = -n * polygamma(1, alpha)  # polygamma(1, alpha) is the trigamma function
    return derivative


                </pre></code>

                <h4>Problem Set-Up</h4>
                <code style="margin: auto; width: 50%;"><pre>
# True parameters
alpha_true = 5.0  # Shape parameter
beta_known = 2.0  # Known scale parameter

# Generate sample data
np.random.seed(0)
sample_size = 1000
data = np.random.gamma(shape=alpha_true, scale=beta_known, size=sample_size)
                    
    
    
                </pre></code>
                <h4>Implementation</h4>

<code style="margin: auto; width: 50%;"><pre>
# Initial guess
alpha_initial = 1.0

def newton_raphson(f, f_prime, x0, args=(), tol=1e-6, max_iter=100):
    alpha = x0
    for i in range(max_iter):
        f_val = f(alpha, *args)
        f_prime_val = f_prime(alpha, *args)
        if np.isnan(f_val) or np.isnan(f_prime_val) or f_prime_val == 0:
            raise ValueError("Invalid function value or derivative encountered.")
        alpha_new = alpha - f_val / f_prime_val
        if alpha_new <= 0:
            alpha_new = alpha / 2  # Ensure alpha remains positive
        if abs(alpha_new - alpha) < tol:
            print(f"Converged in {i+1} iterations.")
            return alpha_new
        alpha = alpha_new
    raise ValueError("Maximum iterations reached without convergence.")

# Run Newton-Raphson method
alpha_estimated = newton_raphson(score_function, score_function_derivative,
 x0=alpha_initial, args=(data, beta_known))
print(f"Estimated alpha (Newton-Raphson Method): {alpha_estimated:.4f}")   
</pre></code>

<h3>Secant Method</h3>

$$\alpha_{n+1}=\alpha_{n}-f(\alpha_n)\times\frac{\alpha_n-\alpha_{n-1}}{f(\alpha_n)-f(\alpha_{n-1})}$$

where \(f(\cdot)=S(\alpha)\).

<h4>Defining the Functions</h4>

                <code style="margin: auto; width: 50%;"><pre>
def neg_log_likelihood(alpha, data, beta):
    n = len(data)
    if alpha <= 0:
        return np.inf  # Shape parameter must be positive
    sum_ln_x = np.sum(np.log(data))
    sum_x = np.sum(data)
    ll = (alpha - 1) * sum_ln_x - n * alpha * np.log(beta) - n 
    \* np.log(np.math.gamma(alpha)) - sum_x / beta
    return -ll  # Negative log-likelihood

def score_function(alpha, data, beta):
    if alpha <= 0:
        return np.inf
    n = len(data)
    mean_ln_x = np.mean(np.log(data))
    score = n * (mean_ln_x - np.log(beta) - psi(alpha))
    return score



                </pre></code>

<h4>Initialize</h4>
<code style="margin: auto; width: 50%;"><pre>
# Initial guesses
alpha0 = 1.0
alpha1 = 10.0


</pre></code>

<h4>Implementation</h4>
<code style="margin: auto; width: 50%;"><pre>
def secant_method(f, x0, x1, args=(), tol=1e-6, max_iter=100):
    alpha_prev, alpha = x0, x1
    for i in range(max_iter):
        f_prev = f(alpha_prev, *args)
        f_curr = f(alpha, *args)
        if np.isnan(f_prev) or np.isnan(f_curr) or (f_curr - f_prev) == 0:
            raise ValueError("Invalid function values or zero denominator encountered.")
        alpha_new = alpha - f_curr * (alpha - alpha_prev) / (f_curr - f_prev)
        if alpha_new <= 0:
            alpha_new = (alpha + alpha_prev) / 2  # Ensure alpha remains positive
        if abs(alpha_new - alpha) < tol:
            print(f"Converged in {i+1} iterations.")
            return alpha_new
        alpha_prev, alpha = alpha, alpha_new
    raise ValueError("Maximum iterations reached without convergence.")

alpha_estimated_secant = secant_method(score_function, x0=alpha0, x1=alpha1,
 args=(data, beta_known))
print(f"Estimated alpha (Secant Method): {alpha_estimated_secant:.4f}")
</pre></code>

<h3>Comparison</h3>
<figure>
    <img src="./gamma-alpha-convergence-likelihood.png" alt="Gamma Parameter Fitting" style="margin:auto; display: block;  width:clamp(300px,80vw, 800px)"/>
    <figcaption>Gamma Alpha Fitting Likelihood</figcaption>
</figure>
<figure>
    <img src="./gamma-alpha-convergence-iteration.png" alt="Gamma Parameter Fitting Iterations" style="margin:auto; display: block;  width:clamp(300px,80vw, 800px)"/>
    <figcaption>Gamma Alpha Fitting Iterations</figcaption>
</figure>





            </div>
             




                <div id="Exercises">
                    <h2>Optimization Practice Problems</h2>

                    <ol>
                        <li>
                            Solve for the root of the function \( f(x) = x^2 - 4 \) using the secant method.
                            <ol>
                                <li>Use initial guesses \( x_0 = 1.5 \) and \( x_1 = 2.5 \).</li>
                                <li>Perform 3 iterations and approximate the root to 4 decimal places.</li>
                            </ol>
                        </li>
                        <li>
                            Minimize the function \( f(x) = (x - 3)^2 + 4 \) using the secant method.
                            <ul>
                                <li>Use initial guesses \( x_0 = 1 \) and \( x_1 = 5 \)</li>
                                <ul>Perform 3 iterations to approximate the minimum to 4 decimal places.</ul>
                            </ul>
                        </li>
                        <li>
                            Minimize the function \( f(x) = x^3 - 3x^2 + 2x \).
                            <ul>
                                <li>Apply the secant method starting with \( x_0 = 0 \) and \( x_1 = 2 \).</li>
                                <li>Perform 4 iterations and determine the approximate value of \( x \) at the minimum.</li>
                            </ul>
                        </li>
                        <li>
                            For \( f(x) = \ln(x) + x^2 - 2x \), find the minimum using the secant method.
                            <ul>
                                <li>Start with \( x_0 = 0.5 \) and \( x_1 = 1.5 \).</li>
                                <li>Perform 3 iterations. Does the method converge? Why or why not?</li>
                            </ul>
                        </li>
                        <li>
                            Minimize the function \( f(x) = x^4 - 4x^3 + 6x^2 - 4x \) using Newton's method.
                            <ul>
                                <li>Use \( x_0 = 2.5 \) as the initial guess.</li>
                                <li>Perform 3 iterations and approximate the minimum to 4 decimal places.</li>
                            </ul>
                        </li>
                        <li>
                            The function \( f(x) = \sin(x) + x^2/2 \) has a minimum near \( x = -1 \).
                            <ul>
                                <li>Apply Newton's method with the initial guess \( x_0 = -1 \).</li>
                                <li>Perform 3 iterations to approximate the minimum to 4 decimal places.</li>
                            </ul>
                        </li>
                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>