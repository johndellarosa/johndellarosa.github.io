<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Introduction to Optimization</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Copula, Probability Integral Transform">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to copulas, probability integral transforms, multivariate distributions.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/copula"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Copulas">
                <h1>Introduction to Optimization</h1>
                <h2>Basic Terminology</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./probability-2.html">Probability II</a></li>
                    <li><a href="./sampling-continuous.html">Sampling</a></li>
                    <li><a href="./multivariate-intro">Introduction to Multivariate Distributions</a></li>
               
                </ol>

                In the last section, we covered parameter estimation strategies. In this section, we will continue coming that, but with a focus on the algorithm and optimization.

                <h3>Fundamentals of Optimization</h3>

                Optimization is the process of finding the value of a parameter \(\theta\) that results in the maximum or minimum value of the given function \(f(\theta)\). 
                <ul>
                    <li>Maximization: Finding \(\theta\) such that \(f(\theta)\) is as large as possible. This value of \(f(\theta)\) is the maximum and \(\theta\) is called the maximizer.</li>
                    <li>Minimization: Finding \(\theta\) such that \(f(\theta)\) is as small as possible. This value of \(f(\theta)\) is the minimum and \(\theta\) is called the minimizer.</li>
                </ul>
                In practice, an maximization problem can be turned into a minimization problem through multiplying the objective function by -1, so any technique applicable to one works on the other through slight modification. 
                <h4>Objective Function</h4>
                An objective function is a mathematical expression that we aim to maximize or minimize with respect to the parameter \(\theta\).

                <h5>Likelihood Function</h5>
                This measures the probability of observing the given data \(X\) under the parameter \(\theta\). 

                $$L(\theta)=P(X|\theta)$$

                <h6>Example: Bernoulli</h6>

                Consider a Bernoulli distribution where X can be 1 with probability p or 0 with probability \(1-p\). 

                $$L(p)=\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$$

                <h5>Log-Likelihood Function</h5>

                Often it is more convenient to work in the log-likelihood since the logarithm turns products into sums.

                $$\ell(\theta)=\log L(\theta)$$

                <h6>Bernoulli Log-Likelihood</h6>

                $$\ell(p)=\sum_{i=1}^{n}[x_i\log(p)+(1-x_i)\log(1-p)]$$

                <h5>Loss Function</h5>
                A general term which measures the discrepancy between observed values and those predicted by a model with parameter \(\theta\). One example is the sum of squared residuals (SSR).

                <h2>Analytical Optimization</h2>
                Analytical optimization involves using calculus.
                <h3>First Derivative Test</h3>
                The first derivative test looks at points where the first derivative of the function is zero or undefined.

                <ol>
                    <li>Compute the first derivative \(f'(\theta)\)</li>
                    <li>Solve \(f'(\theta)=0\) to find critical points</li>
                </ol>
                Why does this make sense? If we are trying to minimize (maximize) a function and the derivative is non-zero, then we can further decrease (increase) it by increasing (decreasing) \(\theta\).


                <h3>Second Derivative Test</h3>

                <ul>
                    <li>If \(f"(\theta)\gt 0\), the function is concave upward, indicating a local minimum</li>
                    <li>If \(f"(\theta)\lt 0\), the function is concave downward, indicating a local maximum.</li>
                </ul>

                <ol>
                    <li>Compute the second derivative \(f"(\theta)\)</li>
                    <li>Evaluate \(f"(\theta)\) at each critical point</li>
                </ol>

                <h3>Example: MLE for Mean of a Normal</h3>

                Example: Estimate the mean \(\mu\) of a normal distribution with a known variance \(\sigma^2\), given data \(X=\{x_1,x_2,\dots,x_n\}\).

                $$L(\mu)=\prod_{i=1}^{n}\frac{1}{2\pi\sigma^2}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2})$$

                Taking the log:

                $$\ell(\mu)=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2$$

                Computing the first derivative:

                $$\frac{d\ell}{d\mu}=-\frac{1}{\sigma^2}\sum_{i=1}^{n}(x_i-\mu)(-1)=\frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu)$$
                Set derivative to zero:

                $$\frac{d\ell}{d\mu}=0\Longrightarrow \sum_{i=1}^{n}(x_i-\mu)=0$$

                $$\mu=\frac{1}{n}\sum_{i=1}^{n}x_i$$

                Second derivative test:

                $$\frac{d^2\ell}{d\mu^2}=-\frac{n}{\sigma^2}\lt 0$$

                Since the second derivative is negative, the critical point corresponds to a maximum.

                $$\hat{\mu}=\bar{x}$$

                <h2>Numerical Optimization Methods</h2>
                For more complex problems, it is not feasible or possible to get a closed form answer. Numerical Methods use an iterative approach by having an estimate point, taking derivatives there, and then moving the estimate point accordingly. 

                <h3>Newton-Raphson Method</h3>

                The Newton-Raphson uses the first and second derivative to adjust the estimate.

                <ol>
                    <li>Initial Guess: Choose an initial estimate \(\theta_0\)</li>
                    <li>Iteration: \(\theta_{k+1}=\theta_k-\frac{f'(\theta_k)}{f"(\theta_k)}\)</li>
                    <li>Convergence Check: If \(|\theta_{k+1}-\theta_k|\lt \varepsilon\), stop; else, repeat</li>
                </ol>

                For the Newton-Raphson Method, the function must be twice differentiable, with the second derivative not being zero at the optimum. 

                <h4>Example: Exponential Distribution</h4>
                The Exponential distribution has a simple closed form MLE solution, but for pedagogical reasons, we will apply the Newton-Raphson Method to estimate \(\lambda\).

                <br><br>
                Given data \(X=\{x_1,x_2,\dots,x_n\}\) from an exponential distribution, estimate \(\lambda\):

                $$f(x;\lambda)=\lambda e^{-\lambda x}$$

                $$L(\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i}=\lambda^n e^{-\lambda \sum x_i}$$

                $$\ell(\lambda)=n \log(\lambda)-\lambda\sum_{i=1}^n x_i$$
                Now for Newton-Raphson, we have to calculate the first and second derivative:
                $$\ell'(\lambda)=\frac{n}{\lambda}-\sum x_i$$

                $$\ell"(\lambda)=-\frac{n}{\lambda^2}$$

                Thus, we get the iteration formula:

                $$\lambda_{k+1}=\lambda_k-\frac{\ell'(\lambda_k)}{\ell"(\lambda_k)}=\lambda_k-\left(\frac{\frac{n}{\lambda_k}-\sum x_i}{-\frac{n}{\lambda_{k}^2}}\right)$$

                This simplifies to

                $$\lambda_{k+1}=\lambda_k+\left(\frac{n-\lambda_k\sum x_i}{n}\right)$$
                
                <h3>Secant Method</h3>
            The Secant Method is similar to Newton-Raphson but doesn't require computation of the second derivative. 

            <h4>Algorithm</h4>
            <ol>
                <li>Initial Estimates: Choose two initial guesses \(\theta_0\) and \(\theta_1\)</li>
                <li>Iteration: \(theta_{k+1}=\theta_k-f'(\theta_k)\left(\frac{\theta_k-\theta_{k-1}}{f'(\theta_k)-f'(\theta_{k-1})}\right)\)</li>
                <li>Convergence Check: If \(\theta_{k+1}-\theta_k|\lt \varepsilon\), stop; else, repeat.</li>
            </ol>

            <h3>Bisection Method</h3>
            This technique can be used to find where the derivative is 0 (or more generally a function). 
            

            <h4>Algorithm</h4>

            <ol>
                <li>Initial Interval: Choose \([a,b]\) such that \(f(a)\cdot f(b)\lt 0\)</li>
                <li>Compute Midpoint: \(c=\frac{a+b}{2}\)</li>
                <li>Check Sign:
                    <ul>
                        <li>If \(f(a)\cdot f(c)\lt 0\), set \(b\leftarrow c\)</li>
                        <li>Else, set \(a\leftarrow c\)</li>
                    </ul>

                </li>
                <li>Convergence Check: If \(|b-a|\lt \varepsilon\), stop; else repeat</li>
            </ol>
            
            
            
            </div>



             




                <div id="Exercises">
                    <h2>Copula Practice Problems</h2>

                    <ol>
                        
                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>