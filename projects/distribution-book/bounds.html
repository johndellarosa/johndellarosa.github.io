<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Bounds and Inequalities</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to probability.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/bounds"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Probability-II">
                <h1>Bounds and Inequalities</h1>
                <!-- <h2></h2> -->
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                
                </ol>
                Probability inequalities provide bounds on probabilities and expectations, allowing us to make precise statements about random variables without knowing their exact distributions. Additionally, sometimes a quantity may not have a closed form expression. Inequalities can help provide loose estimates and also bounds for computational techniques. 



                <h2>General Inequalities</h2>
                These inequalities broadly apply to many families of distributions, although there may be conditions for application. 

                <h3>McKay's Inequality</h3>
                For any real-valued random variable X with finite mean \(\mu\), finite variance \(\sigma^2\), and median \(m\):

                $$|\mu-m|\leq \sigma$$

                <h4>Higher Moment Analogue</h4>
                A similar statement can be done with respect to the skewness \(\gamma\) (provided it is finite):

                $$|\mu-m|\leq \frac{\gamma \sigma}{3}$$


                <h3>Markov's Inequality</h3>
                Let \(X\) be a non-negative random variable with finite expected value \(\mathbb{E}[X]\). Then, for any \(a\gt 0\):
                $$P(X\geq a)\leq \frac{\mathbb{E}[X]}{a}$$

                <h4>Extended Markov's Inequality</h4>

                Let \(\phi\) be a non-decreasing non-negative function with \(\phi(a)\gt 0\):

                $$P(X\geq a)\leq \frac{\mathbb{E}[\phi(X)]}{\phi(a)}$$

                

                <h3>Chebyshev's Inequality</h3>
                Chebyshev's inequality provides a bound on the probability that a random variable deviates from its mean by more than a certain number of standard deviations, with mild conditions. 
                <br><br>
                
                Let \(X\) be a random variable with finite mean \(\mu\) and finite variance \(\sigma^2\). Then for \(k\gt 0\):

                $$P(|X-\mu|\geq k\sigma)\leq \frac{1}{k^2}$$

                <h4>Vysochanskij-Petunin</h4>

                If X is unimodal, we can get tigheter bounds for a given distance away:

                $$P(|X-\mu|\geq \lambda\sigma)\leq \frac{4}{9\lambda^2}$$

                for \(\lambda \gt \sqrt{\frac{8}{3}}\)

                <h4>Cantelli's Inequality</h4>
                Cantelli's Inequality is a one-sided analogue:

                $$P(X-\mu\geq k\sigma)\leq \frac{1}{1+k^2}$$


                <h3>Jensen's Inequality</h3>

                Let \(X\) be a random variable, and let \(\phi\) be a convex function. Then:

                $$\phi(\mathbb{E}[X])\leq \mathbb{E}[\phi(X)]$$

                

                <h2>Distribution-Specific</h2>


                <h2>Convergence of Random Variables</h2>

                <h3>Almost Sure Convergence</h3>

                A sequence of random variables \(\left\{X_n\right\}\) converges almost surely (a.s.) to \(X\) if:

                $$P(\lim_{n\rightarrow \infty}X_n=X)=1$$
                Pointwise convergence allows it to not converge where the measure is 0.

                <h4>Strong Law of Large Numbers</h4>

                $$P(\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^nX_i=\mu)=1$$

                <h3>Converges in Probability</h3>

                \(\left\{X_n\right\}\) converges in probability to X if, for every \(\varepsilon\gt0\):
                $$\lim_{n\rightarrow\infty}P(|X_n-X|\geq \varepsilon)=0$$

                Almost sure convergence implies convergence in probability, but the converse is not necessarily true. 

                <h4>Weak Law of Large Numbers</h4>

                $$\lim_{n\rightarrow\infty}P(|\frac{1}{n}\sum_{i=1}^nX_i-\mu|\geq\varepsilon)=0$$



                <h3>Convergence in Distribution</h3>
                \(\left\{X_n\right\}\) converges in distribution to \(X\) if, for all continuity points x of \(F_X(x)\):

                $$\lim_{n\rightarrow\infty}F_{X_n}(x)=F_X(x)$$
                
                <h4>Central Limit Theorem</h4>
                Let \(\{X_i\}\) be iid random variables with finite mean \(\mu\) and finite variance \(\sigma^2\). Then as \(n\rightarrow\infty\):

                $$\frac{\sum_{i=1}^nX_i-n\mu}{\sigma\sqrt{n}}\overset{d}{\rightarrow}N(0,1)$$
                or equivalently
                $$\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\overset{d}{\rightarrow}N(0,1)$$
                <h4>Continuous Mapping Theorem</h4>
                If \(X_n\overset{d}{\rightarrow}X\) and g is a continuous function, then:

                $$g(X_n)\overset{d}{\rightarrow}g(X)$$
                
                </div>



             




                <div id="Exercises">
                    <h2>Distribution Inequality and Bounds Practice Problems</h2>

                    <ol>
                        <li>Let \(X\) be a non-negative random variable with \(\mathbb{E}[X]=5\). Give bounds for \(P(X\geq 25)\).</li>
                        <li>Let \(X\) be a non-negative random variable with \(\mathbb{E}[X]=8\). Use Markov's Inequality to bound \(P(X\geq 16)\)</li>
                        <li>Let \(X\) be a non-negative random variable with \(E[X]=9\). Provide a bound for \(P(\sqrt{X} \geq 4)\)

                        <li>Let \(X\) have mean \(\mu=10\) and variance \(\sigma^2=4\). What is the maximum probability that X deviates from its mean by at least 6?</li>
                        <li>Let \(Y\) be a random variable with mean 20 and variance 25. Find an upper bound for \(P(|Y-20|\geq 10)\)</li>
                        <li>A random variable \(X\) has mean \(\mu=50\) and standard deviation \(\sigma=5\). Give the range of possible values for the median.</li>
                        <li>A random variable \(X\) has mean \(\mu=100\), standard deviation \(\sigma=10\), and skewness \(\gamma=0.6\). 
                            <ol>
                                <li>Use McKay's inequality to estimate the range where the median m lies.</li>
                                <li>Use the skewness inequality to estimate the range where the median m lies.</li>
                            </ol>
                            
                        </li>

                        <li>Given a random variable \(X\) with mean \(\mu=1\) and variance \(\sigma^2=1\), use Cantelli's inequality to bound \(P(X\geq 3)\).</li>


                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>