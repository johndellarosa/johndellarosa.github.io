<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Bounds and Inequalities</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Probability">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to probability.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/bounds"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Probability-II">
                <h1>Bounds and Inequalities</h1>
                <!-- <h2></h2> -->
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                
                </ol>
                Probability inequalities provide bounds on probabilities and expectations, allowing us to make precise statements about random variables without knowing their exact distributions. Additionally, sometimes a quantity may not have a closed form expression. Inequalities can help provide loose estimates and also bounds for computational techniques. 



                <h2>General Inequalities</h2>
                These inequalities broadly apply to many families of distributions, although there may be conditions for application. 

                <h3>McKay's Inequality</h3>
                For any real-valued random variable X with finite mean \(\mu\), finite variance \(\sigma^2\), and median \(m\):

                $$|\mu-m|\leq \sigma$$

                <h4>Higher Moment Analogue</h4>
                A similar statement can be done with respect to the skewness \(\gamma\) (provided it is finite):

                $$|\mu-m|\leq \frac{\gamma \sigma}{3}$$


                <h3>Markov's Inequality</h3>
                Let \(X\) be a non-negative random variable with finite expected value \(\mathbb{E}[X]\). Then, for any \(a\gt 0\):
                $$P(X\geq a)\leq \frac{\mathbb{E}[X]}{a}$$

                <h4>Extended Markov's Inequality</h4>

                Let \(\phi\) be a non-decreasing non-negative function with \(\phi(a)\gt 0\):

                $$P(X\geq a)\leq \frac{\mathbb{E}[\phi(X)]}{\phi(a)}$$

                

                <h3>Chebyshev's Inequality</h3>
                Chebyshev's inequality provides a bound on the probability that a random variable deviates from its mean by more than a certain number of standard deviations, with mild conditions. 
                <br><br>
                
                Let \(X\) be a random variable with finite mean \(\mu\) and finite variance \(\sigma^2\). Then for \(k\gt 0\):

                $$P(|X-\mu|\geq k\sigma)\leq \frac{1}{k^2}$$

                <h4>Vysochanskij-Petunin</h4>

                If X is unimodal, we can get tigheter bounds for a given distance away:

                $$P(|X-\mu|\geq \lambda\sigma)\leq \frac{4}{9\lambda^2}$$

                for \(\lambda \gt \sqrt{\frac{8}{3}}\)

                <h4>Cantelli's Inequality</h4>
                Cantelli's Inequality is a one-sided analogue:

                $$P(X-\mu\geq k\sigma)\leq \frac{1}{1+k^2}$$


                <h3>Jensen's Inequality</h3>

                Let \(X\) be a random variable, and let \(\phi\) be a convex function. Then:

                $$\phi(\mathbb{E}[X])\leq \mathbb{E}[\phi(X)]$$

                

                <h2>Distribution-Specific</h2>


                <h2>Convergence of Random Variables</h2>

                <h3>Sure Convergergence</h3>
                Also known as pointwise convergence.
                Let \(\left\{X_n\right\}\) be a sequence of random variables. 
                Let \(\Omega\) be the sample space of \(X_i\). \(X_n\) converges pointwise (surely) if
                $$\forall \omega\in\Omega: \lim_{n\rightarrow\infty}X_n(\omega)=X(\omega)$$



                <h3>Almost Sure Convergence</h3>

                A sequence of random variables \(\left\{X_n\right\}\) converges almost surely (a.s.) to \(X\) if:

                $$P(\lim_{n\rightarrow \infty}X_n=X)=1$$
                We can denote this as \(X_n\overset{\text{a.s.}}{{\to}X}\).
                How is this different from sure convergence? The notation of "almost sure" refers to allowing \(X_n\) and X to disagree on sets of probability (measure) 0. 
                <h4>Strong Law of Large Numbers</h4>
                An example of almost sure convergence is the Strong Law of Large Numbers.
                $$P(\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^nX_i=\mu)=1$$

                <h3>Converges in Probability</h3>

                \(\left\{X_n\right\}\) converges in probability to X if, for every \(\varepsilon\gt0\):
                $$\lim_{n\rightarrow\infty}P(|X_n-X|\geq \varepsilon)=0$$

                Almost sure convergence implies convergence in probability, but the converse is not necessarily true. It only requires that the probability of large deviations goes to zero.

                <h4>Weak Law of Large Numbers</h4>
                An example of convergence in probability is the weak law of large numbers.
                $$\lim_{n\rightarrow\infty}P(|\frac{1}{n}\sum_{i=1}^nX_i-\mu|\geq\varepsilon)=0$$

                <h4>Example of Convergence in Probability but not Almost Surely</h4>

                <h3>Convergence in Distribution</h3>
                \(\left\{X_n\right\}\) converges in distribution to \(X\) if, for all continuity points x of \(F_X(x)\):

                $$\lim_{n\rightarrow\infty}F_{X_n}(x)=F_X(x)$$
                We can denote this as

                $$F_n(x)\overset{d}{\rightarrow}F(x)$$

                Convergence in distribution does not imply convergence in probability. However, convergence in probability does imply convergence in distribution. 

                <!-- <h4>Convergence in Distribution but not in Probability</h4>
                This can occur when the limit is a non-degenerate random variable. 

                Let \(\{X_n\}\) be a sequence of random variables given by 

                $$X_n\sim U(0,n)$$
                <h5>CDF</h5>
                $$F_{X_n}=\begin{cases}0, & x\lt 0\\
                \frac{x}{n}, & 0\leq x\leq n\\
                1, & x\geq n\end{cases}$$

                For any fixed \(x \geq 0\):
                $$lim_{n\rightarrow\infty}F_{X_n}(x)=\lim_{n\to\infty}\frac{x}{n}=0$$
                Thus, for all finite \(x\geq 0\):

                $$\lim_{n\to\infty}F_{X_n}(x)=0$$
                At \(x=\infty\):
                $$\lim_{n\to\infty}F_{X_n}(\infty)=1$$
                Thus, the limiting distribution \(F_X(x)\) is:
                $$F_X(x)=\begin{cases}0, x\lt \infty\\
                1, & x=\infty\end{cases}$$
                which is the CDF of a degenerate random variable concentrated at infinity.

                <br>
                Since \(F_{X_n}(x)\to F_{X}(x)\) at all continuity point of \(F_X(x)\), we have:
                $$X_n\overset{d}{\to}X$$
                where X is a degenerate random variable taking the value \(\infty\) almost surely

                <h5>Lack of Convergence in Probability</h5>

                Suppose, for contradiction, that \(X_n\overset{P}{\to}X\) for some random variable X:

                Then for any fixed \(x\geq 0\) and \(\varepsilon\gt 0\):

                $$P(|X_n-x|\geq \varepsilon)=P(X_n\leq x-\varepsilon)+P(X_n\geq x+\varepsilon)=1-P(x-\varepsilon \lt X_n \lt x+\varepsilon)$$

                $$P(x-\varepsilon\lt X_n\lt x+\varepsilon)=\frac{2\varepsilon}{n}$$
                $$P(|X_n-x|\geq \varepsilon)=1-\frac{2\varepsilon}{n}$$
                As \(n\to\infty\):
                $$P(|X_n-x|\geq \varepsilon)\to 1$$
                 -->

                <h4>Central Limit Theorem</h4>
                Let \(\{X_i\}\) be iid random variables with finite mean \(\mu\) and finite variance \(\sigma^2\). Then as \(n\rightarrow\infty\):

                $$\frac{\sum_{i=1}^nX_i-n\mu}{\sigma\sqrt{n}}\overset{d}{\rightarrow}N(0,1)$$
                or equivalently
                $$\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\overset{d}{\rightarrow}N(0,1)$$
                <h4>Continuous Mapping Theorem</h4>
                If \(X_n\overset{d}{\rightarrow}X\) and g is a continuous function, then:

                $$g(X_n)\overset{d}{\rightarrow}g(X)$$
                


                <h3>Borel-Cantelli Lemma</h3>
                <h4>First Borel-Cantelli Lemma</h4>
                If \(\sum_{n=1}^\infty P(A_n)\lt \infty\):
                $$P(\limsup_{n\rightarrow\infty}A_n)=0$$
                <h4>Second Borel-Cantelli Lemma</h4>
                If \(\left\{A_n\right\}\) are independent events and \(\sum_{n=1}^\infty P(A_n)=\infty\):

                $$P(\limsup_{n\rightarrow\infty}A_n)=1$$

                <h4>Example of Using Borel-Cantelli</h4>
                Let \(\left\{X_n\right\}\) be a sequence of independent Bernouilli random variables defined on a probability space where \(X_n\) takes values:

                $$X_n=\begin{cases}1 & \text{with probability }p_n=\frac{1}{n}\\
                0 & \text{with probability }1-p_n=1-\frac{1}{n}\end{cases}$$
                <h5>Showing Convergence in Probability</h5>
                $$X_n\to 0$$
                
                For any \(\varepsilon\gt 0\):
                $$P(|X_n-0|\geq \varepsilon)=P(X_n=1)=\frac{1}{n}\to 0\text{ as n }\to\infty$$
                Therefore, \(X_n\) to 0.

                <h5>Showing No Almost Sure Convergence</h5>
                Let \(A_n=\left\{X_n=1\right\}\). These are the events where \(X_n\) deviates from the limiting distribution of 0. 

                
                $$\sum_{n=1}^\infty P(A_n)=\sum_{n=1}^\infty\frac{1}{n}=\infty$$
                From the second Borel-Cantelli Lemma, if \(\{A_n\}\) are independent events and \(\sum P(A_n)=\infty\) then:

                $$P(\limsup_{n\to\infty}A_n)=1$$
                Thus, \(X_n\) does not converge almost surely to 0.
                </div>



             




                <div id="Exercises">
                    <h2>Distribution Inequality and Bounds Practice Problems</h2>

                    <ol>
                        <li>Let \(X\) be a non-negative random variable with \(\mathbb{E}[X]=5\). Give bounds for \(P(X\geq 25)\).</li>
                        <li>Let \(X\) be a non-negative random variable with \(\mathbb{E}[X]=8\). Use Markov's Inequality to bound \(P(X\geq 16)\)</li>
                        <li>Let \(X\) be a non-negative random variable with \(E[X]=9\). Provide a bound for \(P(\sqrt{X} \geq 4)\)

                        <li>Let \(X\) have mean \(\mu=10\) and variance \(\sigma^2=4\). What is the maximum probability that X deviates from its mean by at least 6?</li>
                        <li>Let \(Y\) be a random variable with mean 20 and variance 25. Find an upper bound for \(P(|Y-20|\geq 10)\)</li>
                        <li>A random variable \(X\) has mean \(\mu=50\) and standard deviation \(\sigma=5\). Give the range of possible values for the median.</li>
                        <li>A random variable \(X\) has mean \(\mu=100\), standard deviation \(\sigma=10\), and skewness \(\gamma=0.6\). 
                            <ol>
                                <li>Use McKay's inequality to estimate the range where the median m lies.</li>
                                <li>Use the skewness inequality to estimate the range where the median m lies.</li>
                            </ol>
                            
                        </li>

                        <li>Given a random variable \(X\) with mean \(\mu=1\) and variance \(\sigma^2=1\), use Cantelli's inequality to bound \(P(X\geq 3)\).</li>


                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>