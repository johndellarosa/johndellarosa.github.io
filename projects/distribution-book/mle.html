<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->

            <title>Maximum Likelihood Estimation</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Maximum Likelihood Method, MLE">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to the Maximum Likelihood Method for parameter estimation.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/mle"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <script src="./mle-funcs.js"></script>


<style>
    .data-point {
    cursor: pointer;
    color: blue;
    margin-right: 5px;
}

.data-point:hover {
    text-decoration: underline;
}

#bounds-button, #clearDataButton, #addDataButton {
    cursor: pointer; /* Changes the cursor when hovering over the button */
        background-color: #007bff; /* Sets a background color for the button */
        color: white; /* Sets the text color for the button */
        border: none; /* Removes the border from the button */
        margin: 10px; /* Adds space around the input and button */
        padding: 10px; /* Adds space inside the input and button */
        border: 1px solid #ccc; /* Adds a border to the input and button */
        border-radius: 5px; /* Rounds the corners of the input and button */
}

.data-point-button {
    cursor: pointer;
    margin: 5px;
    background-color: #4CAF50; /* Green background */
    border: none;
    color: white;
    padding: 5px 10px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    border-radius: 5px; /* Rounded corners for better aesthetics */
    box-shadow: 0 4px #999;
}

.data-point-button:hover {
    background-color: #45a049; /* Darker shade of green */
}

.data-point-button:active {
    background-color: #3e8e41;
    box-shadow: 0 2px #666;
    transform: translateY(2px); /* Slight push effect on click */
}
#likelihoodChart {
    max-width: 900px;
              /* width:100%; */
            width: 80vw;
            max-height: 50vh;
              /* height: 20%; */
              /* height: 600px; */
              /* max-height: 1000px; */
                display: block; /* Block display to fill the width */
                margin: 0 auto; /* Auto margins to horizontally center */
            }

            #intervalsSlider {
    width: 200px; /* Adjust width as needed */
    margin: 5px;
}

#intervalsDisplay {
    font-size: 16px;
    margin-left: 10px;
}

.container {
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 100%; /* Adjust based on preference */
    max-width: 900px; /* Adjust based on preference */
    
                margin: 0 auto; /* Auto margins to horizontally center */
}

#dataDisplay, #chartContainer, #controls {
    width: 100%;
    margin: 10px 0; /* Adds vertical spacing between sections */
    text-align: center; /* Centers the text inside the divs */
}

#modelStats {
    margin-top: 20px;
    font-size: 16px;
    color: #333; /* Adjust color based on your theme */
}

/* Smaller screens (e.g., smartphones) */
@media (max-width: 600px) {
    .container {
        width: 100%;
        padding: 10px;
    }
    .data-point-button {
        padding: 8px; /* Larger touch targets */
    }
    input[type="number"], input[type="range"] {
        width: 100%; /* Full width inputs for easier access */
    }
}


.table-container {
    display: flex;
    justify-content: center;
    margin: 0;
  }

  .comparison-table {
    width: 80%;
    max-width: 900px;
    border-collapse: collapse;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    background-color: #fff;
    border-radius: 8px;
    overflow: hidden;
  }

  .comparison-table thead tr {
    background-color: #72A0C1;
    color: white;
    text-align: left;
    font-size: 18px;
    /* font-weight: bold; */
  }

  .comparison-table th,
  .comparison-table td {
    padding: 15px 20px;
    text-align: center;
    border-bottom: 1px solid #ddd;
  }

  .comparison-table tbody tr:nth-child(even) {
    background-color: #f9f9f9;
  }

  .comparison-table tbody tr:hover {
    background-color: #f1f1f1;
  }

  .comparison-table td {
    font-family: 'Arial', sans-serif;
    font-size: 16px;
    line-height: 1.6;
  }

  /* Add smooth transition effect */
  .comparison-table tbody tr:hover td {
    transition: background-color 0.3s ease;
  }

  /* Add some padding to the top and bottom */
  h2 {
    font-family: 'Arial', sans-serif;
    font-size: 24px;
    margin-bottom: 20px;
  }

  .table-container {
    padding: 20px;
    display: flex;
    justify-content: center;
  }
</style>

</style>


    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="MLE-explanation">
                <h1>Maximum Likelihood Estimation (MLE)</h1>
                <h2>Introduction to Maximum Likelihood Estimation</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/statistics">Statistics</a></li>
                    <li><a href="probability-2.html">Probability II</a></li>
                    <li><a href="./estimators.html">Estimators</a></li>
                </ol>
                
                <h3>What is MLE?</h3>
                Maximum Likelihood Estimation (MLE) is a widely-used method for estimating the parameters of a statistical model. It is based on the principle of finding parameter values that maximize the likelihood function, which measures how likely it is that a given set of parameters would produce the observed data. You can think of likelihood function as the product of the PDF/PMF for each data point for a given set of parameters. While typically we think of data values as being the axis, but now it is the parameters. 

                <h3>Likelihood Function</h3>
                Given a statistical model with a probability density function (pdf) or probability mass function (pmf) \(f(x|\theta)\), where \(x\) represents the observed data and \(\theta\) denotes the parameters of the model. 
                The likelihood function is defined as 
                $$L(\theta|x)=f(x|\theta)$$
                For a sample \(x_1,x_2,\dots,x_n\)drawn from this distribution, the likelihood function for the entire sample is:
                $$L(\theta|x_1,x_2,\dots,x_n)=\prod_{i=1}^{n}f(x_i|\theta)$$
                The likelihood function measures how probable the observed data is for different values of \(\theta\). This is in contrast to the PDF/PMF which talks about probability of a given data point over its support. With MLE, your axis is the possible values of the <b>parameter(s)</b> for a given set of observations.

                

                <h3>Log-Likelihood Function</h3>
                The likelihood function can be unwiedly due to its multiplicative nature, especially when dealing with large samples. To simplify calculations, we often use the log-likelihood function \(l(\theta|x)\), which is the natural logarithm of the likelihood function:
                $$\ell(\theta \mid x) = \log L(\theta \mid x) = \log \left( \prod_{i=1}^{n} f(x_i \mid \theta) \right) = \sum_{i=1}^{n} \log f(x_i \mid \theta)$$
                Maximizing the log-likelihood function is equivalent to maximizing the likelihood function as the log is a strictly increasing function, but it simplifies the computations and is less prone to numerical issues.
                
                
                <h3>Score</h3>
                Let \(L(\theta|X)\) represent the likelihood function of the parameter vector \theta given the data \(X=(X_1,X_2,\dots,X_n)\). 
                Then the score function is given by the gradient of the log-likelihood:
                $$S(\theta,X)=\nabla \log L(\theta|X)$$

                <h4>Interpreting the Score</h4>
                <ul>
                    <li>\(S(\theta|X)=0\): If the score function is zero, this means the log-likelihood has reached a critical point, which could be a maximum, minimum, or saddle point. </li>
                    <li>\(S(\theta|X)\gt 0\):If the score is positive, increasing \(\theta\) raises the likelihood, suggesting that the parameter is underestimated</li>
                    <li>\(S(\theta|X)\lt 0\):If the score is negative, decreasing \(\theta\) raises the likelihood, suggesting that the parameter is overestimated</li>
                    
                </ul>
                <h4>Expected Value</h4>
                $$\mathbb{E}[S(\theta|X)]=0$$

                <h4>Fisher Information</h4>
                

                $$\mathcal{I}(\theta)=\mathbb{E}[(\frac{\partial}{\partial \theta}\log L(\theta|X))^2]$$
                
                <h3>Akaike Information Criterion</h3>
                $$\text{AIC}=-2\log(L)+2k$$

                where L is the likelihood of the model given the data and k is the number of estimated parameters in the model. The second term penalizes having a more complex model unless the benefit that the extra parameter brings outweighs that cost. 

                <h3>Bayesian Information Criterion</h3>

                $$\text{BIC}=-2\log(L)+k\log(n)$$
                where L is the likelihood of the model given the data, k is the number of estimated parameters in the model, and n is the number of data points. 
                This has a strong penalty as the value of the logarithm will be greater than 1. This makes it favor simpler models much more than AIC based selection. 

                <h4>Example</h4>


                <p>Letâ€™s consider a simple case of fitting a regression model with different numbers of predictor variables. You fit two models:</p>

                <ul>
                <li>Model 1: Includes 3 predictors, has \(L_1 = 0.01\), and \(n = 100\).</li>
                <li>Model 2: Includes 5 predictors, has \(L_2 = 0.02\), and \(n = 100\).</li>
                </ul>

                <h5>AIC Calculation</h5>
                <p>For Model 1:</p>
                \[
                \text{AIC}_1 = -2 \log(0.01) + 2(3) = 9.21 + 6 = 15.21
                \]
                <p>For Model 2:</p>
                \[
                \text{AIC}_2 = -2 \log(0.02) + 2(5) = 7.60 + 10 = 17.60
                \]
                <p>Since Model 1 has the lower AIC, it is the preferred model.</p>

                <h5>BIC Calculation</h5>
                <p>For Model 1:</p>
                \[
                \text{BIC}_1 = -2 \log(0.01) + 3 \log(100) = 9.21 + 13.82 = 23.03
                \]
                <p>For Model 2:</p>
                \[
                \text{BIC}_2 = -2 \log(0.02) + 5 \log(100) = 7.60 + 23.03 = 30.63
                \]
                <p>Since Model 1 has the lower BIC, it is the preferred model.</p>


                <h4>Comparison of AIC and BIC</h4>
                <div class="table-container">
                    <table class="comparison-table">
                      <thead>
                        <tr>
                          <th>Feature</th>
                          <th>AIC</th>
                          <th>BIC</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td>Penalty term</td>
                          <td>\( 2k \)</td>
                          <td>\( k \log(n) \)</td>
                        </tr>
                        <tr>
                          <td>Penalty strength</td>
                          <td>Lower penalty for more parameters</td>
                          <td>Stronger penalty, especially as \(n\) increases</td>
                        </tr>
                        <tr>
                          <td>Model selection</td>
                          <td>Tends to favor more complex models</td>
                          <td>Tends to favor simpler models</td>
                        </tr>
                        <tr>
                          <td>Asymptotic behavior</td>
                          <td>Minimizes prediction error</td>
                          <td>Consistent in selecting the true model</td>
                        </tr>
                        <tr>
                          <td>Philosophy</td>
                          <td>Predictive accuracy (minimize information loss)</td>
                          <td>Bayesian model comparison (posterior likelihood)</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>


                <h2>Maximum Likelihood Estimation</h2>
                To find the Maximum Likelihood Estimates (MLEs) of the parameters \(\theta\) we solve the optimization problem:
                $$\hat{\theta} = \arg\max_{\theta} \ell(\theta \mid x)$$
                This involves taking the derivative of the log-likelihood function with respect to the parameters and setting it to zero:
                $$\frac{\partial \ell(\theta \mid x)}{\partial \theta} = 0$$
            

                <h3>Estimating Parameters</h3>


                <h4>Example</h4>

                <h5>Problem</h5>
                  We have a dataset with:

                  $$x_1=1,x_2=2,x_3=1.5,x_4=3,x_5=2.5$$

                  Let's assume that we want to fit this data to an exponential distribution due to some prior knowledge. 
                  <h5>Calculate Likelihood function</h5>
                  $$f(x|\lambda)=\lambda e^{-\lambda x}$$

                  The likelihood function is the product of the PDF evaluated at every datapoint for a given \(\lambda\)

                  $$L(\lambda)=\prod_{i=1}^{n}f(x_i|\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i}$$

                  By the properties of exponentials, we can rewrite this as the following:

                  $$L(\lambda)=\lambda^n e^{-\lambda \sum_{i=1}^n x_i}$$

                  Plugging in our datapoints:

                  $$L(\lambda)=\lambda^5 e^{-\lambda(1+2+1.5+3+2.5)}=\lambda^5e^{-10\lambda}$$

                  <h5>Take Log-Likelihood</h5>

                  $$\log L(\lambda)=5\log(\lambda)-10\lambda$$

                  <h5>Differentiate and Set to 0</h5>
                  $$\frac{d}{d\lambda} \log L(\lambda)=\frac{5}{\lambda}-10=0$$

                  $$\lambda=0.5$$

                <h4>Example 2</h4>

                <p>You are given the following data drawn from a shifted Exponential distribution:</p>
                <ul>
                <li>\( x_1 = 2 \), \( x_2 = 3 \), \( x_3 = 2.5 \), \( x_4 = 4 \), \( x_5 = 3.5 \)</li>
                </ul>
                <p>Assume that the data comes from a two-parameter Exponential distribution with unknown rate \( \lambda \) and shift \( \theta \). Use Maximum Likelihood Estimation (MLE) to estimate both \( \lambda \) and \( \theta \).</p>

               

                <h5>Step 1: Write Down the Likelihood Function</h5>
                <p>The PDF of the two-parameter Exponential distribution is:</p>
                \[
                f(x \mid \lambda, \theta) = \lambda e^{-\lambda (x - \theta)}, \quad x \geq \theta
                \]
                <p>The likelihood function for \( n \) independent observations \( x_1, x_2, \dots, x_n \) is the product of the individual densities:</p>
                \[
                L(\lambda, \theta) = \prod_{i=1}^{n} \lambda e^{-\lambda (x_i - \theta)} = \lambda^n e^{-\lambda \sum_{i=1}^{n} (x_i - \theta)}
                \]

                

                <h5>Step 2: Take the Log-Likelihood</h5>
                <p>To simplify the maximization, take the natural logarithm of the likelihood function. The log-likelihood is:</p>
                \[
                \log L(\lambda, \theta) = n \log(\lambda) - \lambda \sum_{i=1}^{n} (x_i - \theta)
                \]
                <p>where \( n = 5 \) in this case.</p>

                

                <h5>Step 3: Differentiate with Respect to \( \lambda \) and Set to Zero</h5>
                <p>First, differentiate the log-likelihood with respect to \( \lambda \) and set it equal to zero:</p>
                \[
                \frac{\partial}{\partial \lambda} \log L(\lambda, \theta) = \frac{n}{\lambda} - \sum_{i=1}^{n} (x_i - \theta) = 0
                \]
                <p>Solving for \( \lambda \) gives:</p>
                \[
                \lambda = \frac{n}{\sum_{i=1}^{n} (x_i - \theta)}
                \]
                <p>This is the MLE for \( \lambda \), given the shift parameter \( \theta \).</p>

                <h5>Step 4: Differentiate with Respect to \( \theta \) and Set to Zero</h5>
                <p>Next, differentiate the log-likelihood with respect to \( \theta \) and set it equal to zero:</p>
                \[
                \frac{\partial}{\partial \theta} \log L(\lambda, \theta) = \lambda n - \lambda \sum_{i=1}^{n} \frac{1}{x_i - \theta} = 0
                \]
                <p>However, solving for \( \theta \) here simplifies in many cases by assuming that \( \theta = \min(x_1, x_2, \dots, x_n) \), which is often the MLE for the shift parameter.</p>
                <p>Thus, \( \hat{\theta} = \min(x_1, x_2, \dots, x_n) = 2 \).</p>



                <h5>Step 5: Final Estimates</h5>
                <p>With \( \hat{\theta} = 2 \), we can substitute this back into the equation for \( \lambda \) to find its MLE:</p>
                \[
                \hat{\lambda} = \frac{5}{(2 + 3 + 2.5 + 4 + 3.5) - 5 \times 2} = \frac{5}{15 - 10} = \frac{5}{5} = 1
                \]
                <p>Therefore, the Maximum Likelihood Estimates for the parameters are:</p>
                <ul>
                <li>\( \hat{\lambda} = 1 \)</li>
                <li>\( \hat{\theta} = 2 \)</li>
                </ul>


                <h3>Properties of MLE</h3>
                <ol>
                    <li>Consistency: As the sample size n approaches infinity, the MLE converges to the true parameter values with probability 1.</li>
                </ol>





            </div>


            <h2>Likelihood Function Interactive Chart</h2>


            <div class="container">
            <label for="distributionSelect">Distribution:</label>
            <select id="distributionSelect" onchange="updateDistributionType();">
                <option value="poisson">Poisson</option>
                <option value="exponential">Exponential</option>

            </select>
            <div id="chartContainer">
                <canvas id="likelihoodChart"></canvas>
            </div>
            <div id="controls">
            <div>
                <label for="intervalsSlider">Number of Intervals:</label>
                <input type="range" id="intervalsSlider" min="10" max="500" value="100" step="10" oninput="updateIntervalDisplay(this.value); updateChart();">
                <span id="intervalsDisplay">100</span>
            </div>

            <input type="number" id="dataPoint" placeholder="Enter a data point" onchange="updateChart()" inputmode="decimal">
            <button id="addDataButton" onclick="addDataPoint()">Add Data Point</button>
            <div id="dataDisplay">
                <h3>Data Points:</h3>
                <div id="dataPointsList">None</div>
            </div>
            <button id="clearDataButton" onclick="clearDataPoints()">Clear Data</button>
            
        
            <div>
                <label for="xMin">X-Axis Min:</label>
                <input type="number" id="xMin" value="0" step="0.1" inputmode="decimal">
                <label for="xMax">X-Axis Max:</label>
                <input type="number" id="xMax" value="10" step="0.1" inputmode="decimal">
                <button id="bounds-button" onclick="updateChart()">Set Bounds</button>
            </div>
            <div id="modelStats">
                <p id="aicValue">AIC: N/A</p>
                <p id="bicValue">BIC: N/A</p>
            </div>
        </div>
            </div>
             




                <div id="Exercises">
                    <h2>Maximum Likelihood Estimate Practice Problems</h2>
                    <ol>
                        <li>

                            <p>You fit two models to a dataset with 150 data points:</p>
<ul>
  <li>Model A: Includes 4 parameters and has a likelihood \(L_A = 0.03\).</li>
  <li>Model B: Includes 6 parameters and has a likelihood \(L_B = 0.04\).</li>
</ul>

<p>Calculate the AIC and BIC for both models. Which model is preferred by AIC? Which model is preferred by BIC?</p>

                        </li>

                        <li>

                            <p>Assume you have the following data drawn from a uniform distribution \( U(0, \theta) \), where \( \theta \) is unknown. The observed data is:</p>
                            <ul>
                            <li>Data: \( x_1 = 2, \, x_2 = 3, \, x_3 = 1.5, \, x_4 = 4, \, x_5 = 2.5 \)</li>
                            </ul>
                            <p>1. Write down the likelihood function for the unknown parameter \( \theta \).</p>
                            <p>2. Take the log-likelihood and find the value of \( \theta \) that maximizes it.</p>
                            <p>3. Solve for \( \theta \) to find the MLE estimate.</p>
                        </li>


                        <li>
                            Given datapoints \((0.5,1.5,2,2.5,1)\) that come from an exponential distribution:
                            <ol type="a">
                                <li>Write down the likelihood function</li>
                                <li>Solve for the MLE for \(\lambda\)</li>
                            </ol>
                        </li>



                        <li><p>Suppose you have the following data points drawn from a normal distribution with unknown mean \( \mu \) and known variance \( \sigma^2 = 1 \):</p>
                            <ul>
                              <li>Data: \( x_1 = 2, \, x_2 = 3, \, x_3 = 2.5, \, x_4 = 4, \, x_5 = 3.5 \)</li>
                            </ul>
                            <p>1. Write down the likelihood function for the unknown parameter \( \mu \).</p>
                            <p>2. Take the log-likelihood and differentiate it with respect to \( \mu \).</p>
                            <p>3. Solve for \( \mu \) to find the MLE estimate.</p>
                            
                            
                            </li>

                        <li>
                            <p>Assume you have the following data from a series of Bernoulli trials, where \( x_i \in \{0,1\} \) represents success (1) or failure (0) in a trial. The probability of success is \( p \), and you want to estimate \( p \) using MLE:</p>
<ul>
  <li>Data: \( x_1 = 1, \, x_2 = 0, \, x_3 = 1, \, x_4 = 1, \, x_5 = 0 \)</li>
</ul>
<ol type="a">
<li>Write down the likelihood function for the unknown parameter \( p \).</li>
<li>Take the log-likelihood and differentiate it with respect to \( p \).</li>
<li>Solve for \( p \) to find the MLE estimate.</li>
</ol>
                        </li>
                        <li>
                            <p>Consider the following data drawn from a geometric distribution where the random variable represents the number of trials until the first success. The probability of success is \( p \), and you want to estimate \( p \) using MLE:</p>
                            <ul>
                                <li>Data: \( x_1 = 3, \, x_2 = 1, \, x_3 = 2, \, x_4 = 4, \, x_5 = 2 \)</li> </ul>
                                <ol type="a">
                                <li>Write down the likelihood function for the unknown parameter \( p \).</li>
                                <li>Take the log-likelihood and differentiate it with respect to \( p \).</li>
                                <li>Solve for \( p \) to find the MLE estimate.</li>
                                </ol>
                        </li>
                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 

            document.getElementById('clearDataButton').addEventListener('click', clearDataPoints);
            updateDistributionType();
            </script>
</body>
</html>