<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
<script src="mixture-funcs.js"></script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Mixture Distributions</title>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <!-- Meta tags -->
        
        <meta name="keywords" content="Mixture distribution">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to mixture distributions.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/mixture-distributions"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->
        <style>
            body {
              font-family: Arial, sans-serif;
              /* background-color: #f5f5f5; */
            }
            
            .container {
              max-width: 1800px;
              width:100%;
              margin: 50px auto;
              padding: 20px;
              background-color: white;
              border-radius: 8px;
              box-shadow: 0 0 15px rgba(0, 0, 0, 0.1);
              display: flex;
              flex-direction: column;
              align-items: center;
            }
            
            .input-section, .output-section {
              width: 100%;
              display: flex;
              flex-direction: column;
              align-items: center;
            }
            
            .input-group, #dataPointsList {
              width: 100%;
              margin-bottom: 20px;
              max-width: 500px;
              margin: 0% auto;
              align-items: center;
            }
            
            #dataPointsList button {
                padding:15px;
                margin: 5px;
                background-color: #6CB4EE;
                border-radius: 10px;
            }
            
            #dataPointsList button:hover{
            
              background-color: #6699CC
            
            }

            code {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            padding: 10px;
            display: block;
            white-space: pre-wrap; /* Ensure code wraps correctly */
            font-family: monospace;
            line-height: 1.5;
        }

            #mixtureChart {
              max-width: 900px;
              /* width:100%; */
            width: 80vw;
            max-height: 50vh;
              /* height: 20%; */
              /* height: 600px; */
              /* max-height: 1000px; */
            }
            
            
            .input-group label {
              display: block;
              font-weight: bold;
              margin-bottom: 5px;
            }
            
            .input-group input, .input-group select {
              width: 100%;
              padding: 8px;
              font-size: 14px;
              margin-bottom: 5px;
              border: 1px solid #ccc;
              border-radius: 4px;
            
            }
            
            button {
              padding: 10px 20px;
              font-size: 16px;
              background-color: #28a745;
              color: white;
              border: none;
              border-radius: 4px;
              cursor: pointer;
              transition: background-color 0.3s ease;
            
            }
            
            button:hover {
              background-color: #218838;
            }
            
            input, select {
                margin-bottom:10px;
            }
            
            #chart {
              max-width: 900px;
              /* width:100%; */
            width: 80vw;
            max-height: 50vh;
              /* height: 20%; */
              /* height: 600px; */
              /* max-height: 1000px; */
            }
            
            .output-section {
              display: none;
              width: 100%;
              text-align: center;
            }
            
            h2 {
              margin-bottom: 20px;
            }
            
            h1 {
              text-align: center;
            }
            
            /* Smaller screens (e.g., smartphones) */
            @media (max-width: 600px) {
                .container {
                    width: 100%;
                    padding: 10px;
                }
                .data-point-button {
                    padding: 8px; /* Larger touch targets */
                }
                input[type="number"], input[type="range"] {
                    width: 95%; /* Full width inputs for easier access */
                }
            }

            input[type="color"] {
    -webkit-appearance: none;
    -moz-appearance: none;
    appearance: none;
    width: 50px;
    height: 50px;
    border: none;
    cursor: pointer;
}

input[type="color"]::-webkit-color-swatch-wrapper {
    padding: 0;
}

input[type="color"]::-webkit-color-swatch {
    border: none;
}

input[type="color"] {
    border-radius: 50%;  /* Make the color picker round */
    box-shadow: 0px 0px 5px rgba(0, 0, 0, 0.2);  /* Add a shadow for better visibility */
    border: 2px solid #ccc;  /* Add a border to make it more visible */
}

            
            </style>
    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Mixture-Distribution-Explanation">
                <h1>Mixture Distributions</h1>
                <h2>Introduction to Mixture Distributions</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="probability-2.html">Probability 2</a></li>
                    <li><a href="./mle.html">Maximum Likelihood Estimation</a></li>


                </ol>
                <h3>Explanation</h3>
                In probability and statistics, a mixture distribution arises when the population being studied is composed of several distinct subpopulations, each of which follows a different probability distribution. Instead of modeling the population as a whole with a single distribution, we assume that the data is generated from one of several component distributions, each with its own characteristics.

                Formally, a mixture distribution represents a probability distribution made up of a weighted combination of other distributions. This should not be confused with the concept of a sum of the realizations of random variables, which is different, but easily mistaken for it; that would be given by convolution. 

                <h3>Definition</h3>
                A mixture distribution is defined as a weighted sum of two or more component distributions. Suppose there are k component distributions 
                \(f_1(x),f_2(x),\dots,f_k(x)\), with weights \(w_1,w_2,\dots,w_k\), where \(w_i\geq 0\) and \(\sum_{i=1}^{k}w_if_i(x)=1\). 
                Then the PDF or PMF of the mixture distribution is given by:

                $$f(x)=\sum_{i=1}^{k}w_if_i(x)$$

                One possible interpretation is that \(w_i\) is the probability of sampling from \(f_i(x)\). 

                <h2>Expectation-Maximization Algorithm</h2>

                <strong>Note:</strong> The reader may want to come back to this section after reading the <a href="./marginal-likelihood.html">Marginal Likelihood</a> section. There is somewhat of a circular dependency where that chapter builds off of things that builds off of this chapter. The contents of this section remain here since they are strongly tied to mixture distributions. 
                <br><br>
                In the <a href="./mle.html">MLE chapter</a>, we went over the eponymous parameter estimation method. We will now build on that in the context of Mixture Distributions with something called the Expectation-Maximization Algorithm.

                The Expectation-Maximization (EM) algorithm is a powerful iterative method for estimating parameters in statistical models, particularly in cases where the data is incomplete or has a latent variable structure. It is widely used in a variety of applications such as clustering (e.g., Gaussian Mixture Models), missing data problems, and latent variable models. The EM algorithm alternates between two steps: the Expectation step (E-step) and the Maximization step (M-step), with the goal of finding the parameters that maximize the likelihood function. 

              
                <h4>Setup</h4>
    
                Consider a statistical model with observed data \(X=\left\{x_1,x_2,\dots,x_n\right\}\), and unobserved (latent) variables \(Z=\left\{z_1,z_2,\dots,z_n\right\}\). The model is governed by a set of parameters \(\theta\), and we wish to estimate these parameters given only the observed data \(X\). 
    
                The complete data likelihood is given by:
    
                $$L(\theta;X,Z)=P(X,Z|\theta)$$
    
                However, since we do not observe Z, we must work with the marginal likelihood of the observed data:
    
                $$L(\theta;X)=\sum_{Z}P(X,Z|\theta)$$
    
                <h4>Algorithm</h4>
    
                The EM algorithm seeks to maximize the marginal likelihood by iteratively applying the following two steps:
    
                <h5>E-step (Expectation Step)</h5>
                In this step, we compute the expected value of the complete-data log likelihood, \(Q(\theta;\theta^{(t)})\), given the observed data and the current estimate of the parameters \(\theta^{(t)}\). This expectation is taken with respect to the conditional distribution of the latent variables Z given the observed data X and the parameter estimates. 
    
                $$Q(\theta;\theta^{(t)})=\mathbb{E}_{Z|X,\theta^{(t)}}[\log L(\theta;X,Z)]$$
    
                <h5>M-Step (Maximization Step)</h5>
    
                In this step, we maximize the expected complete-data log-likelihood \(Q(\theta;\theta^{(t)})\) with respect to the parameters \(\theta\), to obtain an updated estimate of the parameters \(\theta^{(t+1)}\). 
    
                $$\theta^{(t+1)}=\arg \max_{\theta}Q(\theta;\theta^{(t)})$$
    
    
                The algorithm repeats these two steps until convergence, typically when the change in log-likelihood or parameter values between iterations falls below a predetermined threshold.
    
                <h3>Basic Example</h3>
    
                Before we dive into the general Gaussian Mixture Model (GMM) case, let's consider a simplified version: a mixture of two 1-dimensional normal distributions. This will help clarify the steps of the Expectation-Maximization (EM) algorithm in a less complex context. 
    
                <h4>Setup</h4>
                Suppose we observed data \(X=\left\{x_1,x_2,\dots,x_n\right\}\), and we believe the data comes from a mixture of two normal distributiosn with unknown parameters. Specifically we assume each \(x_i\) is generated by one of the following two distributions:

                <ol>
                  <li>\(N(\mu_1,\sigma_1^2)\) with weight \(\pi_1\)</li>
                  <li>\(N(\mu_2,\sigma_2^2)\) with weight \(1-\pi_1\)</li>
                  
                </ol>
                Our goal is to estimate the parameters \(\mu_1,\sigma_1^2,\mu_2,\sigma_2^2,\pi_1\) from the data.

                We will initialize the parameters \(\mu_1,\sigma_1^2,\mu_2,\sigma_2^2,\pi_1\) randomly or using some heuristic. For simplicity, we can start with:

                <ul>
                  <li>Initial means: \(\mu_1^{(0)}=\min(X); \mu_2^{(0)}=\max(X)\)</li>
                  <li>Initial variances: \(\sigma_1^2\) and \(\sigma_2^2\) can be initialized to the variance of \(X\)</li>
                  <li>Mixing coefficient: \(\pi_1=0.5\) (assuming both components contribute equally at the start)</li>

                </ul>

                <h4>E-Step</h4>
                In the E-step, we compute the responsibilities, which tell us how much each data point \(x_i\) "belongs" to each component (i.e., how likely it is that each point was generated by component 1 or component 2).

                The responsibility for the first component is calculated as:


                $$\gamma_{i1}^{(t)} = \frac{\pi_1^{(t)} \mathcal{N}(x_i \mid \mu_1^{(t)}, \sigma_1^{(t)2})}{\pi_1^{(t)} \mathcal{N}(x_i \mid \mu_1^{(t)}, \sigma_1^{(t)2}) + (1 - \pi_1^{(t)}) \mathcal{N}(x_i \mid \mu_2^{(t)}, \sigma_2^{(t)2})}$$

                Since the responsibilities add up to 1, we can simply calculate the responsibility for the second component as:
                $$\gamma_{i2}^{(t)} = 1 - \gamma_{i1}^{(t)}$$

                <h4>M-Step</h4>

                In the M-step, we update the parameters based on the responsibilities computed in the E-step:

                We perform the following calculations
                <h5>Update the mean</h5>
                $$\mu_1^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{i1}^{(t)} x_i}{\sum_{i=1}^n \gamma_{i1}^{(t)}}$$
                $$\mu_2^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{i2}^{(t)} x_i}{\sum_{i=1}^n \gamma_{i2}^{(t)}}$$

                <h5>Update the variances</h5>
                $$\sigma_1^{(t+1)2} = \frac{\sum_{i=1}^n \gamma_{i1}^{(t)} (x_i - \mu_1^{(t+1)})^2}{\sum_{i=1}^n \gamma_{i1}^{(t)}}$$

                $$\sigma_2^{(t+1)2} = \frac{\sum_{i=1}^n \gamma_{i2}^{(t)} (x_i - \mu_2^{(t+1)})^2}{\sum_{i=1}^n \gamma_{i2}^{(t)}}$$

                <h5>Update the mixing coefficient</h5>

                $$\pi_1^{(t+1)} = \frac{1}{n} \sum_{i=1}^n \gamma_{i1}^{(t)}$$
                $$\pi_2^{(t+1)} = 1 - \pi_1^{(t+1)}$$


                <h4>With Numbers</h4>

                Let \(X=\left\{-5,-4,-3,0,1,2,3,8,9,10\right\}\).

                Using our heuristic mentioned above for the mean and mixing coefficient and basic guess of 1 for variances, we'll give initial guesses of 

                $$\mu_1^{(0)}=-5;\mu_2^{(0)}=10$$
                $$\sigma_1^{2(0)}=1, \sigma_2^{2(0)}=1$$
                $$\pi_1=0.5$$

                <h5>Step E-1</h5>

                $$\gamma_{i1}=\frac{0.5\times \mathcal{N}(-5|-5,1)}{0.5\times \mathcal{N}(-5|-5,1)+0.5\times \mathcal{N}(-5|10,1)}$$

                Since \(x=-5\) is close to \(\mu_1=-5\), the responsibility \(\gamma_{i1}\) will be near 1.

                We then continue to do this for each data point.

                <h5>Step M-1</h5>

                After computing responsibilities for all data points, we update the parameters. The updates means might be, for example:

                $$\mu_1^{(1)}=-4.9; \mu_2^{(1)}=9.2$$


                <h3>Other Notes</h3>

                In the example, we were given the number of components and their distribution families. If these are not known, it becomes a more difficult problem. For unknown number of components, you can use some sort of penalization method like AIC, BIC. Another quantitative method of assessing performance is cross-validation.  

                

                <div class="container">
                    <h2>Mixture Distribution Generator</h2>
                    
                    <div class="input-group">
                        <label for="numComponents">Number of Components:</label>
                        <input type="number" id="numComponents" min="1" value="2" inputmode="numeric">
                    </div>
            
                    <div id="componentInputs">
                        <!-- Inputs for distribution components will be dynamically added here -->
                    </div>
            
                    <div class="input-group">
                        <h3>Graph Customization</h3>
                        <div class="axis-settings">
                            <div class="input-field">
                                <label for="xMin">x Min:</label>
                                <input type="number" id="xMin" value="-5" inputmode="decimal">
                            </div>
                            <div class="input-field">
                                <label for="xMax">x Max:</label>
                                <input type="number" id="xMax" value="10" inputmode="decimal">
                            </div>
                        </div>
                        <div class="axis-settings">
                            <div class="input-field">
                                <label for="yMin">y Min:</label>
                                <input type="number" id="yMin" value="0" inputmode="decimal">
                            </div>
                            <div class="input-field">
                                <label for="yMax">y Max:</label>
                                <input type="number" id="yMax" value="1" inputmode="decimal">
                            </div>
                        </div>
                        <label for="interval">Interval Step Size:</label>
                        <input type="number" id="interval" value="0.1" step="0.01" min="0.01" inputmode="decimal">
                        
                    </div>

                    <button id="generate-mixture" >Generate Mixture</button>
                    
                    <canvas id="mixtureChart" width="600" height="400"></canvas>
                </div>
            

                <h3>Sampling from a Mixture Distribution</h3>

                Sampling from a mixture distribution is relatively simple, provided the component distributions are easy to sample from. 
                <h4>Steps</h4>
                <h5>Step 1: Select a Component Based on Weights</h5>
                Select a Component Based on Weights: The first step is to randomly select one of the component distributions according to their weights. This step uses the weights as probabilities, so if a component has a larger weight, it is more likely to be selected.

                You can think of this step as performing a weighted coin flip or drawing from a categorical distribution where each category corresponds to one of the component distributions. This is essentially a multinomial sampling with 1 trial which was covered in <a href="./sampling-discrete.html">Discrete Sampling</a>.
                <br><br>
                Example:
                <ul>
                  <li>Assume you have two components: a normal distribution \(N(\mu_1,\sigma_1^2)\) with weight 0.7 and an exponential distribution \(\text{Exp}(\lambda_2)\) with weight 0.3.</li>
                  <li>Generate a random number u from the uniform distribution U(0,1). If \(u\leq 0.7\), select the normal distribution. Else, select the exponential distribution</li>
                </ul>

                <h5>Step 2: Sample from the Selected Component</h5>
                After selecting a component distribution, the next step is to generate a sample from the chosen distribution.

                <br>
                Example:
                <ul>
                  <li>If you selected the normal distribution in the previous step, sample a value from \(N(\mu_1,\sigma_1^2)\)</li>
                  <li>If you selected the exponential distribution, sample a value from \(\text{Exp}(\lambda_2)\).</li>
                </ul>


                <h4>Pseudocode</h4>
                Here is pseudocode for sampling from a mixture distribution:
                
                <pre><code>
                  Input:
                  - List of component distributions: D1, D2, ..., Dk
                    (Each D represents a different distribution, like normal, exponential, or uniform. These are the components of the mixture.)
                  
                  - List of corresponding weights: w1, w2, ..., wk
                    (The weights determine how likely each component is to be selected. They must sum to 1 so that they form a valid probability distribution.)
                
                Output:
                  - Sampled value from the mixture distribution
                    (This is the final value sampled from one of the component distributions, determined by the weights.)
                
                1. Generate a random number u from U(0, 1)
                   (Generate a uniform random number between 0 and 1. This number will be used to select a component distribution based on the weights. The value of `u` will help simulate the probability of selecting each component.)
                
                2. Use the weights to determine which component distribution to sample from:
                   - Set cumulative_weight = 0
                     (This is an accumulator that will store the running sum of weights. It helps track when the random number `u` falls into a range corresponding to a specific component.)
                   
                   - For each component i from 1 to k:
                     (Iterate through each of the k component distributions.)
                     
                     a. cumulative_weight += wi
                        (Add the weight of the current component `wi` to the cumulative total. This step accumulates the probability mass for each component.)
                
                     b. If u <= cumulative_weight:
                        (Check if the random number `u` falls within the cumulative probability range for the current component.)
                        
                        - Select component Di
                          (If the condition is met, the current component `Di` is selected for sampling, since the random number `u` fell within its weight range.)
                
                        - Break the loop
                          (Exit the loop once a component is selected, as there's no need to check the remaining components.)
                
                3. Sample a value from the selected component distribution
                   (After a component is selected, sample a value from that specific distribution. Each distribution has its own sampling method, such as using the Box-Muller transform for a normal distribution, or inverse transform sampling for an exponential distribution.)
                
                4. Return the sampled value
                   (The function ends by returning the value sampled from the selected component distribution, which represents the final sample from the mixture distribution.)
                
                </code></pre>




                </div>



             




                <div id="Exercises">
                    <h2>Mixture Distribution Practice Problems</h2>

                  For any parameter estimation problem that doesn't explicitly ask for it to be done by hand, feel free to use your programming language of choice and a reasonable stopping threshold. It is unreasonble to do more than 1 iteration for more than a few data points in the ME Algorithm. 



                    <ol>
                      <li>Write code in your preferred language which samples from a mixture distribution that is N(0,1) with weight 0.3 and N(4,2) with weight 0.7.</li> 
                      <li>
                        <strong>Understanding Mixture Distributions:</strong>
                        <ol>
                          <li>What is a mixture distribution, and how is it different from a simple distribution? Explain the concept of a mixture distribution using an example of combining two normal distributions.</li>
                        </ol>
                      </li>
                      <li>
                        <strong>Plotting a Simple Mixture Distribution:</strong>
                        <ol>
                          <li>Given two normal distributions \(N(2, 1)\) and \(N(6, 1)\) with equal weights in the mixture, plot the probability density function of the mixture. What do you observe about the shape of the mixture distribution?</li>
                        </ol>
                      </li>
                      <li>
                        <strong>Comparing Mixtures:</strong>
                        <ol>
                          <li>How does increasing the weight of one component in a mixture affect the overall mixture distribution? Consider the case of mixing two normal distributions with different means. Provide an example to illustrate your answer.</li>
                        </ol>
                      </li>

                      <li>
                        <strong>Simulating a Mixture Distribution:</strong>
                        <ol>
                          <li>Generate random samples from a mixture of two normal distributions with the following parameters: \(X_1 \sim N(1, 1)\) and \(X_2 \sim N(4, 1)\). The mixture proportion is 0.7 for \(X_1\) and 0.3 for \(X_2\). Simulate 1000 points and plot the histogram of the resulting mixture.</li>
                        </ol>
                      </li>
                      <li>
                        <strong>Mixture of Exponential Distributions:</strong>
                        <ol>
                          <li>Consider a mixture of two exponential distributions with rate parameters \(\lambda_1 = 1\) and \(\lambda_2 = 0.5\). The mixture proportions are 0.4 for \(X_1\) and 0.6 for \(X_2\). Write down the PDF of the mixture distribution.</li>
                          <li>What is the expected value of the mixture distribution?</li>
                        </ol>
                      </li>
                      <li>
                        <strong>Mean of a Mixture Distribution:</strong>
                        <ol>
                          <li>Suppose you have two normal distributions \(X_1 \sim N(3, 1)\) and \(X_2 \sim N(7, 2)\), with mixture weights 0.3 and 0.7, respectively. Find the mean of the mixture distribution.</li>
                        </ol>
                      </li>
                      <li>
                        <strong>Mixture of Categorical Distributions:</strong>
                        <ol>
                          <li>Consider a mixture of two categorical distributions with the following probability mass functions:
                            \[
                            P_1 = (0.1, 0.3, 0.6) \quad \text{and} \quad P_2 = (0.5, 0.3, 0.2)
                            \]
                            If the mixture weight for \(P_1\) is 0.4 and for \(P_2\) is 0.6, what is the probability mass function of the mixture distribution?
                          </li>
                        </ol>
                      </li>
                      <li>
                        <strong>Sampling from a Normal-Exponential Mixture Distribution</strong>
                        <p>
                            Consider a mixture distribution with two components:
                            <ul>
                                <li>A normal distribution \( N(0, 1) \) with weight \( 0.6 \)</li>
                                <li>An exponential distribution with rate parameter \( \lambda = 2 \) and weight \( 0.4 \)</li>
                            </ul>
                        </p>
                        <ol type="a">
                            <li>Write the probability density function (PDF) of the mixture distribution.</li>
                            <li>If you were to generate a sample from this mixture distribution, what is the probability that the sample is drawn from the normal distribution?</li>
                            <li>Write a pseudocode algorithm to generate a sample from this mixture distribution.</li>
                        </ol>

                    </li>

                    <li>
                      <strong>Mixture of Uniform Distributions</strong>
                      <p>You are given a mixture of two uniform distributions:</p>
                      <ul>
                          <li>\( U(0, 1) \) with weight \( 0.3 \)</li>
                          <li>\( U(2, 4) \) with weight \( 0.7 \)</li>
                      </ul>
                      <ol type="a">
                          <li>Compute the probability that a randomly sampled value from the mixture is less than 1.</li>
                          <li>Find the cumulative distribution function (CDF) of the mixture distribution.</li>
                          <li>Write a Python function that samples from this mixture distribution.</li>
                      </ol>
                      
                  </li>
                  <li>
                    <strong>Basic Definition of Mixture Distributions:</strong>
                    <ol>
                      <li>Suppose you have two component distributions: \(X_1 \sim N(\mu_1, \sigma_1^2)\) and \(X_2 \sim N(\mu_2, \sigma_2^2)\). The mixture distribution is given by:
                        \[
                        f_X(x) = p f_{X_1}(x) + (1 - p) f_{X_2}(x)
                        \]
                        Derive the mean and variance of the mixture distribution in terms of the means and variances of \(X_1\) and \(X_2\), as well as the mixture weight \(p\).</li>
                      <li>If \( \mu_1 = 1 \), \( \sigma_1 = 1 \), \( \mu_2 = 4 \), and \( \sigma_2 = 2 \), compute the mixture's mean and variance for \(p = 0.3\).</li>
                    </ol>
                  </li>
                  <li>
                    <strong>Mixture of Three Normal Distributions</strong>
                    <p>Consider a mixture of three normal distributions with the following parameters:</p>
                    <ul>
                        <li>\( N(-1, 0.5^2) \) with weight \( 0.2 \)</li>
                        <li>\( N(2, 1^2) \) with weight \( 0.5 \)</li>
                        <li>\( N(4, 0.25^2) \) with weight \( 0.3 \)</li>
                    </ul>
                    <ol type="a">
                        <li>Derive the expected value \( E[X] \) of the mixture distribution.</li>
                        <li>Derive the variance \( \text{Var}(X) \) of the mixture distribution.</li>
                        <li>Describe the steps to sample from this mixture distribution.</li>
                    </ol>
                    
                </li>
                    

                    <li>
                      <strong>Mixture of Exponential Distributions:</strong>
                      <ol>
                        <li>Suppose a dataset is generated from a mixture of two exponential distributions with rate parameters \(\lambda_1\) and \(\lambda_2\). The mixture model is:
                          \[
                          f_X(x) = p \lambda_1 e^{-\lambda_1 x} + (1 - p) \lambda_2 e^{-\lambda_2 x}
                          \]
                          <ol>
                            <li>Find the mean and variance of the mixture distribution in terms of \(\lambda_1\), \(\lambda_2\), and \(p\).</li>
                            <li>Given the dataset:
                              \[
                              \{0.2, 0.5, 0.8, 1.1, 2.5, 2.8, 3.2, 3.5\}
                              \]
                              Estimate the parameters \(\lambda_1\), \(\lambda_2\), and \(p\) using the method of moments.
                            </li>
                          </ol>
                        </li>
                      </ol>
                    </li>
                    <li>
                      <strong>Mixture of Binomial Distributions:</strong>
                      <ol>
                        <li>Consider a mixture of two binomial distributions, where:
                          \[
                          X_1 \sim \text{Binomial}(n, p_1), \quad X_2 \sim \text{Binomial}(n, p_2)
                          \]
                          The mixture distribution is:
                          \[
                          P(X = k) = p \binom{n}{k} p_1^k (1 - p_1)^{n-k} + (1 - p) \binom{n}{k} p_2^k (1 - p_2)^{n-k}
                          \]
                          <ol>
                            <li>Derive the mean and variance of the mixture distribution.</li>
                            <li>If \(n = 10\), \(p_1 = 0.3\), \(p_2 = 0.7\), and \(p = 0.4\), calculate the probability mass function for \(X\).</li>
                          </ol>
                        </li>
                      </ol>
                    </li>
                    <li>
                      <strong>Identifying Mixture Distributions from Data:</strong>
                      <ol>
                        <li>Given the following data:
                          \[
                          \{0.5, 0.7, 1.0, 1.5, 4.2, 4.5, 5.0, 5.2\}
                          \]
                          Assume the data comes from a mixture of two normal distributions. Estimate the parameters (means, variances, and mixing proportion) for the component distributions using the method of moments.
                        </li>
                      </ol>
                    </li>
    
                    <li>
                      <strong>Mixture of Poisson Distributions:</strong>
                      <ol>
                        <li>Suppose you have a mixture of two Poisson distributions:
                          \[
                          X_1 \sim \text{Poisson}(\lambda_1), \quad X_2 \sim \text{Poisson}(\lambda_2)
                          \]
                          The mixture is defined as:
                          \[
                          P(X = k) = p \frac{\lambda_1^k e^{-\lambda_1}}{k!} + (1 - p) \frac{\lambda_2^k e^{-\lambda_2}}{k!}
                          \]
                          <ol>
                            <li>Derive the mean and variance of the mixture distribution.</li>
                            <li>If \(\lambda_1 = 3\), \(\lambda_2 = 7\), and \(p = 0.6\), compute the probability that \(X = 4\).</li>
                          </ol>
                        </li>
                      </ol>
                    </li>
                  </ol>
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>