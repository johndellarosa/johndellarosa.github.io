<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Distribution Distance</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Copula, Probability Integral Transform">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to distribution distance metrics">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/distribution-distance"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Copulas">
                <h1>Convolution</h1>
                <h2>Introduction to Convolution</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./probability-2.html">Probability II</a></li>
                    <li><a href="./sampling-continuous.html">Sampling</a></li>
                    <li><a href="./multivariate-intro">Introduction to Multivariate Distributions</a></li>
               
                </ol>

                <h3>Notion of Distance Between Distributions</h3>

                In regular geometry, distances, especially L2 or Euclidian, are intuitive and have several properties that we take for granted. When we start talking about distances between distributions, some of these properties may not hold. 
                One term is a <strong>metric</strong>, d(a,b) which has properties:

                <ol>
                    <li>Distance to itself is 0; \(d(a,a)=0\)</li>
                    <li>Distances are positive if not covered by the first axiom. If \(a\neq b\), \(d(a,b)\gt 0\)</li>
                    <li>Distance from A to B is the same as the distance from B to A (Symmetry); \(d(a,b)=d(b,a)\) </li>
                    <li>Triangle Inequality, which given 3 points and 2 known distances between them, places a range on possible values for the unknown 3rd distance. \(d(a,c)\leq d(a,b)+d(b,c)\)</li>

                </ol>

                In probability theory and statistics, it is often necessary to measure the distance or similarity between probability distributions. 
                This begs the question, what does it mean for two probability distributions to be close or far apart? The idea of distance between distributions captures how different two distributions are in terms of their likelihood of producing the same outcomes. Unlike traditional geometric distances, the space of probability distributions is not inherently spatial, and the comparison involves aspects such as shape, spread, and probability mass. 



                <h3>Total Variational Distance</h3>

                The Total Variation Distance (TVD) between two probability distributions P and Q on a measurable space \(\Omega\) is defined as:

                $$\delta_{TV}(P,Q)=\sup_{A\subset\Omega}|P(A)-Q(A)|$$



                 The TVD obeys the properties of a metric. 

                <h3>Kullback-Leibler Divergence</h3>
                Shannon Entropy in Information Theory is given by

                $$H(X)=-\sum_{x\in\Omega}p(x)\log(p(x))$$

                A continuous analogue, differential entropy, extends this concept to non-discrete variables:

                $$H(X)=\mathbb{E}[-\log(f(X))]=\int_{\Omega}f(x)\log(f(x))dx$$

                KL Divergence builds off that idea:
                <br><br>
                For continuous distributions,
                $$D_{KL}(P||Q)=\int_{\Omega}f_P(x)\log\left(\frac{f_P(x)}{f_Q(x)}\right)dx$$
                <br><br>
                For discrete distributions:
                $$D_{KL}(P||Q)=\sum_{x\in\Omega}P(x)\log\left(\frac{P(x)}{Q(x)}\right)$$

                One important thing to note is that KL Divergence is not symmetric; i.e., \(D_{KL}(P||Q)\neq D_{KL}(Q||P)\) unless P=Q (almost surely). 
                








                </div>



             




                <div id="Exercises">
                    <h2>Distribution Distance Practice Problems</h2>

                    <ol>
                       
                    </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>