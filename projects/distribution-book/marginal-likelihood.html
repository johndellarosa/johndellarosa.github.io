<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Marginal Likelihood</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Marginal Likelihood">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to marginal likelihood.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/marginal-likelihood"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Marginal-Likelihood-Explanation">
                <h1>Marginal Likelihood</h1>
                <h2>Introduction to </h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                </ol>
                
                <h3>Introduction</h3>
                In the previous chapters, we explored compound distributions. In a compound distribution, one distribution controls the parameters of another, leading to a multi-layered structure. 
                As we extend this understanding, marginal likelihood provides a framework for evaluating how well these models (and others) explain observed data, accounting for all possible values of the parameters. By integrating over the unknown parameters, marginal likelihood gives us a comprehensive measure of the model's fit to the data, offering insights that can guide model selection and decision-making. 

                <h3>Recap on Marginalization in Context of Previous Chapters</h3>

                <h4>Mixture Distributions</h4>

                In a mixture distribution, the observed data is generated from one of several component distributions. The components are chosen according to a set of probabilities (weights). For example, in a Gaussian Mixture Model (GMM) with K components:

                $$X\sim \sum_{k=1}^{K}\pi_{k}\mathcal{N}(X|\mu_k,\sigma_k^2)$$

                Here, each component, \(\mathcal{N}(X|\mu_k,\sigma_k^2)\), is a Gaussian distribution, and \(\pi_k\) are the mixture weights (summing to 1). The marginal distribution of X is the weighted sum of the component densities, integrating over the latent variable (which component generated the data). 


                <h4>Compound Distributions</h4>
                For example, in a Gamma-Poisson compound distribution, the rate parameter \(\lambda\) of a Poisson distribution is drawn from a Gamma distribution: 

                $$\lambda\sim \text{Gamma}(\alpha,\beta),\quad X|\lambda\sim\text{Poisson}(\lambda)$$

                The marginal distribution of \(X\) is found by integrating out the intermediate parameter \(\lambda\):

                $$p(X=x)=\int_{0}^{\infty}p(X=x|\lambda)p(\lambda)d\lambda$$
                The integration removes the dependence on the unknown/random parameter \(\lambda\), yielding the marginal distribution of X.



                <h3>Bridging to Marginal Likelihood</h3>
                In both compound and mixture distributions, marginalization plays a crucial role: we integrate over hidden or latent variables (such as the parameters in compound distributions or the component assignments in mixture models) to find the overall probability of the observed data. This exact idea underpins marginal likelihood.

                The marginal likelihood of a model is the probability of the observed data D given the model, integrating out the unknown parameters \(\theta\):

                $$p(D|M)=\int_{\theta}p(D|\theta,M)p(\theta,M)d\theta$$
                where:
                <ul>
                    <li>\(p(D|\theta,M)\) is the likelihood of the data given parameters \(\theta\)</li>
                    <li>\(p(\theta|M)\) is the prior distribution over the parameters</li>
                    <li>M is the model</li>
                </ul>
                This integral is precisely what we encountered in compound distributions: marginalizing over \(\theta\), an nknown parameter, which itself follows a prior distribution. In essence, marginal likelihood is an application of the same principle: we integrate out the uncertainty in the parameters to assess how well the model explains the data on average across all parameter configurations. 


                <h2>Regularization</h2>

                <h3>Basic</h3>
                In frequentist statistics, regularization is a technique used to prevent models from overfitting the data by adding a penalty for complexity. This penalty is applied in various forms, such as:

                <ul>
                    <li>LASSO (L1 regularization): Adds a penalty proportional to the absolute value of the model parameters</li>
                    <li>Ridge (L2 regularization): Adds a penalty proportional to the square of the model parameters</li>
                </ul>

                The ridge objective function to be minimized for OLS is given by:

                $$\sum_{i}^{n}(y_i-x_i^T\beta)^2+\lambda\sum_{j=1}^{p}\beta_j^2$$
                where \(\beta\) is the vector of model coefficients and \(\lambda\) is the regularization parameter controlling the strength of the penalty. 
                <h3>Bayesian</h3>

                From a Bayesian point of view, regularization arises naturally through the prior distribution on the parameters. Regularization can be interpreted as placing a prior belief on the parameters, which encodes our preference for simpler models or smaller parameter values. 

                <h4>Ridge</h4>
                In Ridge regression, the quadratic penalty discourages large coefficients by assigning an increasingly steep penalty. In Bayesian terms, this is equivalent to placing a Gaussian prior on the regression coefficients \(\beta\), with mean zero and variance \(\sigma^2\). 
                $$p(\beta_j|\sigma^2)\sim \mathcal{N}(0,\sigma^2)$$

                The regularization strength, \(\lambda\), corresponds to this variance as follows:

                $$\lambda=\frac{1}{\sigma^2}$$

                <h4>LASSO</h4>
                Similarly, LASSO adds a penalty proportional to the absolute value of the coefficients, corresponding to a Laplace prior on the parameters: 

                $$p(\beta_j|\lambda)\sim\frac{1}{2\lambda}\exp\left(-\frac{|\beta_j|}{\lambda}\right)$$





                </div>



             




                <div id="Exercises">
                    <h2>Marginal Likelihood Practice Problems</h2>
                    <ol>
                        
                        </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>