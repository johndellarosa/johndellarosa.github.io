<!DOCTYPE html>
<html lang="en-US">
    <head>

        <meta charset="UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2C44LTKBE1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2C44LTKBE1');
</script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
            <title>Regularization</title>
    
        <!-- Meta tags -->
        
        <meta name="keywords" content="Regularization">
        <meta name="author" content="John Della Rosa" >
        <meta name="description" content="Introduction to regularization in Bayesian framework.">
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://johndellarosa.github.io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="https://johndellarosa.github.io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="https://johndellarosa.github.io/favicon-16x16.png">
        <link rel="manifest" href="https://johndellarosa.github.io/site.webmanifest">
        <link rel="canonical" href="https://johndellarosa.github.io/projects/distribution-book/regularization"/>    
        <link rel="stylesheet" href="https://johndellarosa.github.io/style.css"> 
<!-- 
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
      

        <script src="../../math.js" type="text/javascript"></script> -->

    </head>
    <body>
        <div class="navbar">
            <b style="margin-right:10px">John Della Rosa</b>| 
            <a href="https://johndellarosa.github.io/">Home</a>|
            <a href="https://johndellarosa.github.io/resume">Resume</a>|
            <a href="https://johndellarosa.github.io/biography">About</a>|
            <a href="https://johndellarosa.github.io/projects">Projects</a>|
            <a href="https://johndellarosa.github.io/miscellaneous">Misc</a>|
            <a href="https://www.linkedin.com/in/johndellarosa/" target="_blank">Linkedin</a>|
            <a href="https://github.com/johndellarosa" target="_blank">Github</a>|
            <a href="https://play.google.com/store/apps/developer?id=JohnDellaRosa" target="_blank">Google Play</a>|
            <a href="https://apps.apple.com/us/developer/john-della-rosa/id1684177615" target="_blank">Apple Store</a>
        
        </div>
        <h2><a href= "table-of-contents.html">Distribution Textbook (Work in Progress)</a></h2>

<h3>by John Della Rosa</h3>
        <div id="text-contents" style="width:90%; margin:auto">
            <div id="Marginal-Likelihood-Explanation">
                <h1>Regularization</h1>
                <h2>Introduction to Regularizatoin</h2>
                
                <h3>Recommended Prerequesites</h3>
                <ol>
                    <li><a href="https://johndellarosa.github.io/projects/biophysics-book/probability">Probability</a></li>
                    <li><a href="./mle.html">MLE</a></li>
                    <li><a href="./marginal-likelihood.html">Marginal Likelihood</a></li>
                    <li><a href="./bayesian.html">Bayesian Inference</a></li>
                </ol>
                


                <h2>OLS Regularization</h2>

                <h3>Basic</h3>
                In frequentist statistics, regularization is a technique used to prevent models from overfitting the data by adding a penalty for complexity. This penalty is applied in various forms, such as:

                <ul>
                    <li>LASSO (L1 regularization): Adds a penalty proportional to the absolute value of the model parameters</li>
                    <li>Ridge (L2 regularization): Adds a penalty proportional to the square of the model parameters</li>
                </ul>

                For OLS, the objective function that gets minimized is the Residual Sum of Squares:

                $$\text{Objective}=\text{RSS}=\sum_{i=1}^n(y_i-\hat{y}_i)^2$$

                The LASSO objective function to be minimized for OLS is given by:

                $$\text{Objective}=\text{RSS}+\lambda\sum_{j=1}^{p}|\beta_j|$$

                The ridge objective function to be minimized for OLS is given by:

                $$\text{Objective}=\text{RSS}+\lambda\sum_{j=1}^{p}\beta_j^2$$
                where \(\beta\) is the vector of model coefficients and \(\lambda\) is the regularization parameter controlling the strength of the penalty. 
                <h2>Bayesian</h2>

                From a Bayesian point of view, regularization arises naturally through the prior distribution on the parameters. Regularization can be interpreted as placing a prior belief on the parameters, which encodes our preference for simpler models or smaller parameter values. 

                Two priors that we will go over are the Gaussian prior and the Laplace prior.

                <h3>Gaussian Prior</h3>

                In Bayesian inference, placing a Gaussian prior on the parameters of a linear regression model leads to Ridge regression. 

                If we place a zero-mean normal prior on each parameter \(\beta_j\):

                $$P(\beta_j)=\frac{1}{\sqrt{2\pi\tau^2}}\exp\left(-\frac{\beta_j^2}{2\tau^2}\right)$$
                This prior expresses the belief that the parameters are likely to be small, centered around zero, with \(\tau^2\) controlling the prior variance. 

                For linear regression with Gaussian noise, the Maximum a Posteriori estimate of \(\beta\) is:

                $$\text{MAP Objective}=\text{RSS}+\frac{1}{2\tau^2}\sum_{j=1}^p\beta_j^2$$
                Comparing objective functions, we can see that \(\frac{1}{2\tau^2}=\lambda\)

                <h3>Laplace Prior</h3>

                Placing a (centered) Laplace prior on the parameters corresponds to a LASSO regression. The Laplace prior is given by:

                $$P(\beta_j)=\frac{\lambda}{2}\exp\left(-\lambda|\beta_j|\right)$$

                $$\text{MAP Objective}=\text{RSS}+\lambda\sum_{j=1}^p|\beta_j|$$

                The Laplace distribution has a sharper peak and heavier tails than a Gaussian. This leads to parameters being set to exactly 0 or being higher in magnitude compared to a Gaussian prior which strongly punishes large coefficients, but won't reduce to 0.

                


                </div>



             




                <div id="Exercises">
                    <h2>Regularization Practice Problems</h2>
                    <ol>
                        
                        </ol>
    
    
                </div>
        </div>
        <script>
            var coll = document.getElementsByClassName("collapsible");
            var i;
    
            for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                content.style.display = "none";
                } else {
                content.style.display = "block";
                }
            });
            } 
            </script>
</body>
</html>